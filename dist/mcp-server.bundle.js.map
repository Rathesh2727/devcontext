{
  "version": 3,
  "sources": ["../src/config.js", "../src/utils/logger.js", "../src/db.js", "../src/logic/RelationshipContextManagerLogic.js", "../src/main.js", "../src/tools/initializeConversationContext.tool.js", "../src/logic/ConversationIntelligence.js", "../src/logic/TextTokenizerLogic.js", "../src/logic/ContextIndexerLogic.js", "../src/logic/CodeStructureAnalyzerLogic.js", "../src/logic/ConversationSegmenter.js", "../src/logic/ContextCompressorLogic.js", "../src/logic/ConversationPurposeDetector.js", "../src/logic/IntentPredictorLogic.js", "../src/logic/TimelineManagerLogic.js", "../src/logic/SmartSearchServiceLogic.js", "../src/logic/ActiveContextManager.js", "../src/logic/ContextPrioritizerLogic.js", "../src/logic/GlobalPatternRepository.js", "../src/schemas/toolSchemas.js", "../src/tools/updateConversationContext.tool.js", "../src/logic/KnowledgeProcessor.js", "../src/tools/retrieveRelevantContext.tool.js", "../src/logic/InsightEngine.js", "../src/tools/recordMilestoneContext.tool.js", "../src/logic/LearningSystem.js", "../src/logic/SemanticPatternRecognizerLogic.js", "../src/tools/finalizeConversationContext.tool.js", "../src/tools/index.js", "../src/tools/mcpDevContextTools.js"],
  "sourcesContent": ["/**\n * Configuration module that loads and exports environment variables\n */\n\nimport dotenv from \"dotenv\";\n\n// Load environment variables from .env file\ndotenv.config();\n\n// Critical TursoDB credentials - required for server operation\nexport const TURSO_DATABASE_URL = process.env.TURSO_DATABASE_URL;\nexport const TURSO_AUTH_TOKEN = process.env.TURSO_AUTH_TOKEN;\n\n// Logging configuration\nexport const LOG_LEVEL = process.env.LOG_LEVEL || \"INFO\";\nexport const DB_LOGGING_ENABLED = process.env.DB_LOGGING_ENABLED === \"true\";\n\n// Context retrieval configuration\nexport const DEFAULT_TOKEN_BUDGET = parseInt(\n  process.env.DEFAULT_TOKEN_BUDGET || \"4000\",\n  10\n);\nexport const CONTEXT_DECAY_RATE = parseFloat(\n  process.env.CONTEXT_DECAY_RATE || \"0.95\"\n);\nexport const MAX_CACHE_SIZE = parseInt(\n  process.env.MAX_CACHE_SIZE || \"1000\",\n  10\n);\n", "/**\n * Logger utility module\n * Provides logging functionality with level-based filtering and optional DB persistence\n */\n\nimport { LOG_LEVEL, DB_LOGGING_ENABLED } from \"../config.js\";\n\n// Log level priorities (higher number = higher priority)\nconst LOG_LEVELS = {\n  DEBUG: 0,\n  INFO: 1,\n  WARN: 2,\n  ERROR: 3,\n};\n\n/**\n * Logs a message with the specified level and optional data\n * @param {string} level - Log level ('DEBUG', 'INFO', 'WARN', 'ERROR')\n * @param {string} message - Log message\n * @param {object|null} data - Optional data to include with the log\n */\nexport const logMessage = (level, message, data = null) => {\n  // Convert level to uppercase for consistency\n  const upperLevel = level.toUpperCase();\n\n  // Only log if the message level is at or above the configured level\n  if (\n    !LOG_LEVELS.hasOwnProperty(upperLevel) ||\n    LOG_LEVELS[upperLevel] < LOG_LEVELS[LOG_LEVEL]\n  ) {\n    return;\n  }\n\n  // Create timestamp\n  const timestamp = new Date().toISOString();\n\n  // Format the log message\n  let logString = `[${timestamp}] [${upperLevel}]: ${message}`;\n  if (data) {\n    const dataString = typeof data === \"string\" ? data : JSON.stringify(data);\n    logString += ` - ${dataString}`;\n  }\n\n  // Output to appropriate stream\n  if (upperLevel === \"DEBUG\" || upperLevel === \"INFO\") {\n    console.log(logString);\n  } else {\n    console.error(logString);\n  }\n\n  // Database logging would happen here, but we're avoiding circular dependency\n  // If DB_LOGGING_ENABLED is true, we would log to the database\n  // But since we need to avoid importing from db.js, we'll skip this part\n};\n\nexport default logMessage;\n", "/**\n * Database client module for TursoDB connections\n * Manages the connection to the TursoDB database and provides query utilities\n */\n\nimport { createClient } from \"@libsql/client\";\nimport { TURSO_DATABASE_URL, TURSO_AUTH_TOKEN } from \"./config.js\";\nimport { logMessage } from \"./utils/logger.js\";\n\n// Module-scoped singleton instance\nlet dbClient = null;\n\n/**\n * Initialize and return a TursoDB client instance (singleton)\n * @returns {Object} TursoDB client instance\n * @throws {Error} If database URL or auth token is missing\n */\nexport const getDbClient = () => {\n  if (dbClient) {\n    return dbClient;\n  }\n\n  if (!TURSO_DATABASE_URL) {\n    throw new Error(\n      \"TURSO_DATABASE_URL is not defined in environment variables\"\n    );\n  }\n\n  if (!TURSO_AUTH_TOKEN) {\n    throw new Error(\"TURSO_AUTH_TOKEN is not defined in environment variables\");\n  }\n\n  dbClient = createClient({\n    url: TURSO_DATABASE_URL,\n    authToken: TURSO_AUTH_TOKEN,\n  });\n\n  return dbClient;\n};\n\n/**\n * Test the database connection by executing a simple query\n * @param {Object} client - TursoDB client instance from getDbClient()\n * @returns {Promise<boolean>} True if connection is successful\n * @throws {Error} If connection fails\n */\nexport const testDbConnection = async (client = null) => {\n  try {\n    const dbClient = client || getDbClient();\n    await dbClient.execute(\"SELECT 1\");\n    return true;\n  } catch (error) {\n    throw new Error(`Database connection test failed: ${error.message}`);\n  }\n};\n\n/**\n * Execute a SQL query with optional parameters\n * @param {string} sqlQuery - SQL query to execute\n * @param {Array} [args=[]] - Optional array of query parameters for parameterized queries\n * @returns {Promise<Object>} Query result\n * @throws {Error} If query execution fails\n */\nexport const executeQuery = async (sqlQuery, args = []) => {\n  try {\n    // Log the query for debugging\n    console.log(\"DB - EXECUTING QUERY:\", {\n      sql: sqlQuery.substring(0, 150) + (sqlQuery.length > 150 ? \"...\" : \"\"),\n      args:\n        args.length > 0\n          ? JSON.stringify(args.slice(0, 3)) + (args.length > 3 ? \"...\" : \"\")\n          : \"[]\",\n    });\n\n    const client = getDbClient();\n    const result = await client.execute({\n      sql: sqlQuery,\n      args: args,\n    });\n\n    // Log the result for debugging\n    console.log(\"DB - QUERY RESULT:\", {\n      rowCount: result.rows?.length || 0,\n      rowsPreview:\n        result.rows?.length > 0\n          ? JSON.stringify(result.rows[0]).substring(0, 100) + \"...\"\n          : \"No rows\",\n      affectedRows: result.rowsAffected,\n    });\n\n    return result;\n  } catch (error) {\n    console.error(\"DB - QUERY ERROR:\", {\n      message: error.message,\n      query: sqlQuery.substring(0, 150),\n      args: args.length > 0 ? JSON.stringify(args.slice(0, 3)) : \"[]\",\n    });\n\n    throw new Error(\n      `Query execution failed: ${error.message}\\nQuery: ${sqlQuery}`\n    );\n  }\n};\n\n/**\n * Check if a column exists in a table\n * @param {string} tableName - Name of the table\n * @param {string} columnName - Name of the column to check\n * @returns {Promise<boolean>} True if the column exists\n */\nasync function columnExists(tableName, columnName) {\n  try {\n    if (!tableName || !columnName) {\n      logMessage(\"error\", \"Invalid table or column name provided\");\n      return false;\n    }\n\n    const client = getDbClient();\n    const result = await client.execute({\n      sql: `PRAGMA table_info(${tableName})`,\n    });\n\n    // Check if result and rows are valid\n    if (!result || !result.rows || result.rows.length === 0) {\n      logMessage(\"warn\", `No table info found for ${tableName}`);\n      return false;\n    }\n\n    // Check each row for the column name\n    for (const row of result.rows) {\n      if (row && row.name === columnName) {\n        return true;\n      }\n    }\n\n    logMessage(\"debug\", `Column ${columnName} not found in ${tableName}`);\n    return false;\n  } catch (error) {\n    logMessage(\"error\", `Error checking if column exists: ${error.message}`);\n    return false;\n  }\n}\n\n/**\n * Migrate the project_patterns table to add the language column if it doesn't exist\n * @returns {Promise<void>}\n */\nasync function migrateProjectPatternsTable() {\n  try {\n    // First check if the table exists\n    const tableExistsQuery = await executeQuery(`\n      SELECT name FROM sqlite_master\n      WHERE type='table' AND name='project_patterns'\n    `);\n\n    const tableExists =\n      tableExistsQuery &&\n      tableExistsQuery.rows &&\n      tableExistsQuery.rows.length > 0;\n\n    if (!tableExists) {\n      logMessage(\n        \"info\",\n        \"project_patterns table doesn't exist yet, skipping migration\"\n      );\n      return;\n    }\n\n    // Then check if the language column exists\n    const hasLanguageColumn = await columnExists(\n      \"project_patterns\",\n      \"language\"\n    );\n\n    if (!hasLanguageColumn) {\n      logMessage(\"info\", \"Adding language column to project_patterns table\");\n\n      try {\n        // Add the language column to the table\n        await executeQuery(\n          \"ALTER TABLE project_patterns ADD COLUMN language TEXT\"\n        );\n\n        logMessage(\n          \"info\",\n          \"Successfully added language column to project_patterns table\"\n        );\n      } catch (alterError) {\n        // If the column already exists, SQLite will throw an error\n        if (alterError.message.includes(\"duplicate column\")) {\n          logMessage(\"info\", \"Language column already exists, skipping\");\n        } else {\n          // For other errors, rethrow\n          throw alterError;\n        }\n      }\n\n      // Create index for the language column if needed\n      try {\n        await executeQuery(\n          \"CREATE INDEX IF NOT EXISTS idx_project_patterns_language ON project_patterns(language)\"\n        );\n        logMessage(\"info\", \"Created index for language column\");\n      } catch (indexError) {\n        logMessage(\"warn\", `Error creating index: ${indexError.message}`);\n      }\n    } else {\n      logMessage(\n        \"debug\",\n        \"Language column already exists in project_patterns table\"\n      );\n    }\n  } catch (error) {\n    throw new Error(`Migration failed: ${error.message}`);\n  }\n}\n\n/**\n * Initialize the database schema by creating all required tables and indexes\n * This function executes all CREATE TABLE, CREATE INDEX and CREATE TRIGGER statements\n * defined in the project blueprint\n * @returns {Promise<boolean>} True if schema initialization was successful\n */\nexport const initializeDatabaseSchema = async () => {\n  try {\n    const client = getDbClient();\n    let success = true;\n\n    // First, check if we need to migrate the project_patterns table by adding the language column\n    try {\n      await migrateProjectPatternsTable();\n    } catch (migrationError) {\n      logMessage(\"warn\", `Migration warning: ${migrationError.message}`);\n      // Continue with schema initialization, migration error is not fatal\n    }\n\n    // Array of SQL statements to execute sequentially\n    const schemaStatements = [\n      // ========= CODE ENTITIES =========\n      `CREATE TABLE IF NOT EXISTS code_entities (\n        entity_id TEXT PRIMARY KEY, -- UUID\n        file_path TEXT, -- Full path for file entities, or path to file containing the entity\n        entity_type TEXT NOT NULL, -- e.g., 'file', 'function', 'class', 'method', 'variable', 'interface', 'comment_block'\n        name TEXT, -- Name of the function, class, variable etc.\n        start_line INTEGER,\n        end_line INTEGER,\n        content_hash TEXT, -- Hash of the raw content to detect changes\n        raw_content TEXT,\n        summary TEXT, -- AI or rule-based summary\n        language TEXT, -- Programming language\n        parent_entity_id TEXT, -- For hierarchical structure (e.g., function inside a class, class inside a file)\n        last_modified_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n        last_accessed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, -- For recency\n        importance_score REAL DEFAULT 1.0, -- For prioritization, can decay\n        custom_metadata TEXT, -- JSON blob for other properties\n        FOREIGN KEY (parent_entity_id) REFERENCES code_entities(entity_id) ON DELETE CASCADE\n      )`,\n\n      // Indexes for code_entities\n      `CREATE INDEX IF NOT EXISTS idx_code_entities_file_path ON code_entities(file_path)`,\n      `CREATE INDEX IF NOT EXISTS idx_code_entities_type ON code_entities(entity_type)`,\n      `CREATE INDEX IF NOT EXISTS idx_code_entities_name ON code_entities(name)`,\n      `CREATE INDEX IF NOT EXISTS idx_code_entities_last_accessed ON code_entities(last_accessed_at DESC)`,\n      `CREATE INDEX IF NOT EXISTS idx_code_entities_importance ON code_entities(importance_score DESC)`,\n\n      // ========= ENTITY KEYWORDS =========\n      `CREATE TABLE IF NOT EXISTS entity_keywords (\n        keyword_id INTEGER PRIMARY KEY AUTOINCREMENT,\n        entity_id TEXT NOT NULL,\n        keyword TEXT NOT NULL,\n        term_frequency REAL,\n        weight REAL DEFAULT 1.0,\n        keyword_type TEXT, -- e.g., 'identifier', 'comment', 'string_literal', 'n_gram_2', 'n_gram_3'\n        FOREIGN KEY (entity_id) REFERENCES code_entities(entity_id) ON DELETE CASCADE\n      )`,\n\n      // Indexes for entity_keywords\n      `CREATE INDEX IF NOT EXISTS idx_entity_keywords_keyword ON entity_keywords(keyword)`,\n      `CREATE INDEX IF NOT EXISTS idx_entity_keywords_entity ON entity_keywords(entity_id)`,\n      `CREATE UNIQUE INDEX IF NOT EXISTS idx_entity_keywords_entity_keyword_type ON entity_keywords(entity_id, keyword, keyword_type)`,\n\n      // ========= FULL-TEXT SEARCH =========\n      `CREATE VIRTUAL TABLE IF NOT EXISTS code_entities_fts USING fts5(\n        entity_id UNINDEXED,\n        name,\n        searchable_content\n      )`,\n\n      // Triggers to keep FTS table in sync with code_entities\n      `CREATE TRIGGER IF NOT EXISTS code_entities_ai AFTER INSERT ON code_entities BEGIN\n        INSERT INTO code_entities_fts (entity_id, name, searchable_content)\n        VALUES (new.entity_id, new.name, new.raw_content || ' ' || COALESCE(new.summary, ''));\n      END`,\n\n      `CREATE TRIGGER IF NOT EXISTS code_entities_ad AFTER DELETE ON code_entities BEGIN\n        DELETE FROM code_entities_fts WHERE entity_id = old.entity_id;\n      END`,\n\n      `CREATE TRIGGER IF NOT EXISTS code_entities_au AFTER UPDATE ON code_entities BEGIN\n        UPDATE code_entities_fts SET\n          name = new.name,\n          searchable_content = new.raw_content || ' ' || COALESCE(new.summary, '')\n        WHERE entity_id = old.entity_id;\n      END`,\n\n      // ========= CODE RELATIONSHIPS =========\n      `CREATE TABLE IF NOT EXISTS code_relationships (\n        relationship_id TEXT PRIMARY KEY, -- UUID\n        source_entity_id TEXT NOT NULL,\n        target_entity_id TEXT NOT NULL,\n        relationship_type TEXT NOT NULL,\n        weight REAL DEFAULT 1.0,\n        metadata TEXT, -- JSON blob\n        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n        FOREIGN KEY (source_entity_id) REFERENCES code_entities(entity_id) ON DELETE CASCADE,\n        FOREIGN KEY (target_entity_id) REFERENCES code_entities(entity_id) ON DELETE CASCADE\n      )`,\n\n      // Indexes for code_relationships\n      `CREATE INDEX IF NOT EXISTS idx_code_relationships_source ON code_relationships(source_entity_id, relationship_type)`,\n      `CREATE INDEX IF NOT EXISTS idx_code_relationships_target ON code_relationships(target_entity_id, relationship_type)`,\n      `CREATE UNIQUE INDEX IF NOT EXISTS idx_code_relationships_unique ON code_relationships(source_entity_id, target_entity_id, relationship_type)`,\n\n      // ========= CONVERSATION HISTORY =========\n      `CREATE TABLE IF NOT EXISTS conversation_history (\n        message_id TEXT PRIMARY KEY, -- UUID\n        conversation_id TEXT NOT NULL,\n        role TEXT NOT NULL, -- 'user', 'assistant', 'system'\n        content TEXT NOT NULL,\n        timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n        related_context_entity_ids TEXT, -- JSON array of entity_ids\n        summary TEXT,\n        user_intent TEXT,\n        topic_segment_id TEXT,        -- Reference to topic segment\n        semantic_markers TEXT,        -- JSON array of semantic markers found in message\n        sentiment_indicators TEXT      -- JSON structure for sentiment analysis\n      )`,\n\n      // Indexes for conversation_history\n      `CREATE INDEX IF NOT EXISTS idx_conversation_history_conversation_ts ON conversation_history(conversation_id, timestamp DESC)`,\n      `CREATE INDEX IF NOT EXISTS idx_conversation_history_topic ON conversation_history(topic_segment_id)`,\n\n      // ========= CONVERSATION TOPICS =========\n      `CREATE TABLE IF NOT EXISTS conversation_topics (\n        topic_id TEXT PRIMARY KEY, -- UUID\n        conversation_id TEXT NOT NULL,\n        topic_name TEXT NOT NULL,\n        description TEXT,\n        start_message_id TEXT NOT NULL,\n        end_message_id TEXT,    -- NULL if ongoing\n        start_timestamp TIMESTAMP NOT NULL,\n        end_timestamp TIMESTAMP,  -- NULL if ongoing\n        primary_entities TEXT,    -- JSON array of entity_ids\n        keywords TEXT,            -- JSON array of keywords\n        summary TEXT,\n        parent_topic_id TEXT,     -- For hierarchical topic structure\n        FOREIGN KEY (start_message_id) REFERENCES conversation_history(message_id) ON DELETE CASCADE,\n        FOREIGN KEY (parent_topic_id) REFERENCES conversation_topics(topic_id) ON DELETE SET NULL\n      )`,\n\n      // Indexes for conversation_topics\n      `CREATE INDEX IF NOT EXISTS idx_conversation_topics_conversation ON conversation_topics(conversation_id)`,\n      `CREATE INDEX IF NOT EXISTS idx_conversation_topics_timestamps ON conversation_topics(start_timestamp, end_timestamp)`,\n\n      // ========= CONVERSATION PURPOSES =========\n      `CREATE TABLE IF NOT EXISTS conversation_purposes (\n        purpose_id TEXT PRIMARY KEY, -- UUID\n        conversation_id TEXT NOT NULL,\n        purpose_type TEXT NOT NULL,  -- 'debugging', 'feature_planning', 'code_review', etc.\n        confidence REAL NOT NULL DEFAULT 0.0,\n        start_timestamp TIMESTAMP NOT NULL,\n        end_timestamp TIMESTAMP,     -- NULL if ongoing\n        metadata TEXT                -- JSON with additional classification data\n      )`,\n\n      // Indexes for conversation_purposes\n      `CREATE INDEX IF NOT EXISTS idx_conversation_purposes_conversation ON conversation_purposes(conversation_id)`,\n      `CREATE INDEX IF NOT EXISTS idx_conversation_purposes_type ON conversation_purposes(purpose_type, confidence DESC)`,\n\n      // ========= TIMELINE EVENTS =========\n      `CREATE TABLE IF NOT EXISTS timeline_events (\n        event_id TEXT PRIMARY KEY, -- UUID\n        event_type TEXT NOT NULL,\n        timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n        data TEXT, -- JSON blob\n        associated_entity_ids TEXT, -- JSON array of code_entity_ids\n        conversation_id TEXT -- If related to a specific conversation\n      )`,\n\n      // Indexes for timeline_events\n      `CREATE INDEX IF NOT EXISTS idx_timeline_events_ts ON timeline_events(timestamp DESC)`,\n      `CREATE INDEX IF NOT EXISTS idx_timeline_events_type ON timeline_events(event_type)`,\n\n      // ========= CONTEXT SNAPSHOTS =========\n      `CREATE TABLE IF NOT EXISTS context_snapshots (\n        snapshot_id TEXT PRIMARY KEY, -- UUID\n        name TEXT,\n        description TEXT,\n        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n        timeline_event_id TEXT,\n        snapshot_data TEXT NOT NULL, -- JSON blob\n        FOREIGN KEY (timeline_event_id) REFERENCES timeline_events(event_id) ON DELETE SET NULL\n      )`,\n\n      // Indexes for context_snapshots\n      `CREATE INDEX IF NOT EXISTS idx_context_snapshots_name ON context_snapshots(name)`,\n\n      // ========= FOCUS AREAS =========\n      `CREATE TABLE IF NOT EXISTS focus_areas (\n        focus_id TEXT PRIMARY KEY, -- UUID\n        focus_type TEXT NOT NULL,\n        identifier TEXT,\n        description TEXT,\n        related_entity_ids TEXT, -- JSON array of code_entity_ids\n        keywords TEXT, -- JSON array of defining keywords\n        last_activated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n        is_active BOOLEAN DEFAULT FALSE\n      )`,\n\n      // Indexes for focus_areas\n      `CREATE INDEX IF NOT EXISTS idx_focus_areas_active ON focus_areas(is_active, last_activated_at DESC)`,\n\n      // ========= PROJECT PATTERNS =========\n      `CREATE TABLE IF NOT EXISTS project_patterns (\n        pattern_id TEXT PRIMARY KEY, -- UUID\n        pattern_type TEXT NOT NULL,\n        name TEXT,\n        description TEXT,\n        representation TEXT NOT NULL, -- JSON or textual\n        detection_rules TEXT,\n        frequency INTEGER DEFAULT 0,\n        last_detected_at TIMESTAMP,\n        utility_score REAL DEFAULT 0.0,\n        confidence_score REAL DEFAULT 0.5, -- confidence in pattern validity\n        reinforcement_count INTEGER DEFAULT 1, -- times pattern was reinforced\n        is_global BOOLEAN DEFAULT FALSE, -- indicates if promoted to global status\n        session_origin_id TEXT, -- originating session if any\n        language TEXT, -- NEW COLUMN: programming language the pattern applies to\n        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n      )`,\n\n      // Indexes for project_patterns\n      `CREATE INDEX IF NOT EXISTS idx_project_patterns_type ON project_patterns(pattern_type)`,\n      `CREATE INDEX IF NOT EXISTS idx_project_patterns_global ON project_patterns(is_global, confidence_score DESC)`,\n      `CREATE INDEX IF NOT EXISTS idx_project_patterns_utility ON project_patterns(utility_score DESC)`,\n\n      // ========= PATTERN OBSERVATIONS =========\n      `CREATE TABLE IF NOT EXISTS pattern_observations (\n        observation_id TEXT PRIMARY KEY, -- UUID\n        pattern_id TEXT NOT NULL,\n        conversation_id TEXT,\n        context_entities TEXT, -- JSON array of entity_ids\n        observation_type TEXT NOT NULL, -- 'usage', 'confirmation', 'rejection'\n        observation_data TEXT, -- JSON with details\n        timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n        FOREIGN KEY (pattern_id) REFERENCES project_patterns(pattern_id) ON DELETE CASCADE\n      )`,\n\n      // Indexes for pattern_observations\n      `CREATE INDEX IF NOT EXISTS idx_pattern_observations_pattern ON pattern_observations(pattern_id)`,\n      `CREATE INDEX IF NOT EXISTS idx_pattern_observations_type_ts ON pattern_observations(observation_type, timestamp DESC)`,\n\n      // ========= SYSTEM LOGS =========\n      `CREATE TABLE IF NOT EXISTS system_logs (\n        log_id INTEGER PRIMARY KEY AUTOINCREMENT,\n        timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n        level TEXT NOT NULL, -- 'INFO', 'WARN', 'ERROR', 'DEBUG'\n        message TEXT NOT NULL,\n        data TEXT -- Optional JSON blob\n      )`,\n\n      // Indexes for system_logs\n      `CREATE INDEX IF NOT EXISTS idx_system_logs_timestamp_level ON system_logs(timestamp DESC, level)`,\n    ];\n\n    // Execute each statement in sequence\n    for (const statement of schemaStatements) {\n      try {\n        await client.execute(statement);\n      } catch (error) {\n        // Log error but continue with other statements\n        logMessage(\n          \"error\",\n          `Error executing schema statement: ${error.message}`\n        );\n        logMessage(\n          \"error\",\n          `Failed statement: ${statement.substring(0, 100)}...`\n        );\n        success = false;\n      }\n    }\n\n    return success;\n  } catch (error) {\n    throw new Error(`Database schema initialization failed: ${error.message}`);\n  }\n};\n\nexport default {\n  getDbClient,\n  testDbConnection,\n  executeQuery,\n  initializeDatabaseSchema,\n};\n", "/**\n * RelationshipContextManagerLogic.js\n *\n * Provides functions for managing relationships between code entities.\n */\n\nimport { v4 as uuidv4 } from \"uuid\";\nimport { executeQuery } from \"../db.js\";\n\n/**\n * Adds a relationship between two code entities\n *\n * @param {string} sourceEntityId - ID of the source entity\n * @param {string} targetEntityId - ID of the target entity\n * @param {string} relationshipType - Type of relationship (e.g., 'calls', 'imports', 'extends')\n * @param {number} weight - Weight of the relationship (default: 1.0)\n * @param {object} metadata - Additional metadata about the relationship\n * @returns {Promise<void>}\n */\nexport async function addRelationship(\n  sourceEntityId,\n  targetEntityId,\n  relationshipType,\n  weight = 1.0,\n  metadata = {}\n) {\n  // Validate required parameters\n  if (!sourceEntityId || !targetEntityId || !relationshipType) {\n    throw new Error(\n      \"Source entity ID, target entity ID, and relationship type are required\"\n    );\n  }\n\n  // Generate a new UUID for the relationship\n  const relationshipId = uuidv4();\n\n  // Convert metadata object to JSON string\n  const metadataJson = JSON.stringify(metadata);\n\n  try {\n    // Insert the relationship into the database\n    const query = `\n      INSERT INTO code_relationships (\n        relationship_id, source_entity_id, target_entity_id, relationship_type, weight, metadata\n      ) VALUES (?, ?, ?, ?, ?, ?)\n    `;\n\n    await executeQuery(query, [\n      relationshipId,\n      sourceEntityId,\n      targetEntityId,\n      relationshipType,\n      weight,\n      metadataJson,\n    ]);\n  } catch (error) {\n    // Check if error is due to unique constraint violation\n    if (error.message && error.message.includes(\"UNIQUE constraint failed\")) {\n      // If duplicate, we'll update the existing relationship\n      const updateQuery = `\n        UPDATE code_relationships \n        SET weight = ?, metadata = ? \n        WHERE source_entity_id = ? AND target_entity_id = ? AND relationship_type = ?\n      `;\n\n      await executeQuery(updateQuery, [\n        weight,\n        metadataJson,\n        sourceEntityId,\n        targetEntityId,\n        relationshipType,\n      ]);\n    } else {\n      // For other errors, rethrow\n      console.error(\n        `Error adding relationship between ${sourceEntityId} and ${targetEntityId}:`,\n        error\n      );\n      throw error;\n    }\n  }\n}\n\n/**\n * Relationship type definition matching code_relationships table structure\n * @typedef {Object} Relationship\n * @property {string} relationship_id - Unique identifier for the relationship\n * @property {string} source_entity_id - ID of the source entity\n * @property {string} target_entity_id - ID of the target entity\n * @property {string} relationship_type - Type of relationship\n * @property {number} weight - Weight of the relationship\n * @property {Object} metadata - Additional metadata about the relationship\n */\n\n/**\n * Gets relationships for a specific entity\n *\n * @param {string} entityId - ID of the entity to get relationships for\n * @param {string} direction - Direction of relationships to get ('outgoing', 'incoming', or 'both')\n * @param {string[]} types - Types of relationships to filter by (empty array for all types)\n * @returns {Promise<Relationship[]>} Array of relationship objects\n */\nexport async function getRelationships(\n  entityId,\n  direction = \"outgoing\",\n  types = []\n) {\n  // Validate required parameters\n  if (!entityId) {\n    throw new Error(\"Entity ID is required\");\n  }\n\n  // Validate direction parameter\n  if (![\"outgoing\", \"incoming\", \"both\"].includes(direction)) {\n    throw new Error(\"Direction must be 'outgoing', 'incoming', or 'both'\");\n  }\n\n  // Build the base query\n  let query = `\n    SELECT \n      relationship_id, \n      source_entity_id, \n      target_entity_id, \n      relationship_type, \n      weight, \n      metadata\n    FROM code_relationships\n    WHERE \n  `;\n\n  const queryParams = [];\n\n  // Add direction-specific conditions\n  if (direction === \"outgoing\") {\n    query += \"source_entity_id = ?\";\n    queryParams.push(entityId);\n  } else if (direction === \"incoming\") {\n    query += \"target_entity_id = ?\";\n    queryParams.push(entityId);\n  } else {\n    // direction === \"both\"\n    query += \"(source_entity_id = ? OR target_entity_id = ?)\";\n    queryParams.push(entityId, entityId);\n  }\n\n  // Add relationship type filter if provided\n  if (types.length > 0) {\n    // Create placeholders for the IN clause\n    const typePlaceholders = types.map(() => \"?\").join(\", \");\n    query += ` AND relationship_type IN (${typePlaceholders})`;\n    queryParams.push(...types);\n  }\n\n  try {\n    // Execute the query\n    const relationships = await executeQuery(query, queryParams);\n\n    // Process metadata for each relationship\n    return relationships.map((relationship) => ({\n      ...relationship,\n      // Parse metadata JSON string to object, default to empty object if null or invalid\n      metadata: relationship.metadata ? JSON.parse(relationship.metadata) : {},\n    }));\n  } catch (error) {\n    console.error(`Error getting relationships for entity ${entityId}:`, error);\n    throw error;\n  }\n}\n\n/**\n * GraphSnippet type definition for call graph data\n * @typedef {Object} GraphSnippet\n * @property {Array<{id: string, name: string, type: string}>} nodes - Entities in the graph\n * @property {Array<{source: string, target: string, type: string}>} edges - Relationships between entities\n */\n\n/**\n * Builds a call graph snippet starting from a function entity\n *\n * @param {string} functionEntityId - ID of the function entity to start from\n * @param {number} depth - Maximum depth of the call graph (default: 2)\n * @returns {Promise<GraphSnippet>} Call graph snippet with nodes and edges\n */\nexport async function buildCallGraphSnippet(functionEntityId, depth = 2) {\n  // Validate required parameters\n  if (!functionEntityId) {\n    throw new Error(\"Function entity ID is required\");\n  }\n\n  // Validate depth\n  if (depth < 1) {\n    throw new Error(\"Depth must be at least 1\");\n  }\n\n  try {\n    // Use a recursive Common Table Expression (CTE) to get function calls up to specified depth\n    const outgoingCallsQuery = `\n      WITH RECURSIVE call_graph AS (\n        -- Base case: start with the source function\n        SELECT \n          cr.source_entity_id, \n          cr.target_entity_id, \n          cr.relationship_type,\n          0 AS depth\n        FROM code_relationships cr\n        WHERE cr.source_entity_id = ? \n          AND cr.relationship_type = 'calls'\n        \n        UNION ALL\n        \n        -- Recursive case: find further calls up to max depth\n        SELECT \n          cr.source_entity_id, \n          cr.target_entity_id, \n          cr.relationship_type,\n          cg.depth + 1 AS depth\n        FROM code_relationships cr\n        JOIN call_graph cg ON cr.source_entity_id = cg.target_entity_id\n        WHERE cr.relationship_type = 'calls'\n          AND cg.depth < ?\n      )\n      SELECT DISTINCT source_entity_id, target_entity_id, relationship_type, depth\n      FROM call_graph\n      ORDER BY depth\n    `;\n\n    const outgoingCalls = await executeQuery(outgoingCallsQuery, [\n      functionEntityId,\n      depth - 1,\n    ]);\n\n    // Get incoming calls (functions that call our target function)\n    const incomingCallsQuery = `\n      SELECT \n        cr.source_entity_id, \n        cr.target_entity_id, \n        cr.relationship_type,\n        0 AS depth\n      FROM code_relationships cr\n      WHERE cr.target_entity_id = ? \n        AND cr.relationship_type = 'calls'\n    `;\n\n    const incomingCalls = await executeQuery(incomingCallsQuery, [\n      functionEntityId,\n    ]);\n\n    // Combine outgoing and incoming calls\n    const allCalls = [...outgoingCalls, ...incomingCalls];\n\n    // Extract all unique entity IDs involved\n    const entityIds = new Set();\n    entityIds.add(functionEntityId); // Add the root function\n\n    allCalls.forEach((call) => {\n      entityIds.add(call.source_entity_id);\n      entityIds.add(call.target_entity_id);\n    });\n\n    // Get entity details for all involved entities\n    const entityIdsArray = Array.from(entityIds);\n    const placeholders = entityIdsArray.map(() => \"?\").join(\",\");\n\n    const entitiesQuery = `\n      SELECT \n        id, \n        name, \n        type\n      FROM code_entities\n      WHERE id IN (${placeholders})\n    `;\n\n    const entities = await executeQuery(entitiesQuery, entityIdsArray);\n\n    // Build the graph nodes\n    const nodes = entities.map((entity) => ({\n      id: entity.id,\n      name: entity.name,\n      type: entity.type,\n    }));\n\n    // Build the graph edges\n    const edges = allCalls.map((call) => ({\n      source: call.source_entity_id,\n      target: call.target_entity_id,\n      type: call.relationship_type,\n    }));\n\n    // Return the call graph snippet\n    return {\n      nodes,\n      edges,\n    };\n  } catch (error) {\n    console.error(\n      `Error building call graph for function ${functionEntityId}:`,\n      error\n    );\n    throw error;\n  }\n}\n\n/**\n * Path type definition for code paths\n * @typedef {string[]} Path - An array of entity IDs representing a path\n */\n\n/**\n * Finds all paths between two entities with a specific relationship type\n *\n * @param {string} startEntityId - ID of the starting entity\n * @param {string} endEntityId - ID of the ending entity\n * @param {string} relationshipType - Type of relationship to follow\n * @returns {Promise<Path[]>} Array of paths (each path is an array of entity IDs)\n */\nexport async function findCodePaths(\n  startEntityId,\n  endEntityId,\n  relationshipType\n) {\n  // Validate required parameters\n  if (!startEntityId || !endEntityId || !relationshipType) {\n    throw new Error(\n      \"Start entity ID, end entity ID, and relationship type are required\"\n    );\n  }\n\n  try {\n    // Use a recursive CTE to find all paths\n    const query = `\n      WITH RECURSIVE paths(path, current_id, visited) AS (\n        -- Base case: start with the starting entity\n        SELECT \n          startEntityId || '', -- Initialize path with just the start entity\n          startEntityId,\n          startEntityId -- Initialize visited set with start entity\n        FROM (SELECT ? AS startEntityId)\n        \n        UNION ALL\n        \n        -- Recursive case: extend paths that haven't reached the end entity\n        SELECT\n          paths.path || ',' || cr.target_entity_id, -- Append target to path\n          cr.target_entity_id, -- New current entity is the target\n          paths.visited || ',' || cr.target_entity_id -- Update visited set\n        FROM\n          code_relationships cr\n        JOIN\n          paths ON cr.source_entity_id = paths.current_id\n        WHERE\n          cr.relationship_type = ?\n          AND cr.target_entity_id != paths.startEntityId -- Avoid immediate cycles back to start\n          AND paths.visited NOT LIKE '%,' || cr.target_entity_id || ',%' -- Check for cycles\n          AND paths.visited NOT LIKE cr.target_entity_id || ',%' -- Check for cycles at start\n          AND paths.visited NOT LIKE '%,' || cr.target_entity_id -- Check for cycles at end\n      )\n      -- Select paths that end at the target entity\n      SELECT path\n      FROM paths\n      WHERE current_id = ?\n    `;\n\n    const results = await executeQuery(query, [\n      startEntityId,\n      relationshipType,\n      endEntityId,\n    ]);\n\n    // Process the results into an array of paths\n    return results.map((row) => {\n      // Split the path string into an array of entity IDs\n      return row.path.split(\",\");\n    });\n  } catch (error) {\n    console.error(\n      `Error finding paths between ${startEntityId} and ${endEntityId}:`,\n      error\n    );\n\n    // SQLite might not fully support the recursive CTE with the cycle detection as written\n    // If we get an error, let's use a more basic approach that has limited depth\n\n    try {\n      // Fallback to a simpler implementation with finite depth\n      const maxDepth = 10; // Reasonable limit to prevent excessive path lengths\n\n      const fallbackQuery = `\n        WITH RECURSIVE paths(path, current_id, depth) AS (\n          -- Base case: start with the starting entity\n          SELECT \n            ? AS path,\n            ? AS current_id,\n            0 AS depth\n          \n          UNION ALL\n          \n          -- Recursive case: extend paths that haven't reached the end entity\n          SELECT\n            paths.path || ',' || cr.target_entity_id,\n            cr.target_entity_id,\n            paths.depth + 1\n          FROM\n            code_relationships cr\n          JOIN\n            paths ON cr.source_entity_id = paths.current_id\n          WHERE\n            cr.relationship_type = ?\n            AND paths.depth < ?\n            AND paths.path NOT LIKE '%' || cr.target_entity_id || '%' -- Simple cycle check\n        )\n        -- Select paths that end at the target entity\n        SELECT path\n        FROM paths\n        WHERE current_id = ?\n      `;\n\n      const fallbackResults = await executeQuery(fallbackQuery, [\n        startEntityId,\n        startEntityId,\n        relationshipType,\n        maxDepth,\n        endEntityId,\n      ]);\n\n      // Process the fallback results\n      return fallbackResults.map((row) => {\n        return row.path.split(\",\");\n      });\n    } catch (fallbackError) {\n      console.error(\n        \"Fallback path finding approach also failed:\",\n        fallbackError\n      );\n\n      // If all else fails, return an empty array\n      return [];\n    }\n  }\n}\n\n/**\n * Gets entities related to a given entity\n *\n * @param {string} entityId - ID of the entity to get related entities for\n * @param {string[]} [relationshipTypes=[]] - Types of relationships to filter by (empty array for all types)\n * @param {number} [maxResults=20] - Maximum number of results to return\n * @returns {Promise<string[]>} Array of related entity IDs\n */\nexport async function getRelatedEntities(\n  entityId,\n  relationshipTypes = [],\n  maxResults = 20\n) {\n  // Validate required parameters\n  if (!entityId) {\n    throw new Error(\"Entity ID is required\");\n  }\n\n  try {\n    // Get both incoming and outgoing relationships\n    const relationships = await getRelationships(\n      entityId,\n      \"both\",\n      relationshipTypes\n    );\n\n    // Extract unique entity IDs from relationships\n    const relatedEntityIds = new Set();\n\n    for (const relationship of relationships) {\n      if (relationship.source_entity_id === entityId) {\n        relatedEntityIds.add(relationship.target_entity_id);\n      } else {\n        relatedEntityIds.add(relationship.source_entity_id);\n      }\n\n      // Stop if we've reached the maximum number of results\n      if (relatedEntityIds.size >= maxResults) {\n        break;\n      }\n    }\n\n    return Array.from(relatedEntityIds);\n  } catch (error) {\n    console.error(`Error getting related entities for ${entityId}:`, error);\n    return [];\n  }\n}\n", "\"use strict\";\n\n/**\n * main.js\n *\n * Main entry point for the MCP server.\n * Initializes the database connection and starts the MCP server.\n */\n\nimport { McpServer } from \"@modelcontextprotocol/sdk/server/mcp.js\";\nimport { StdioServerTransport } from \"@modelcontextprotocol/sdk/server/stdio.js\";\nimport { TURSO_DATABASE_URL, TURSO_AUTH_TOKEN } from \"./config.js\";\nimport {\n  testDbConnection,\n  initializeDatabaseSchema,\n  getDbClient,\n} from \"./db.js\";\nimport { logMessage } from \"./utils/logger.js\";\nimport allTools from \"./tools/index.js\";\nimport {\n  createToolHandler,\n  createInitializeContextHandler,\n  createFinalizeContextHandler,\n} from \"./tools/mcpDevContextTools.js\";\n\n/**\n * Start the MCP server\n * Initializes database and listens for MCP requests\n */\nasync function startServer() {\n  // Check if database credentials are set\n  if (!TURSO_DATABASE_URL || !TURSO_AUTH_TOKEN) {\n    logMessage(\n      \"error\",\n      \"Database credentials not set. TURSO_DATABASE_URL and TURSO_AUTH_TOKEN are required.\"\n    );\n    process.exit(1);\n  }\n\n  // Get database client\n  try {\n    logMessage(\"info\", \"Getting database client...\");\n    const dbClient = getDbClient();\n    logMessage(\"info\", \"Database client created successfully.\");\n  } catch (error) {\n    logMessage(\"error\", `Failed to create database client: ${error.message}`);\n    process.exit(1);\n  }\n\n  // Test database connection\n  try {\n    logMessage(\"info\", \"Testing database connection...\");\n    await testDbConnection();\n    logMessage(\"info\", \"Database connection successful.\");\n  } catch (error) {\n    logMessage(\"error\", `Database connection failed: ${error.message}`);\n    process.exit(1);\n  }\n\n  // Initialize database schema\n  try {\n    logMessage(\"info\", \"Initializing database schema...\");\n    await initializeDatabaseSchema();\n    logMessage(\"info\", \"Database schema initialized successfully.\");\n  } catch (error) {\n    logMessage(\n      \"error\",\n      `Failed to initialize database schema: ${error.message}`\n    );\n    process.exit(1);\n  }\n\n  // Create and initialize the MCP server\n  const server = new McpServer({\n    name: \"cursor10x\",\n    version: \"2.0.0\",\n  });\n\n  // Register all tools with appropriate wrappers\n  for (const tool of allTools) {\n    let wrappedHandler;\n\n    // Use specialized handlers for initialize and finalize context tools\n    if (tool.name === \"initialize_conversation_context\") {\n      wrappedHandler = createInitializeContextHandler(tool.handler);\n    } else if (tool.name === \"finalize_conversation_context\") {\n      wrappedHandler = createFinalizeContextHandler(tool.handler);\n    } else {\n      // Use general handler for other tools\n      wrappedHandler = createToolHandler(tool.handler, tool.name);\n    }\n\n    // Register the tool with the wrapped handler\n    server.tool(tool.name, tool.inputSchema, wrappedHandler);\n    logMessage(\"info\", `Registered tool: ${tool.name}`);\n  }\n\n  const transport = new StdioServerTransport();\n  logMessage(\"info\", `Starting MCP server with PID ${process.pid}...`);\n\n  try {\n    await server.connect(transport);\n    logMessage(\"info\", \"MCP server stopped.\");\n  } catch (error) {\n    logMessage(\"error\", `MCP server error: ${error.message}`);\n    process.exit(1);\n  }\n}\n\n// Run the server unless this file is being required as a module\nif (\n  import.meta.url === import.meta.mainUrl ||\n  process.env.NODE_ENV !== \"test\"\n) {\n  startServer().catch((error) => {\n    logMessage(\"error\", `Unhandled error in startServer: ${error.message}`);\n    console.error(error);\n    process.exit(1);\n  });\n}\n\nexport { startServer };\n", "/**\n * initializeConversationContext.tool.js\n *\n * MCP tool implementation for initializing conversation context\n * This tool gathers comprehensive context about the codebase and project for a new conversation\n */\n\nimport { z } from \"zod\";\nimport { v4 as uuidv4 } from \"uuid\";\nimport { executeQuery } from \"../db.js\";\nimport * as ConversationIntelligence from \"../logic/ConversationIntelligence.js\";\nimport * as ContextCompressorLogic from \"../logic/ContextCompressorLogic.js\";\nimport * as IntentPredictorLogic from \"../logic/IntentPredictorLogic.js\";\nimport * as SmartSearchServiceLogic from \"../logic/SmartSearchServiceLogic.js\";\nimport * as ActiveContextManager from \"../logic/ActiveContextManager.js\";\nimport * as TimelineManagerLogic from \"../logic/TimelineManagerLogic.js\";\nimport * as GlobalPatternRepository from \"../logic/GlobalPatternRepository.js\";\nimport {\n  initializeConversationContextInputSchema,\n  initializeConversationContextOutputSchema,\n} from \"../schemas/toolSchemas.js\";\nimport { logMessage } from \"../utils/logger.js\";\n\n/**\n * Handler for initialize_conversation_context tool\n *\n * @param {object} input - Tool input parameters\n * @param {object} sdkContext - SDK context\n * @returns {Promise<object>} Tool output\n */\nasync function handler(input, sdkContext) {\n  try {\n    logMessage(\"INFO\", `initialize_conversation_context tool started`, {\n      initialQuery: input.initialQuery,\n    });\n\n    // 1. Generate conversation ID if not provided\n    const conversationId = input.conversationId || uuidv4();\n    logMessage(\"DEBUG\", `Using conversation ID: ${conversationId}`);\n\n    // 2. Extract input parameters with defaults\n    const {\n      initialQuery = \"\",\n      focusHint,\n      includeArchitecture = true,\n      includeRecentConversations = true,\n      maxCodeContextItems = 5,\n      maxRecentChanges = 5,\n      contextDepth = \"standard\",\n      tokenBudget = 4000,\n    } = input;\n\n    // 3. Clear any active context and set initial focus if provided\n    try {\n      await ActiveContextManager.clearActiveContext();\n      if (focusHint) {\n        await ActiveContextManager.setActiveFocus(\n          focusHint.type,\n          focusHint.identifier\n        );\n        logMessage(\"INFO\", `Set initial focus`, {\n          type: focusHint.type,\n          identifier: focusHint.identifier,\n        });\n      }\n    } catch (err) {\n      logMessage(\n        \"WARN\",\n        `Failed to set initial focus, continuing with initialization`,\n        {\n          error: err.message,\n          focusHint,\n        }\n      );\n      // Continue with initialization despite focus setting error\n    }\n\n    // 4. Record the conversation start in timeline\n    try {\n      await TimelineManagerLogic.recordEvent(\n        \"conversation_started\",\n        {\n          initialQuery,\n          focusHint,\n          contextDepth,\n        },\n        [], // No associated entity IDs yet\n        conversationId\n      );\n      logMessage(\"DEBUG\", `Recorded conversation start in timeline`, {\n        conversationId,\n      });\n    } catch (err) {\n      // Non-critical failure, log but continue\n      logMessage(\"WARN\", `Failed to record conversation start in timeline`, {\n        error: err.message,\n        conversationId,\n      });\n    }\n\n    // 5. Initialize conversation intelligence tracker\n    try {\n      await ConversationIntelligence.initializeConversation(\n        conversationId,\n        initialQuery\n      );\n      logMessage(\"DEBUG\", `Initialized conversation intelligence tracker`, {\n        conversationId,\n      });\n    } catch (err) {\n      logMessage(\"ERROR\", `Failed to initialize conversation intelligence`, {\n        error: err.message,\n        conversationId,\n      });\n      throw new Error(\n        `Conversation intelligence initialization failed: ${err.message}`\n      );\n    }\n\n    // 6. Predict initial intent based on query\n    let predictedIntent = \"\";\n    if (initialQuery) {\n      try {\n        const intentResult = await IntentPredictorLogic.inferIntentFromQuery(\n          initialQuery\n        );\n        predictedIntent = intentResult.intent;\n        logMessage(\"INFO\", `Predicted initial intent`, {\n          intent: predictedIntent,\n          confidence: intentResult.confidence || \"N/A\",\n        });\n      } catch (err) {\n        // Non-critical failure, log but continue with empty intent\n        logMessage(\n          \"WARN\",\n          `Intent prediction failed, continuing without intent`,\n          {\n            error: err.message,\n            initialQuery,\n          }\n        );\n      }\n    }\n\n    // 7. Gather comprehensive context\n    logMessage(\"INFO\", `Starting comprehensive context gathering`, {\n      conversationId,\n      includeArchitecture,\n      maxCodeContextItems,\n      contextDepth,\n    });\n\n    const comprehensiveContext = await gatherComprehensiveContext(\n      initialQuery,\n      focusHint,\n      conversationId,\n      {\n        includeArchitecture,\n        includeRecentConversations,\n        maxCodeContextItems,\n        maxRecentChanges,\n        contextDepth,\n        tokenBudget,\n      }\n    );\n\n    const contextCounts = {\n      codeContextItems: comprehensiveContext.codeContext?.length || 0,\n      architectureItems: comprehensiveContext.architectureContext?.length || 0,\n      recentChanges: comprehensiveContext.recentChanges?.length || 0,\n      patterns: comprehensiveContext.globalPatterns?.length || 0,\n    };\n\n    logMessage(\n      \"INFO\",\n      `Comprehensive context gathered successfully`,\n      contextCounts\n    );\n\n    // 8. Generate initial context summary\n    const initialContextSummary = generateInitialContextSummary(\n      comprehensiveContext,\n      initialQuery,\n      predictedIntent\n    );\n\n    logMessage(\"INFO\", `Generated initial context summary`, {\n      summaryLength: initialContextSummary?.length || 0,\n    });\n\n    // 9. Return the tool response with all gathered context\n    const responseData = {\n      message: `Conversation context initialized with ID: ${conversationId}`,\n      conversationId,\n      initialContextSummary,\n      predictedIntent,\n      comprehensiveContext,\n    };\n\n    return {\n      content: [\n        {\n          type: \"text\",\n          text: JSON.stringify(responseData),\n        },\n      ],\n    };\n  } catch (error) {\n    // Log detailed error information\n    logMessage(\"ERROR\", `Error in initialize_conversation_context tool`, {\n      error: error.message,\n      stack: error.stack,\n      input: {\n        initialQuery: input.initialQuery,\n        focusHint: input.focusHint,\n        contextDepth: input.contextDepth,\n      },\n    });\n\n    // Return error response\n    const errorResponse = {\n      error: true,\n      errorCode: error.code || \"INITIALIZATION_FAILED\",\n      errorDetails: error.message,\n    };\n\n    return {\n      content: [\n        {\n          type: \"text\",\n          text: JSON.stringify(errorResponse),\n        },\n      ],\n    };\n  }\n}\n\n/**\n * Gathers comprehensive context about the codebase and project\n *\n * @param {string} initialQuery - Initial user query\n * @param {object} focusHint - Focus hint information\n * @param {string} conversationId - Conversation ID\n * @param {object} options - Context gathering options\n * @returns {Promise<object>} Comprehensive context object\n */\nasync function gatherComprehensiveContext(\n  initialQuery,\n  focusHint,\n  conversationId,\n  options\n) {\n  const context = {};\n\n  try {\n    logMessage(\"DEBUG\", `Starting to gather code context`, {\n      initialQuery: initialQuery?.substring(0, 50),\n      focusHint,\n    });\n\n    // 1. Gather code context based on query and/or focus\n    context.codeContext = await gatherCodeContext(\n      initialQuery,\n      focusHint,\n      options\n    );\n\n    logMessage(\"DEBUG\", `Gathered code context`, {\n      itemCount: context.codeContext?.length || 0,\n    });\n\n    // 2. Gather architecture context (READMEs, docs)\n    if (options.includeArchitecture) {\n      try {\n        context.architectureContext = await gatherArchitectureContext(options);\n        logMessage(\"DEBUG\", `Gathered architecture context`, {\n          itemCount: context.architectureContext?.length || 0,\n        });\n      } catch (err) {\n        logMessage(\"WARN\", `Failed to gather architecture context`, {\n          error: err.message,\n        });\n        context.architectureContext = null;\n      }\n    } else {\n      context.architectureContext = null;\n    }\n\n    // 3. Get project structure overview\n    try {\n      context.projectStructure = await gatherProjectStructure();\n      logMessage(\"DEBUG\", `Gathered project structure`, {\n        directoryCount: context.projectStructure?.directories?.length || 0,\n        fileCount: context.projectStructure?.files?.length || 0,\n      });\n    } catch (err) {\n      logMessage(\"WARN\", `Failed to gather project structure`, {\n        error: err.message,\n      });\n      context.projectStructure = { directories: [], files: [] };\n    }\n\n    // 4. Get recent conversations if requested\n    if (options.includeRecentConversations) {\n      try {\n        context.recentConversations = await gatherRecentConversations(options);\n        logMessage(\"DEBUG\", `Gathered recent conversations`, {\n          count: context.recentConversations?.length || 0,\n        });\n      } catch (err) {\n        logMessage(\"WARN\", `Failed to gather recent conversations`, {\n          error: err.message,\n        });\n        context.recentConversations = [];\n      }\n    }\n\n    // 5. Get recent changes\n    try {\n      context.recentChanges = await gatherRecentChanges(\n        options.maxRecentChanges\n      );\n      logMessage(\"DEBUG\", `Gathered recent changes`, {\n        count: context.recentChanges?.length || 0,\n      });\n    } catch (err) {\n      logMessage(\"WARN\", `Failed to gather recent changes`, {\n        error: err.message,\n      });\n      context.recentChanges = [];\n    }\n\n    // 6. Get active workflows and milestones\n    try {\n      context.activeWorkflows = await gatherActiveWorkflows();\n      logMessage(\"DEBUG\", `Gathered active workflows`, {\n        count: context.activeWorkflows?.length || 0,\n      });\n    } catch (err) {\n      logMessage(\"WARN\", `Failed to gather active workflows`, {\n        error: err.message,\n      });\n      context.activeWorkflows = [];\n    }\n\n    // 7. Get relevant global patterns\n    try {\n      context.globalPatterns = await gatherGlobalPatterns(\n        initialQuery,\n        options\n      );\n      logMessage(\"DEBUG\", `Gathered global patterns`, {\n        count: context.globalPatterns?.length || 0,\n      });\n    } catch (err) {\n      logMessage(\"WARN\", `Failed to gather global patterns`, {\n        error: err.message,\n      });\n      context.globalPatterns = [];\n    }\n\n    return context;\n  } catch (error) {\n    logMessage(\"ERROR\", `Error gathering comprehensive context`, {\n      error: error.message,\n      conversationId,\n    });\n    throw error; // Re-throw to be caught by the main handler\n  }\n}\n\n/**\n * Gathers code context based on query and focus\n *\n * @param {string} query - User query\n * @param {object} focusHint - Focus hint\n * @param {object} options - Options\n * @returns {Promise<Array>} Code context items\n */\nasync function gatherCodeContext(query, focusHint, options) {\n  try {\n    // Create search constraints\n    const searchConstraints = {\n      limit: options.maxCodeContextItems * 2, // Get more than we need for filtering\n    };\n\n    // Add focus constraints if provided\n    if (focusHint) {\n      if (focusHint.type === \"file\" || focusHint.type === \"directory\") {\n        searchConstraints.filePaths = [focusHint.identifier];\n      }\n    }\n\n    // Extract keywords if query is provided, otherwise use basic terms\n    const searchTerms = query\n      ? await extractKeywords(query)\n      : [\"README\", \"main\", \"index\", \"config\"];\n\n    // Perform search\n    const searchResults = await SmartSearchServiceLogic.searchByKeywords(\n      searchTerms,\n      searchConstraints\n    );\n\n    // Process and compress search results\n    let codeItems = searchResults.map((result) => ({\n      entity_id: result.entity.entity_id,\n      path: result.entity.file_path,\n      type: result.entity.entity_type,\n      name: result.entity.name,\n      content: result.entity.raw_content,\n      relevanceScore: result.relevanceScore,\n    }));\n\n    // Limit to max items\n    codeItems = codeItems.slice(0, options.maxCodeContextItems);\n\n    // Apply compression based on context depth\n    const compressionOptions = {\n      detailLevel: options.contextDepth,\n      targetTokens: Math.floor(options.tokenBudget * 0.6), // Allocate 60% of token budget to code\n    };\n\n    const compressedItems = await ContextCompressorLogic.compressContext(\n      codeItems,\n      compressionOptions\n    );\n\n    return compressedItems;\n  } catch (error) {\n    console.error(`[gatherCodeContext] Error: ${error.message}`);\n    return [];\n  }\n}\n\n/**\n * Gathers architecture context information\n *\n * @param {object} options - Options\n * @returns {Promise<object>} Architecture context\n */\nasync function gatherArchitectureContext(options) {\n  try {\n    // Search for documentation files\n    const docSearchResults = await SmartSearchServiceLogic.searchByKeywords(\n      [\"README\", \"documentation\", \"architecture\", \"overview\", \"guide\", \"setup\"],\n      {\n        limit: 5,\n        strategy: \"keywords\",\n      }\n    );\n\n    if (docSearchResults.length === 0) {\n      return null;\n    }\n\n    // Extract documentation content\n    const docSources = docSearchResults.map((result) => ({\n      name: result.entity.name,\n      path: result.entity.file_path,\n    }));\n\n    // Combine and summarize documentation\n    const docContents = docSearchResults\n      .map((result) => result.entity.raw_content)\n      .join(\"\\n\\n\");\n\n    // Compress documentation based on context depth\n    const compressionOptions = {\n      detailLevel: options.contextDepth,\n      targetTokens: Math.floor(options.tokenBudget * 0.2), // Allocate 20% of token budget to architecture docs\n    };\n\n    // Generate summary\n    const summary =\n      docContents.length > 1000\n        ? docContents.substring(0, 1000) + \"...\" // Simple truncation for now\n        : docContents;\n\n    return {\n      summary,\n      sources: docSources,\n    };\n  } catch (error) {\n    console.error(`[gatherArchitectureContext] Error: ${error.message}`);\n    return null;\n  }\n}\n\n/**\n * Gathers project structure information\n *\n * @returns {Promise<object>} Project structure\n */\nasync function gatherProjectStructure() {\n  try {\n    // Query for directory structure\n    const dirQuery = `\n      SELECT \n        file_path,\n        COUNT(*) as file_count\n      FROM \n        code_entities\n      WHERE \n        entity_type = 'file'\n      GROUP BY \n        SUBSTR(file_path, 1, INSTR(file_path, '/'))\n      ORDER BY \n        file_count DESC\n      LIMIT 10\n    `;\n\n    const directories = await executeQuery(dirQuery);\n\n    // Check if directories has a rows property and it's an array\n    const rows =\n      directories && directories.rows && Array.isArray(directories.rows)\n        ? directories.rows\n        : Array.isArray(directories)\n        ? directories\n        : [];\n\n    // If no valid results, return basic structure\n    if (rows.length === 0) {\n      return {\n        topLevelDirs: [],\n        totalFiles: 0,\n      };\n    }\n\n    return {\n      topLevelDirs: rows.map((dir) => ({\n        path: dir.file_path.split(\"/\")[0],\n        fileCount: dir.file_count,\n      })),\n      totalFiles: rows.reduce((sum, dir) => sum + dir.file_count, 0),\n    };\n  } catch (error) {\n    console.error(`[gatherProjectStructure] Error: ${error.message}`);\n    return null;\n  }\n}\n\n/**\n * Gathers recent conversations for context\n *\n * @param {object} options - Options\n * @returns {Promise<Array>} Recent conversations\n */\nasync function gatherRecentConversations(options) {\n  try {\n    // Get recent conversation events from timeline\n    const recentConversationEvents = await TimelineManagerLogic.getEvents({\n      types: [\"conversation_completed\"],\n      limit: 3,\n    });\n\n    if (recentConversationEvents.length === 0) {\n      return [];\n    }\n\n    // Format conversation summaries\n    return recentConversationEvents.map((event) => ({\n      timestamp: event.timestamp,\n      summary: event.data.summary || \"Conversation completed\",\n      purpose: event.data.purpose || \"Unknown purpose\",\n    }));\n  } catch (error) {\n    console.error(`[gatherRecentConversations] Error: ${error.message}`);\n    return [];\n  }\n}\n\n/**\n * Gathers recent changes in the project\n *\n * @param {number} maxChanges - Maximum number of changes to retrieve\n * @returns {Promise<Array>} Recent changes\n */\nasync function gatherRecentChanges(maxChanges) {\n  try {\n    // Get recent file change events from timeline\n    const recentChangeEvents = await TimelineManagerLogic.getEvents({\n      types: [\"file_change\", \"file_create\", \"code_commit\"],\n      limit: maxChanges,\n    });\n\n    if (recentChangeEvents.length === 0) {\n      return [];\n    }\n\n    // Format change events\n    return recentChangeEvents.map((event) => ({\n      timestamp: event.timestamp,\n      files: event.data.files || [event.data.filePath || \"Unknown file\"],\n      summary: event.data.message || `${event.event_type} event occurred`,\n    }));\n  } catch (error) {\n    console.error(`[gatherRecentChanges] Error: ${error.message}`);\n    return [];\n  }\n}\n\n/**\n * Gathers active workflows and milestones\n *\n * @returns {Promise<Array>} Active workflows\n */\nasync function gatherActiveWorkflows() {\n  try {\n    // Get recent milestone events\n    const milestoneEvents = await TimelineManagerLogic.getEvents({\n      types: [\"milestone\"],\n      limit: 3,\n      includeMilestones: true,\n    });\n\n    if (milestoneEvents.length === 0) {\n      return [];\n    }\n\n    // Format milestones\n    return milestoneEvents.map((event) => ({\n      name: event.data.name || \"Unnamed milestone\",\n      description: event.data.description || \"No description provided\",\n      timestamp: event.timestamp,\n    }));\n  } catch (error) {\n    console.error(`[gatherActiveWorkflows] Error: ${error.message}`);\n    return [];\n  }\n}\n\n/**\n * Gathers global patterns relevant to the query\n *\n * @param {string} query - User query\n * @param {object} options - Options\n * @returns {Promise<Array>} Global patterns\n */\nasync function gatherGlobalPatterns(query, options) {\n  try {\n    // Get global patterns\n    const globalPatterns = await GlobalPatternRepository.retrieveGlobalPatterns(\n      {\n        minConfidence: 0.4,\n        limit: 5,\n      }\n    );\n\n    if (globalPatterns.length === 0) {\n      return [];\n    }\n\n    // Format patterns\n    return globalPatterns.map((pattern) => ({\n      name: pattern.name,\n      type: pattern.pattern_type,\n      description: pattern.description,\n      confidence: pattern.confidence_score,\n    }));\n  } catch (error) {\n    console.error(`[gatherGlobalPatterns] Error: ${error.message}`);\n    return [];\n  }\n}\n\n/**\n * Generates a summary of the initial context\n *\n * @param {object} context - Comprehensive context\n * @param {string} query - Initial query\n * @param {string} intent - Predicted intent\n * @returns {string} Context summary\n */\nfunction generateInitialContextSummary(context, query, intent) {\n  // Create initial summary\n  let summary = \"Project context initialized\";\n\n  // Add query info if present\n  if (query) {\n    summary += ` for query: \"${query}\"`;\n  }\n\n  // Add intent if predicted\n  if (intent) {\n    summary += ` with intent: ${intent}`;\n  }\n\n  // Count code context items\n  if (context.codeContext && context.codeContext.length > 0) {\n    summary += `. Found ${context.codeContext.length} relevant code items`;\n  }\n\n  // Add architecture context if present\n  if (context.architectureContext) {\n    summary += \". Project documentation available\";\n  }\n\n  // Add recent activity info\n  if (context.recentChanges && context.recentChanges.length > 0) {\n    summary += `. ${context.recentChanges.length} recent file changes detected`;\n  }\n\n  // Add pattern info\n  if (context.globalPatterns && context.globalPatterns.length > 0) {\n    summary += `. ${context.globalPatterns.length} relevant patterns identified`;\n  }\n\n  return summary;\n}\n\n/**\n * Extract keywords from text\n *\n * @param {string} text - Text to extract keywords from\n * @returns {Promise<Array>} Keywords\n */\nasync function extractKeywords(text) {\n  // Simple keyword extraction (in production would call TextTokenizerLogic)\n  return text\n    .toLowerCase()\n    .replace(/[^\\w\\s]/g, \"\")\n    .split(/\\s+/)\n    .filter((word) => word.length > 2)\n    .filter((word) => ![\"the\", \"and\", \"for\", \"with\"].includes(word));\n}\n\n// Export the tool definition for server registration\nexport default {\n  name: \"initialize_conversation_context\",\n  description:\n    \"Initializes a new conversation context with comprehensive codebase information\",\n  inputSchema: initializeConversationContextInputSchema,\n  outputSchema: initializeConversationContextOutputSchema,\n  handler,\n};\n", "/**\n * ConversationIntelligence.js\n *\n * Provides logic for recording and analyzing conversation messages,\n * including semantic markers and sentiment indicators.\n */\n\nimport { executeQuery } from \"../db.js\";\nimport { v4 as uuidv4 } from \"uuid\";\nimport * as TextTokenizerLogic from \"./TextTokenizerLogic.js\";\nimport * as ContextIndexerLogic from \"./ContextIndexerLogic.js\";\nimport * as ConversationSegmenter from \"./ConversationSegmenter.js\";\nimport * as ConversationPurposeDetector from \"./ConversationPurposeDetector.js\";\nimport * as ContextCompressorLogic from \"./ContextCompressorLogic.js\";\n\n/**\n * Records a message in the conversation history, extracting semantic markers and sentiment indicators.\n *\n * @param {string} messageContent - The content of the message\n * @param {string} role - The role of the sender (e.g., 'user', 'assistant')\n * @param {string} conversationId - The conversation ID\n * @param {string[]} [relatedContextEntityIds=[]] - Array of related context entity IDs\n * @param {string} [topicSegmentId] - Optional topic segment ID\n * @returns {Promise<string>} The ID of the recorded message\n */\nexport async function recordMessage(\n  messageContent,\n  role,\n  conversationId,\n  relatedContextEntityIds = [],\n  topicSegmentId\n) {\n  try {\n    // 1. Generate message_id\n    const message_id = uuidv4();\n    const timestamp = new Date().toISOString();\n\n    // Print detailed input parameters for debugging\n    console.log(\"===== RECORD MESSAGE - START =====\");\n    console.log(\"Input parameters:\");\n    console.log(\"- message_id:\", message_id);\n    console.log(\"- conversation_id:\", conversationId);\n    console.log(\"- role:\", role);\n    console.log(\n      \"- content:\",\n      messageContent &&\n        messageContent.substring(0, 50) +\n          (messageContent.length > 50 ? \"...\" : \"\")\n    );\n    console.log(\"- timestamp:\", timestamp);\n    console.log(\"- topic_segment_id:\", topicSegmentId || \"null\");\n    console.log(\n      \"- related_context_entity_ids:\",\n      JSON.stringify(relatedContextEntityIds || [])\n    );\n\n    // 2. Extract semantic markers (e.g., idioms, emphasis, question, etc.)\n    let semantic_markers = [];\n    if (role === \"user\" && TextTokenizerLogic.identifyLanguageSpecificIdioms) {\n      // Use a function if available, otherwise fallback to keyword spotting\n      semantic_markers =\n        TextTokenizerLogic.identifyLanguageSpecificIdioms(\n          messageContent,\n          \"plaintext\"\n        ) || [];\n    } else {\n      // Fallback: simple keyword spotting for emphasis/questions\n      if (messageContent.includes(\"!\")) semantic_markers.push(\"emphasis\");\n      if (messageContent.includes(\"?\")) semantic_markers.push(\"question\");\n    }\n\n    // 3. Extract sentiment indicators (basic regex for positive/negative keywords)\n    const positiveKeywords = [\n      \"great\",\n      \"good\",\n      \"excellent\",\n      \"awesome\",\n      \"love\",\n      \"like\",\n      \"well done\",\n      \"thanks\",\n      \"thank you\",\n      \"perfect\",\n      \"amazing\",\n      \"fantastic\",\n      \"nice\",\n      \"happy\",\n      \"success\",\n      \"yay\",\n    ];\n    const negativeKeywords = [\n      \"bad\",\n      \"error\",\n      \"fail\",\n      \"hate\",\n      \"problem\",\n      \"issue\",\n      \"bug\",\n      \"broken\",\n      \"wrong\",\n      \"difficult\",\n      \"hard\",\n      \"annoy\",\n      \"frustrate\",\n      \"sad\",\n      \"unhappy\",\n      \"disappoint\",\n      \"no\",\n      \"not working\",\n      \"doesn't work\",\n      \"crash\",\n      \"stuck\",\n    ];\n    const foundPositive = positiveKeywords.filter((kw) =>\n      messageContent.toLowerCase().includes(kw)\n    );\n    const foundNegative = negativeKeywords.filter((kw) =>\n      messageContent.toLowerCase().includes(kw)\n    );\n    const sentiment_indicators = {\n      positive_keywords: foundPositive,\n      negative_keywords: foundNegative,\n    };\n\n    // Create the message object to be indexed\n    const messageObject = {\n      message_id,\n      conversation_id: conversationId,\n      role,\n      content: messageContent,\n      timestamp,\n      relatedContextEntityIds: JSON.stringify(relatedContextEntityIds || []),\n      summary: null,\n      userIntent: null,\n      topicSegmentId: topicSegmentId || null,\n      semantic_markers: JSON.stringify(semantic_markers),\n      sentiment_indicators: JSON.stringify(sentiment_indicators),\n    };\n\n    console.log(\"Message object to be indexed:\", {\n      message_id: messageObject.message_id,\n      conversation_id: messageObject.conversation_id,\n      role: messageObject.role,\n    });\n\n    // 4. Call ContextIndexerLogic.indexConversationMessage\n    await ContextIndexerLogic.indexConversationMessage(messageObject);\n\n    console.log(\"===== RECORD MESSAGE - COMPLETE =====\");\n    console.log(\"Successfully recorded message with ID:\", message_id);\n\n    // 5. Return the message_id\n    return message_id;\n  } catch (error) {\n    console.error(\"===== RECORD MESSAGE - ERROR =====\");\n    console.error(\"Failed to record message:\", error);\n    console.error(\"Error stack:\", error.stack);\n    throw new Error(\"Failed to record message: \" + error.message);\n  }\n}\n\n/**\n * Detects if a new message represents a topic shift in the conversation.\n *\n * @param {string} newMessageContent - The content of the new message\n * @param {string} conversationId - The conversation ID\n * @returns {Promise<boolean>} True if a topic shift is detected, false otherwise\n */\nexport async function detectTopicShift(newMessageContent, conversationId) {\n  // Fetch recent conversation history (last 5-10 messages)\n  const history = await getConversationHistory(conversationId, 10);\n  // Build the new message object (assume role is 'user' for this context)\n  const newMessage = { content: newMessageContent, role: \"user\" };\n  // Delegate to ConversationSegmenter\n  const isShift = await ConversationSegmenter.detectTopicShift(\n    newMessage,\n    history\n  );\n  return isShift;\n}\n\n/**\n * Gets all topics for a conversation, either as a flat list or hierarchical structure.\n *\n * @param {string} conversationId - The conversation ID\n * @param {boolean} [hierarchical=false] - Whether to return a hierarchical structure\n * @returns {Promise<Topic[] | {rootTopics: Topic[], topicMap: Record<string, Topic>}>}\n */\nexport async function getConversationTopics(\n  conversationId,\n  hierarchical = false\n) {\n  if (hierarchical) {\n    // Use ConversationSegmenter to build hierarchy\n    return await ConversationSegmenter.buildTopicHierarchy(conversationId);\n  }\n  // Flat list: query conversation_topics table\n  const query = `\n    SELECT * FROM conversation_topics\n    WHERE conversation_id = ?\n    ORDER BY start_timestamp ASC\n  `;\n  const topics = await executeQuery(query, [conversationId]);\n  if (!topics || topics.length === 0) return [];\n  // Parse JSON fields\n  return topics.map((topic) => {\n    try {\n      topic.primary_entities = topic.primary_entities\n        ? JSON.parse(topic.primary_entities)\n        : [];\n      topic.keywords = topic.keywords ? JSON.parse(topic.keywords) : [];\n    } catch (err) {\n      topic.primary_entities = topic.primary_entities || [];\n      topic.keywords = topic.keywords || [];\n    }\n    return topic;\n  });\n}\n\n/**\n * Returns messages most relevant to a query, using keyword overlap and optional topic/purpose boosting.\n *\n * @param {string} query - The search query\n * @param {string} conversationId - The conversation ID\n * @param {Object} [options] - Optional filters and limit\n * @param {boolean} [options.purposeFilter] - Boost messages in active purpose\n * @param {boolean} [options.topicFilter] - Boost messages in active topic\n * @param {number} [options.limit] - Max number of results to return\n * @returns {Promise<Message[]>} Array of relevant messages\n */\nexport async function getRelevantConversationContext(\n  query,\n  conversationId,\n  options = {}\n) {\n  // 1. Fetch all messages for the conversation (limit to 200 for performance)\n  const allMessages = await getConversationHistory(conversationId, 200);\n  if (!allMessages || allMessages.length === 0) return [];\n\n  // 2. Tokenize query and extract keywords\n  const queryTokens = TextTokenizerLogic.tokenize(query);\n  const queryKeywords = new Set(\n    TextTokenizerLogic.extractKeywords(queryTokens, 10)\n  );\n\n  // 3. Get active topic and purpose if needed\n  let activeTopicId = null;\n  let activePurpose = null;\n  if (options.topicFilter) {\n    const activeTopic =\n      await ConversationSegmenter.getActiveTopicForConversation(conversationId);\n    activeTopicId = activeTopic ? activeTopic.topic_id : null;\n  }\n  if (options.purposeFilter) {\n    activePurpose = await ConversationPurposeDetector.getActivePurpose(\n      conversationId\n    );\n  }\n\n  // 4. Score each message\n  const scoredMessages = allMessages.map((msg) => {\n    // Tokenize and extract keywords from message content\n    const msgTokens = TextTokenizerLogic.tokenize(msg.content || \"\");\n    const msgKeywords = new Set(\n      TextTokenizerLogic.extractKeywords(msgTokens, 10)\n    );\n    // Calculate Jaccard index (overlap / union)\n    const intersection = new Set(\n      [...queryKeywords].filter((x) => msgKeywords.has(x))\n    );\n    const union = new Set([...queryKeywords, ...msgKeywords]);\n    let relevance = union.size > 0 ? intersection.size / union.size : 0;\n\n    // Boost for topic\n    if (\n      options.topicFilter &&\n      activeTopicId &&\n      msg.topic_segment_id === activeTopicId\n    ) {\n      relevance += 0.2;\n    }\n    // Boost for purpose (if message timestamp falls within active purpose window)\n    if (\n      options.purposeFilter &&\n      activePurpose &&\n      activePurpose.start_timestamp\n    ) {\n      const msgTime = msg.timestamp || msg.created_at;\n      if (\n        msgTime >= activePurpose.start_timestamp &&\n        (!activePurpose.end_timestamp || msgTime <= activePurpose.end_timestamp)\n      ) {\n        relevance += 0.15;\n      }\n    }\n    return { ...msg, relevance };\n  });\n\n  // 5. Sort by relevance DESC\n  scoredMessages.sort((a, b) => b.relevance - a.relevance);\n\n  // 6. Apply limit\n  const limit = options.limit || 10;\n  const topMessages = scoredMessages.slice(0, limit);\n\n  // 7. Remove temporary relevance field before returning\n  return topMessages.map(({ relevance, ...msg }) => msg);\n}\n\n/**\n * Infers the type of development task being discussed in a conversation.\n *\n * @param {string} conversationId - The conversation ID\n * @returns {Promise<string|null>} The inferred task type or null if not confident\n */\nexport async function getTaskTypeFromConversation(conversationId) {\n  // 1. Classify the conversation\n  const classification = await classifyConversation(conversationId);\n  if (!classification || !classification.purpose) return null;\n\n  const { purpose, confidence } = classification;\n\n  // 2. Define mapping from purpose to task type\n  const purposeToTaskType = {\n    debugging: \"bug_fixing\",\n    feature_planning: \"new_feature_development\",\n    code_review: \"code_review\",\n    learning: \"research\",\n    code_generation: \"implementation\",\n    optimization: \"performance_optimization\",\n    refactoring: \"refactoring\",\n    general_query: \"research\",\n    documentation: \"documentation\",\n    testing: \"testing\",\n  };\n\n  // 3. Heuristic: if confidence is low or purpose is too generic, return null or default\n  if (confidence < 0.55 || purpose === \"general_query\") {\n    return null;\n  }\n\n  // 4. Map purpose to task type\n  const taskType = purposeToTaskType[purpose] || \"general_development_task\";\n  return taskType;\n}\n\n/**\n * Provides purpose-specific summaries of different conversation segments.\n *\n * @param {string} conversationId - The conversation ID\n * @returns {Promise<{purpose: string, summary: string}[]>} Array of summaries by purpose\n */\nexport async function getConversationSummaryByPurpose(conversationId) {\n  // 1. Get purpose history\n  const purposeHistory = await ConversationPurposeDetector.getPurposeHistory(\n    conversationId\n  );\n  if (!purposeHistory || purposeHistory.length === 0) return [];\n\n  const summaries = [];\n\n  for (const segment of purposeHistory) {\n    // 2. Fetch all messages in this purpose segment\n    const query = `\n      SELECT content FROM conversation_history\n      WHERE conversation_id = ?\n        AND timestamp >= ?\n        ${segment.end_timestamp ? \"AND timestamp <= ?\" : \"\"}\n      ORDER BY timestamp ASC\n    `;\n    const params = [conversationId, segment.start_timestamp];\n    if (segment.end_timestamp) params.push(segment.end_timestamp);\n    const messages = await executeQuery(query, params);\n    if (!messages || messages.length === 0) continue;\n\n    // 3. Concatenate message contents\n    const concatenated = messages.map((m) => m.content).join(\" \");\n\n    // 4. Summarize using ContextCompressorLogic\n    const summary = await ContextCompressorLogic.summarizeText(concatenated, {\n      targetLength: 150,\n      preserveKeyPoints: true,\n    });\n\n    summaries.push({\n      purpose: segment.purpose_type,\n      summary,\n    });\n  }\n\n  return summaries;\n}\n\n/**\n * Generates an overall summary for an entire conversation.\n *\n * @param {string} conversationId - The conversation ID\n * @returns {Promise<string>} The generated summary\n */\nexport async function summarizeConversation(conversationId) {\n  // 1. Fetch all messages for the conversation, ordered by timestamp ASC\n  const query = `\n    SELECT role, content FROM conversation_history\n    WHERE conversation_id = ?\n    ORDER BY timestamp ASC\n  `;\n  const messages = await executeQuery(query, [conversationId]);\n  if (!messages || messages.length === 0) return \"\";\n\n  // 2. Concatenate messages as 'role: content' lines\n  const concatenated = messages\n    .map((m) => `${m.role}: ${m.content}`)\n    .join(\"\\n\");\n\n  // 3. Summarize using ContextCompressorLogic\n  const summary = await ContextCompressorLogic.summarizeText(concatenated, {\n    targetLength: 250,\n    preserveKeyPoints: true,\n  });\n\n  // 4. Return the summary string\n  return summary;\n}\n\n/**\n * Initializes a new conversation in the system\n *\n * @param {string} conversationId - The conversation ID\n * @param {string} initialQuery - The initial query that started the conversation\n * @returns {Promise<void>}\n */\nexport async function initializeConversation(conversationId, initialQuery) {\n  try {\n    // Create a new entry in the conversation_history table for the initial system message\n    const timestamp = new Date().toISOString();\n    const messageId = uuidv4();\n\n    const query = `\n      INSERT INTO conversation_history (\n        message_id,\n        conversation_id, \n        role,\n        content,\n        timestamp,\n        related_context_entity_ids,\n        summary,\n        user_intent\n      ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n    `;\n\n    await executeQuery(query, [\n      messageId,\n      conversationId,\n      \"system\",\n      initialQuery || \"Conversation started\",\n      timestamp,\n      JSON.stringify([]),\n      \"Conversation initialization\",\n      \"start_conversation\",\n    ]);\n\n    // Initialize conversation purpose based on initial query\n    if (initialQuery) {\n      await ConversationPurposeDetector.detectInitialPurpose(\n        conversationId,\n        initialQuery\n      );\n    }\n\n    // Create initial topic segment\n    await ConversationSegmenter.createNewTopicSegment(\n      conversationId,\n      messageId,\n      {\n        name: \"Initial conversation\",\n        description: initialQuery || \"Conversation start\",\n        primaryEntities: [],\n        keywords: [],\n      }\n    );\n\n    console.log(`Conversation initialized with ID: ${conversationId}`);\n  } catch (error) {\n    console.error(\"Error initializing conversation:\", error);\n    throw new Error(\"Failed to initialize conversation: \" + error.message);\n  }\n}\n\n/**\n * Gets the conversation history for a specific conversation ID\n *\n * @param {string} conversationId - The conversation ID\n * @param {number} [limit=50] - Maximum number of messages to return\n * @param {number} [offset=0] - Offset for pagination\n * @returns {Promise<Array>} Array of message objects\n */\nexport async function getConversationHistory(\n  conversationId,\n  limit = 50,\n  offset = 0\n) {\n  try {\n    if (!conversationId) {\n      throw new Error(\"Conversation ID is required\");\n    }\n\n    const query = `\n      SELECT \n        message_id,\n        conversation_id,\n        role,\n        content,\n        timestamp,\n        related_context_entity_ids,\n        summary,\n        user_intent,\n        topic_segment_id,\n        semantic_markers,\n        sentiment_indicators\n      FROM \n        conversation_history\n      WHERE \n        conversation_id = ?\n      ORDER BY \n        timestamp ASC\n      LIMIT ? OFFSET ?\n    `;\n\n    const results = await executeQuery(query, [conversationId, limit, offset]);\n\n    // Check if results has a rows property and it's an array\n    if (!results || !results.rows || !Array.isArray(results.rows)) {\n      console.warn(\"No valid rows returned from conversation history query\");\n      return [];\n    }\n\n    // Parse JSON fields\n    return results.rows.map((message) => {\n      try {\n        // Map database column names to camelCase property names for API consistency\n        const mappedMessage = {\n          messageId: message.message_id,\n          conversationId: message.conversation_id,\n          role: message.role,\n          content: message.content,\n          timestamp: message.timestamp,\n          relatedContextEntityIds: [],\n          summary: message.summary,\n          userIntent: message.user_intent,\n          topicSegmentId: message.topic_segment_id,\n          semanticMarkers: [],\n          sentimentIndicators: {},\n        };\n\n        if (message.related_context_entity_ids) {\n          mappedMessage.relatedContextEntityIds = JSON.parse(\n            message.related_context_entity_ids\n          );\n        }\n\n        if (message.semantic_markers) {\n          mappedMessage.semanticMarkers = JSON.parse(message.semantic_markers);\n        }\n\n        if (message.sentiment_indicators) {\n          mappedMessage.sentimentIndicators = JSON.parse(\n            message.sentiment_indicators\n          );\n        }\n\n        return mappedMessage;\n      } catch (err) {\n        console.error(\n          \"Error parsing JSON fields in conversation message:\",\n          err\n        );\n        return {\n          messageId: message.message_id,\n          conversationId: message.conversation_id,\n          role: message.role,\n          content: message.content,\n          timestamp: message.timestamp,\n          relatedContextEntityIds: [],\n          summary: message.summary,\n          userIntent: message.user_intent,\n          topicSegmentId: message.topic_segment_id,\n          semanticMarkers: [],\n          sentimentIndicators: {},\n        };\n      }\n    });\n  } catch (error) {\n    console.error(\n      `Error getting conversation history for ${conversationId}:`,\n      error\n    );\n    return [];\n  }\n}\n\n/**\n * Gets the current purpose of a conversation\n *\n * @param {string} conversationId - The ID of the conversation\n * @returns {Promise<{purposeType: string, confidence: number, startTimestamp: string}>} The conversation purpose information\n */\nexport async function getConversationPurpose(conversationId) {\n  try {\n    if (!conversationId) {\n      throw new Error(\"Conversation ID is required\");\n    }\n\n    // Use the ConversationPurposeDetector to get the active purpose\n    const activePurpose = await ConversationPurposeDetector.getActivePurpose(\n      conversationId\n    );\n\n    if (!activePurpose) {\n      // Default response if no purpose is detected\n      return {\n        purposeType: \"general_query\",\n        confidence: 0.5,\n        startTimestamp: new Date().toISOString(),\n      };\n    }\n\n    return activePurpose;\n  } catch (error) {\n    console.error(\n      `Error getting conversation purpose for ${conversationId}:`,\n      error\n    );\n\n    // Default response in case of error\n    return {\n      purposeType: \"general_query\",\n      confidence: 0.5,\n      startTimestamp: new Date().toISOString(),\n    };\n  }\n}\n\n/**\n * Gets the most recent messages for a conversation\n *\n * @param {string} conversationId - The conversation ID\n * @param {number} [count=5] - Number of most recent messages to return\n * @returns {Promise<Array>} Array of the most recent message objects\n */\nexport async function getRecentMessages(conversationId, count = 5) {\n  try {\n    if (!conversationId) {\n      throw new Error(\"Conversation ID is required\");\n    }\n\n    const query = `\n      SELECT \n        message_id,\n        conversation_id,\n        role,\n        content,\n        timestamp,\n        related_context_entity_ids,\n        summary,\n        user_intent,\n        topic_segment_id,\n        semantic_markers,\n        sentiment_indicators\n      FROM \n        conversation_history\n      WHERE \n        conversation_id = ?\n      ORDER BY \n        timestamp DESC\n      LIMIT ?\n    `;\n\n    const results = await executeQuery(query, [conversationId, count]);\n\n    // Check if results has a rows property and it's an array\n    if (!results || !results.rows || !Array.isArray(results.rows)) {\n      console.warn(\"No valid rows returned from recent messages query\");\n      return [];\n    }\n\n    // Parse JSON fields using the same mapping as getConversationHistory\n    return results.rows.map((message) => {\n      try {\n        // Map database column names to camelCase property names for API consistency\n        const mappedMessage = {\n          messageId: message.message_id,\n          conversationId: message.conversation_id,\n          role: message.role,\n          content: message.content,\n          timestamp: message.timestamp,\n          relatedContextEntityIds: [],\n          summary: message.summary,\n          userIntent: message.user_intent,\n          topicSegmentId: message.topic_segment_id,\n          semanticMarkers: [],\n          sentimentIndicators: {},\n        };\n\n        if (message.related_context_entity_ids) {\n          mappedMessage.relatedContextEntityIds = JSON.parse(\n            message.related_context_entity_ids\n          );\n        }\n\n        if (message.semantic_markers) {\n          mappedMessage.semanticMarkers = JSON.parse(message.semantic_markers);\n        }\n\n        if (message.sentiment_indicators) {\n          mappedMessage.sentimentIndicators = JSON.parse(\n            message.sentiment_indicators\n          );\n        }\n\n        return mappedMessage;\n      } catch (err) {\n        console.error(\n          \"Error parsing JSON fields in conversation message:\",\n          err\n        );\n        return {\n          messageId: message.message_id,\n          conversationId: message.conversation_id,\n          role: message.role,\n          content: message.content,\n          timestamp: message.timestamp,\n          relatedContextEntityIds: [],\n          summary: message.summary,\n          userIntent: message.user_intent,\n          topicSegmentId: message.topic_segment_id,\n          semanticMarkers: [],\n          sentimentIndicators: {},\n        };\n      }\n    });\n  } catch (error) {\n    console.error(\n      `Error getting recent messages for ${conversationId}:`,\n      error\n    );\n    return [];\n  }\n}\n", "/**\n * TextTokenizerLogic.js\n *\n * Provides text tokenization with language-specific enhancements\n * for more accurate code analysis and context understanding.\n */\n\n/**\n * Tokenizes input text with language-specific tokenization rules\n *\n * @param {string} text - The text to tokenize\n * @param {string} language - The programming language of the text (default: 'plaintext')\n * @returns {string[]} An array of tokens\n */\nexport function tokenize(text, language = \"plaintext\") {\n  // Normalize to lowercase as requested\n  const normalizedText = text.toLowerCase();\n\n  // Handle language-specific tokenization based on the language parameter\n  switch (language) {\n    case \"javascript\":\n    case \"typescript\":\n    case \"jsx\":\n    case \"tsx\":\n      return tokenizeJavaScript(normalizedText);\n    case \"python\":\n      return tokenizePython(normalizedText);\n    case \"java\":\n    case \"csharp\":\n    case \"c#\":\n      return tokenizeJavaLike(normalizedText);\n    case \"ruby\":\n      return tokenizeRuby(normalizedText);\n    case \"go\":\n      return tokenizeGo(normalizedText);\n    case \"plaintext\":\n    default:\n      return tokenizeGeneric(normalizedText);\n  }\n}\n\n/**\n * Generates n-grams from an array of tokens, respecting semantic boundaries where possible\n *\n * @param {string[]} tokens - Array of tokens (from tokenize function)\n * @param {number} n - Size of n-grams to generate (e.g., 2 for bigrams, 3 for trigrams)\n * @returns {string[]} Array of n-gram strings\n */\nexport function generateNgrams(tokens, n) {\n  // Handle edge cases\n  if (!tokens || tokens.length === 0) return [];\n  if (n <= 0) return [];\n  if (tokens.length < n) return [tokens.join(\" \")];\n\n  const ngrams = [];\n\n  // Track positions where we should avoid generating n-grams\n  // These represent semantic boundaries\n  const semanticBoundaries = new Set();\n\n  // Identify potential semantic boundaries\n  for (let i = 0; i < tokens.length; i++) {\n    const token = tokens[i];\n\n    // Break at special tokens that indicate syntactic boundaries\n    if (token.startsWith(\"__\") && token.endsWith(\"__\")) {\n      semanticBoundaries.add(i);\n      semanticBoundaries.add(i + 1);\n    }\n\n    // Break at common punctuation that signals the end of statements\n    if ([\";\", \".\", \"{\", \"}\", \"(\", \")\", \"[\", \"]\"].includes(token)) {\n      semanticBoundaries.add(i);\n      semanticBoundaries.add(i + 1);\n    }\n  }\n\n  // Generate n-grams by sliding a window of size n over the tokens array\n  // Skip windows that cross semantic boundaries\n  for (let i = 0; i <= tokens.length - n; i++) {\n    // Check if any semantic boundary exists within this window\n    let hasBoundary = false;\n    for (let j = i; j < i + n - 1; j++) {\n      if (semanticBoundaries.has(j + 1)) {\n        hasBoundary = true;\n        break;\n      }\n    }\n\n    // If no semantic boundaries in this window, generate the n-gram\n    if (!hasBoundary) {\n      const ngram = tokens.slice(i, i + n).join(\" \");\n      ngrams.push(ngram);\n    }\n  }\n\n  return ngrams;\n}\n\n/**\n * Extracts n-grams from an array of tokens (alias for generateNgrams)\n *\n * @param {string[]} tokens - Array of tokens (from tokenize function)\n * @param {number} n - Size of n-grams to generate (e.g., 2 for bigrams, 3 for trigrams)\n * @returns {string[]} Array of n-gram strings\n */\nexport function extractNGrams(tokens, n) {\n  return generateNgrams(tokens, n);\n}\n\n/**\n * Identifies language-specific idioms in code text\n *\n * @param {string} text - Raw text to analyze\n * @param {string} language - Programming language of the text\n * @returns {{idiom: string, type: string, location: {start: number, end: number}}[]} Array of identified idioms\n */\nexport function identifyLanguageSpecificIdioms(text, language) {\n  // Handle empty input\n  if (!text) return [];\n\n  const idioms = [];\n\n  // Normalize language parameter\n  const normalizedLanguage = language.toLowerCase();\n\n  // Use language-specific idiom detection\n  switch (normalizedLanguage) {\n    case \"javascript\":\n    case \"typescript\":\n    case \"jsx\":\n    case \"tsx\":\n      identifyJavaScriptIdioms(text, idioms);\n      break;\n    case \"python\":\n      identifyPythonIdioms(text, idioms);\n      break;\n    case \"csharp\":\n    case \"c#\":\n      identifyCSharpIdioms(text, idioms);\n      break;\n    // Add more languages as needed\n  }\n\n  return idioms;\n}\n\n/**\n * Identifies JavaScript-specific idioms\n *\n * @param {string} text - JavaScript code text\n * @param {Array} idioms - Array to add identified idioms to\n * @private\n */\nfunction identifyJavaScriptIdioms(text, idioms) {\n  // 1. Detect Promise chains (.then().catch())\n  const promiseChainRegex =\n    /\\.\\s*then\\s*\\(\\s*(?:function\\s*\\([^)]*\\)|[^=>(]*=>\\s*[^)]*)\\s*\\)(?:\\s*\\.(?:then|catch|finally)\\s*\\([^)]*\\))+/g;\n  let match;\n\n  while ((match = promiseChainRegex.exec(text)) !== null) {\n    idioms.push({\n      idiom: match[0],\n      type: \"js_promise_chain\",\n      location: {\n        start: match.index,\n        end: match.index + match[0].length,\n      },\n    });\n  }\n\n  // 2. Detect async/await usage\n  const asyncAwaitRegex =\n    /\\basync\\s+(?:function\\s*[a-zA-Z0-9_$]*\\s*\\([^)]*\\)|(?:[a-zA-Z0-9_$]+\\s*=>)|(?:\\([^)]*\\)\\s*=>))(?:(?:.|\\n)*?\\bawait\\b(?:.|\\n)*?)/g;\n\n  while ((match = asyncAwaitRegex.exec(text)) !== null) {\n    idioms.push({\n      idiom: match[0],\n      type: \"js_async_await\",\n      location: {\n        start: match.index,\n        end: match.index + match[0].length,\n      },\n    });\n  }\n\n  // 3. Detect arrow functions as callbacks\n  const arrowCallbackRegex =\n    /(?:\\.|\\()(?:[a-zA-Z0-9_$]+)?\\s*\\(\\s*(?:\\([^)]*\\)|[a-zA-Z0-9_$]+)\\s*=>\\s*(?:{[^}]*}|[^);,]*)/g;\n\n  while ((match = arrowCallbackRegex.exec(text)) !== null) {\n    // Avoid duplicate detection with Promise chains\n    const isDuplicate = idioms.some(\n      (idiom) =>\n        idiom.type === \"js_promise_chain\" &&\n        match.index >= idiom.location.start &&\n        match.index + match[0].length <= idiom.location.end\n    );\n\n    if (!isDuplicate) {\n      idioms.push({\n        idiom: match[0],\n        type: \"js_arrow_callback\",\n        location: {\n          start: match.index,\n          end: match.index + match[0].length,\n        },\n      });\n    }\n  }\n}\n\n/**\n * Identifies Python-specific idioms\n *\n * @param {string} text - Python code text\n * @param {Array} idioms - Array to add identified idioms to\n * @private\n */\nfunction identifyPythonIdioms(text, idioms) {\n  // 1. Detect list comprehensions\n  const listComprehensionRegex =\n    /\\[\\s*[^\\[\\]]*\\s+for\\s+[^\\[\\]]+\\s+in\\s+[^\\[\\]]+(?:\\s+if\\s+[^\\[\\]]+)?\\s*\\]/g;\n  let match;\n\n  while ((match = listComprehensionRegex.exec(text)) !== null) {\n    idioms.push({\n      idiom: match[0],\n      type: \"python_list_comprehension\",\n      location: {\n        start: match.index,\n        end: match.index + match[0].length,\n      },\n    });\n  }\n\n  // 2. Detect dictionary comprehensions\n  const dictComprehensionRegex =\n    /\\{\\s*[^{}]*\\s*:\\s*[^{}]*\\s+for\\s+[^{}]+\\s+in\\s+[^{}]+(?:\\s+if\\s+[^{}]+)?\\s*\\}/g;\n\n  while ((match = dictComprehensionRegex.exec(text)) !== null) {\n    idioms.push({\n      idiom: match[0],\n      type: \"python_dict_comprehension\",\n      location: {\n        start: match.index,\n        end: match.index + match[0].length,\n      },\n    });\n  }\n\n  // 3. Detect lambda functions\n  const lambdaRegex = /lambda\\s+[^:]+:[^,\\n)]+/g;\n\n  while ((match = lambdaRegex.exec(text)) !== null) {\n    idioms.push({\n      idiom: match[0],\n      type: \"python_lambda\",\n      location: {\n        start: match.index,\n        end: match.index + match[0].length,\n      },\n    });\n  }\n\n  // 4. Detect generator expressions\n  const generatorRegex =\n    /\\(\\s*[^()]*\\s+for\\s+[^()]+\\s+in\\s+[^()]+(?:\\s+if\\s+[^()]+)?\\s*\\)/g;\n\n  while ((match = generatorRegex.exec(text)) !== null) {\n    idioms.push({\n      idiom: match[0],\n      type: \"python_generator_expression\",\n      location: {\n        start: match.index,\n        end: match.index + match[0].length,\n      },\n    });\n  }\n}\n\n/**\n * Identifies C#-specific idioms\n *\n * @param {string} text - C# code text\n * @param {Array} idioms - Array to add identified idioms to\n * @private\n */\nfunction identifyCSharpIdioms(text, idioms) {\n  // 1. Detect LINQ queries with method syntax\n  const linqMethodRegex =\n    /\\.\\s*(?:Where|Select|OrderBy|OrderByDescending|GroupBy|Join|Skip|Take|First|FirstOrDefault|Any|All|Count)\\s*\\(\\s*[^)]*\\)(?:\\s*\\.\\s*(?:Where|Select|OrderBy|OrderByDescending|GroupBy|Join|Skip|Take|First|FirstOrDefault|Any|All|Count)\\s*\\(\\s*[^)]*\\))*/g;\n  let match;\n\n  while ((match = linqMethodRegex.exec(text)) !== null) {\n    idioms.push({\n      idiom: match[0],\n      type: \"csharp_linq_method\",\n      location: {\n        start: match.index,\n        end: match.index + match[0].length,\n      },\n    });\n  }\n\n  // 2. Detect LINQ queries with query syntax\n  const linqQueryRegex =\n    /from\\s+\\w+\\s+in\\s+[^{]+(?:where\\s+[^{]+)?(?:orderby\\s+[^{]+)?(?:select\\s+[^{;]+)?(?:group\\s+[^{;]+by\\s+[^{;]+)?/g;\n\n  while ((match = linqQueryRegex.exec(text)) !== null) {\n    idioms.push({\n      idiom: match[0],\n      type: \"csharp_linq_query\",\n      location: {\n        start: match.index,\n        end: match.index + match[0].length,\n      },\n    });\n  }\n\n  // 3. Detect async/await patterns\n  const asyncAwaitRegex =\n    /\\basync\\s+[^(]*\\([^)]*\\)(?:\\s*<[^>]*>)?\\s*(?:=>)?\\s*{(?:(?:.|\\n)*?\\bawait\\b(?:.|\\n)*?)}/g;\n\n  while ((match = asyncAwaitRegex.exec(text)) !== null) {\n    idioms.push({\n      idiom: match[0],\n      type: \"csharp_async_await\",\n      location: {\n        start: match.index,\n        end: match.index + match[0].length,\n      },\n    });\n  }\n\n  // 4. Detect lambda expressions\n  const lambdaRegex = /(?:\\([^)]*\\)|\\w+)\\s*=>\\s*(?:{[^}]*}|[^;]+)/g;\n\n  while ((match = lambdaRegex.exec(text)) !== null) {\n    // Avoid duplicate detection with LINQ methods\n    const isDuplicate = idioms.some(\n      (idiom) =>\n        (idiom.type === \"csharp_linq_method\" ||\n          idiom.type === \"csharp_linq_query\") &&\n        match.index >= idiom.location.start &&\n        match.index + match[0].length <= idiom.location.end\n    );\n\n    if (!isDuplicate) {\n      idioms.push({\n        idiom: match[0],\n        type: \"csharp_lambda\",\n        location: {\n          start: match.index,\n          end: match.index + match[0].length,\n        },\n      });\n    }\n  }\n}\n\n/**\n * Extracts keywords from an array of tokens with language-specific enhancements\n *\n * @param {string[]} tokens - Array of tokens (from tokenize function)\n * @param {number} topN - Number of top keywords to return (default: 10)\n * @param {string} language - Programming language hint (default: 'plaintext')\n * @returns {{keyword: string, score: number}[]} Array of keywords with scores\n */\nexport function extractKeywords(tokens, topN = 10, language = \"plaintext\") {\n  // Get language-specific stop words\n  const stopWords = getStopWords(language);\n\n  // Calculate term frequencies\n  const termFrequencies = {};\n  for (const token of tokens) {\n    if (!termFrequencies[token]) {\n      termFrequencies[token] = 0;\n    }\n    termFrequencies[token]++;\n  }\n\n  // Apply scoring heuristics\n  const scoredKeywords = [];\n\n  for (const [token, frequency] of Object.entries(termFrequencies)) {\n    // Skip stop words unless they're part of something significant\n    // (e.g., longer than typical stop words or contain special characters)\n    if (stopWords.has(token) && token.length < 6 && !/[_\\-$#@]/.test(token)) {\n      continue;\n    }\n\n    // Base score is the term frequency\n    let score = frequency;\n\n    // Boost domain-specific tokens (identifiers)\n    if (isDomainSpecificToken(token, language)) {\n      score *= 2.0;\n    }\n\n    // Boost longer words (they tend to be more meaningful)\n    if (token.length > 6) {\n      score *= 1.5;\n    }\n\n    // Boost tokens with special characters that are likely important in code\n    if (/[_$]/.test(token)) {\n      score *= 1.2;\n    }\n\n    // Penalize very short tokens that aren't likely to be meaningful\n    if (token.length < 3 && !/[_\\-$#@]/.test(token)) {\n      score *= 0.5;\n    }\n\n    // Additional boosts for language-specific patterns\n    score = applyLanguageSpecificBoosts(token, score, language);\n\n    scoredKeywords.push({\n      keyword: token,\n      score: score,\n    });\n  }\n\n  // Sort by score (descending) and return top N\n  return scoredKeywords.sort((a, b) => b.score - a.score).slice(0, topN);\n}\n\n/**\n * Determines if a token is likely a domain-specific identifier\n *\n * @param {string} token - The token to check\n * @param {string} language - The programming language\n * @returns {boolean} True if the token appears to be domain-specific\n */\nfunction isDomainSpecificToken(token, language) {\n  // Check for common patterns that indicate domain-specific tokens\n\n  // CamelCase or PascalCase (common in most languages)\n  if (/[a-z][A-Z]/.test(token) || /^[A-Z][a-z]/.test(token)) {\n    return true;\n  }\n\n  // snake_case (common in Python, Ruby)\n  if (token.includes(\"_\") && token.length > 4) {\n    return true;\n  }\n\n  // Special prefixes/patterns common in various languages\n  if (/^(on|handle|process|get|set|is|has|should|with)/i.test(token)) {\n    return true;\n  }\n\n  // Tokens with numbers are often domain-specific\n  if (/[a-z][0-9]/.test(token)) {\n    return true;\n  }\n\n  // JavaScript/TypeScript specific\n  if (\n    (language === \"javascript\" || language === \"typescript\") &&\n    (/\\$/.test(token) || // Angular, jQuery\n      /^use[A-Z]/.test(token))\n  ) {\n    // React hooks\n    return true;\n  }\n\n  // Python specific\n  if (\n    language === \"python\" &&\n    (/^__.*__$/.test(token) || // dunder methods\n      /^self\\./.test(token))\n  ) {\n    // instance attributes\n    return true;\n  }\n\n  return false;\n}\n\n/**\n * Apply language-specific score boosts to tokens\n *\n * @param {string} token - The token to apply boosts to\n * @param {number} score - The current score\n * @param {string} language - The programming language\n * @returns {number} The updated score\n */\nfunction applyLanguageSpecificBoosts(token, score, language) {\n  switch (language) {\n    case \"javascript\":\n    case \"typescript\":\n    case \"jsx\":\n    case \"tsx\":\n      // Boost React/component related terms\n      if (\n        /^(use|component|props|state|render|effect|memo|callback)/.test(token)\n      ) {\n        score *= 1.5;\n      }\n      // Boost event handler patterns\n      if (/^(on[A-Z]|handle[A-Z])/.test(token)) {\n        score *= 1.3;\n      }\n      break;\n\n    case \"python\":\n      // Boost important Python patterns\n      if (/^(def|class|self|super|__init__|__main__)/.test(token)) {\n        score *= 1.3;\n      }\n      // Boost decorators\n      if (/^@/.test(token)) {\n        score *= 1.4;\n      }\n      break;\n\n    case \"java\":\n    case \"csharp\":\n    case \"c#\":\n      // Boost important Java/C# patterns\n      if (\n        /^(public|private|protected|static|final|override|virtual|abstract)/.test(\n          token\n        )\n      ) {\n        score *= 1.2;\n      }\n      // Boost class/interface/enum declarations\n      if (/^(class|interface|enum|record|struct)/.test(token)) {\n        score *= 1.3;\n      }\n      break;\n\n    case \"ruby\":\n      // Boost Ruby-specific patterns\n      if (/^(attr_|def|class|module|require|include|extend)/.test(token)) {\n        score *= 1.3;\n      }\n      // Boost symbols\n      if (/^:/.test(token)) {\n        score *= 1.2;\n      }\n      break;\n\n    case \"go\":\n      // Boost Go-specific patterns\n      if (/^(func|struct|interface|type|go|chan|defer|goroutine)/.test(token)) {\n        score *= 1.3;\n      }\n      break;\n  }\n\n  return score;\n}\n\n/**\n * Get stop words for the specified language\n *\n * @param {string} language - The programming language\n * @returns {Set<string>} Set of stop words\n */\nfunction getStopWords(language) {\n  // Common English stop words\n  const commonStopWords = new Set([\n    \"a\",\n    \"an\",\n    \"the\",\n    \"and\",\n    \"or\",\n    \"but\",\n    \"if\",\n    \"then\",\n    \"else\",\n    \"when\",\n    \"at\",\n    \"from\",\n    \"by\",\n    \"for\",\n    \"with\",\n    \"about\",\n    \"against\",\n    \"between\",\n    \"into\",\n    \"through\",\n    \"during\",\n    \"before\",\n    \"after\",\n    \"above\",\n    \"below\",\n    \"to\",\n    \"is\",\n    \"am\",\n    \"are\",\n    \"was\",\n    \"were\",\n    \"be\",\n    \"been\",\n    \"being\",\n    \"have\",\n    \"has\",\n    \"had\",\n    \"having\",\n    \"do\",\n    \"does\",\n    \"did\",\n    \"doing\",\n    \"would\",\n    \"should\",\n    \"could\",\n    \"ought\",\n    \"i\",\n    \"you\",\n    \"he\",\n    \"she\",\n    \"it\",\n    \"we\",\n    \"they\",\n    \"their\",\n    \"this\",\n    \"that\",\n    \"these\",\n    \"those\",\n    \"of\",\n    \"in\",\n    \"as\",\n    \"on\",\n    \"not\",\n    \"no\",\n    \"its\",\n    \"his\",\n    \"her\",\n  ]);\n\n  // Common programming language keywords\n  const commonProgrammingStopWords = new Set([\n    \"function\",\n    \"class\",\n    \"if\",\n    \"else\",\n    \"for\",\n    \"while\",\n    \"do\",\n    \"switch\",\n    \"case\",\n    \"break\",\n    \"continue\",\n    \"return\",\n    \"try\",\n    \"catch\",\n    \"finally\",\n    \"throw\",\n    \"throws\",\n    \"public\",\n    \"private\",\n    \"protected\",\n    \"static\",\n    \"final\",\n    \"abstract\",\n    \"interface\",\n    \"extends\",\n    \"implements\",\n    \"import\",\n    \"export\",\n    \"package\",\n    \"namespace\",\n    \"var\",\n    \"let\",\n    \"const\",\n    \"new\",\n    \"this\",\n    \"super\",\n    \"null\",\n    \"undefined\",\n    \"true\",\n    \"false\",\n  ]);\n\n  // Start with common stop words for all languages\n  const stopWords = new Set([\n    ...commonStopWords,\n    ...commonProgrammingStopWords,\n  ]);\n\n  // Add language-specific stop words\n  switch (language) {\n    case \"javascript\":\n    case \"typescript\":\n    case \"jsx\":\n    case \"tsx\":\n      // JavaScript/TypeScript specific\n      [\n        \"typeof\",\n        \"instanceof\",\n        \"async\",\n        \"await\",\n        \"yield\",\n        \"void\",\n        \"delete\",\n        \"module\",\n        \"require\",\n        \"console\",\n        \"log\",\n        \"window\",\n        \"document\",\n        \"event\",\n        \"prototype\",\n        \"constructor\",\n        \"string\",\n        \"number\",\n        \"boolean\",\n        \"object\",\n        \"array\",\n      ].forEach((word) => stopWords.add(word));\n      break;\n\n    case \"python\":\n      // Python specific\n      [\n        \"def\",\n        \"lambda\",\n        \"from\",\n        \"as\",\n        \"import\",\n        \"with\",\n        \"is\",\n        \"in\",\n        \"not\",\n        \"and\",\n        \"or\",\n        \"global\",\n        \"nonlocal\",\n        \"pass\",\n        \"yield\",\n        \"assert\",\n        \"del\",\n        \"raise\",\n        \"except\",\n        \"print\",\n        \"exec\",\n        \"eval\",\n        \"None\",\n        \"True\",\n        \"False\",\n        \"range\",\n        \"len\",\n        \"self\",\n      ].forEach((word) => stopWords.add(word));\n      break;\n\n    case \"java\":\n      // Java specific\n      [\n        \"void\",\n        \"boolean\",\n        \"byte\",\n        \"char\",\n        \"short\",\n        \"int\",\n        \"long\",\n        \"float\",\n        \"double\",\n        \"instanceof\",\n        \"strictfp\",\n        \"synchronized\",\n        \"transient\",\n        \"volatile\",\n        \"native\",\n        \"package\",\n        \"throws\",\n        \"throw\",\n        \"exception\",\n        \"assert\",\n        \"enum\",\n      ].forEach((word) => stopWords.add(word));\n      break;\n\n    case \"csharp\":\n    case \"c#\":\n      // C# specific\n      [\n        \"using\",\n        \"namespace\",\n        \"where\",\n        \"select\",\n        \"from\",\n        \"group\",\n        \"into\",\n        \"orderby\",\n        \"join\",\n        \"equals\",\n        \"out\",\n        \"ref\",\n        \"in\",\n        \"value\",\n        \"is\",\n        \"as\",\n        \"void\",\n        \"int\",\n        \"string\",\n        \"bool\",\n        \"decimal\",\n        \"object\",\n        \"char\",\n        \"byte\",\n        \"sbyte\",\n        \"uint\",\n        \"long\",\n        \"ulong\",\n        \"short\",\n        \"ushort\",\n        \"double\",\n        \"float\",\n        \"dynamic\",\n        \"delegate\",\n        \"event\",\n        \"async\",\n        \"await\",\n        \"partial\",\n        \"virtual\",\n        \"override\",\n        \"sealed\",\n        \"base\",\n      ].forEach((word) => stopWords.add(word));\n      break;\n\n    case \"ruby\":\n      // Ruby specific\n      [\n        \"def\",\n        \"end\",\n        \"module\",\n        \"require\",\n        \"include\",\n        \"extend\",\n        \"attr\",\n        \"attr_reader\",\n        \"attr_writer\",\n        \"attr_accessor\",\n        \"lambda\",\n        \"proc\",\n        \"yield\",\n        \"self\",\n        \"nil\",\n        \"true\",\n        \"false\",\n        \"unless\",\n        \"until\",\n        \"begin\",\n        \"rescue\",\n        \"ensure\",\n        \"alias\",\n      ].forEach((word) => stopWords.add(word));\n      break;\n\n    case \"go\":\n      // Go specific\n      [\n        \"func\",\n        \"type\",\n        \"struct\",\n        \"interface\",\n        \"map\",\n        \"chan\",\n        \"go\",\n        \"select\",\n        \"package\",\n        \"import\",\n        \"const\",\n        \"var\",\n        \"iota\",\n        \"make\",\n        \"new\",\n        \"append\",\n        \"len\",\n        \"cap\",\n        \"nil\",\n        \"true\",\n        \"false\",\n        \"int\",\n        \"int8\",\n        \"int16\",\n        \"int32\",\n        \"int64\",\n        \"uint\",\n        \"uint8\",\n        \"uint16\",\n        \"uint32\",\n        \"uint64\",\n        \"float32\",\n        \"float64\",\n        \"string\",\n        \"byte\",\n        \"rune\",\n        \"defer\",\n        \"panic\",\n        \"recover\",\n      ].forEach((word) => stopWords.add(word));\n      break;\n  }\n\n  return stopWords;\n}\n\n/**\n * Generic tokenization for unknown languages or plaintext\n *\n * @param {string} text - The text to tokenize\n * @returns {string[]} An array of tokens\n */\nfunction tokenizeGeneric(text) {\n  // Replace common punctuation with spaces before splitting\n  // But preserve meaningful symbols like _, -, #, @ if part of identifiers\n  const withSpaces = text\n    // Preserve common identifier patterns\n    .replace(/([a-z0-9])[-_]([a-z0-9])/g, \"$1\\u0001$2\") // Replace with placeholder\n    // Add space around punctuation\n    .replace(/[.,;:(){}[\\]<>?!]/g, \" $& \")\n    // Restore preserved symbols\n    .replace(/\\u0001/g, \"_\");\n\n  // Split by whitespace and filter out empty tokens\n  let tokens = withSpaces.split(/\\s+/).filter((token) => token.length > 0);\n\n  return tokens;\n}\n\n/**\n * JavaScript/TypeScript-specific tokenization\n * Handles camelCase, PascalCase, module imports, JSX tags, template literals, decorators\n *\n * @param {string} text - The JavaScript/TypeScript text to tokenize\n * @returns {string[]} An array of tokens\n */\nfunction tokenizeJavaScript(text) {\n  let tokens = [];\n\n  // Preserve comments for content analysis but mark them specially\n  const commentPlaceholders = {};\n  let commentCounter = 0;\n\n  // Remove block comments first\n  const withoutBlockComments = text.replace(/\\/\\*[\\s\\S]*?\\*\\//g, (match) => {\n    const placeholder = `__COMMENT_BLOCK_${commentCounter++}__`;\n    commentPlaceholders[placeholder] = match;\n    return placeholder;\n  });\n\n  // Remove line comments\n  const withoutComments = withoutBlockComments.replace(\n    /\\/\\/[^\\n]*/g,\n    (match) => {\n      const placeholder = `__COMMENT_LINE_${commentCounter++}__`;\n      commentPlaceholders[placeholder] = match;\n      return placeholder;\n    }\n  );\n\n  // Track string literals and code blocks to avoid tokenizing their contents incorrectly\n  const stringPlaceholders = {};\n  let stringCounter = 0;\n\n  // Handle regex literals - important to do this before handling division operator\n  // Look for patterns like /.../ not preceded by identifiers or closing brackets/parentheses\n  const withoutRegex = withoutComments.replace(\n    /(?<![a-zA-Z0-9_\\)\\]\\}])\\/(?:\\\\\\/|[^\\/\\n])+\\/[gimuy]*/g,\n    (match) => {\n      const placeholder = `__REGEX_${stringCounter++}__`;\n      stringPlaceholders[placeholder] = match;\n      return placeholder;\n    }\n  );\n\n  // Handle template literals with interpolation\n  // Capture the whole template including expressions inside ${}\n  const withoutTemplateLiterals = withoutRegex.replace(\n    /`(?:\\\\`|\\\\\\\\|[^`])*`/g,\n    (match) => {\n      const placeholder = `__TEMPLATE_${stringCounter++}__`;\n      stringPlaceholders[placeholder] = match;\n\n      // Extract interpolated expressions from ${...} and tokenize them separately\n      const expressions = [];\n      let expContent = match.match(/\\${([^}]*)}/g);\n      if (expContent) {\n        expContent.forEach((exp) => {\n          expressions.push(exp.slice(2, -1)); // Remove ${ and }\n        });\n\n        // Tokenize each expression content\n        expressions.forEach((exp) => {\n          const expTokens = tokenizeJavaScript(exp); // Recursively tokenize expressions\n          tokens.push(...expTokens);\n        });\n      }\n\n      return placeholder;\n    }\n  );\n\n  // Handle string literals with placeholder\n  const withoutStrings = withoutTemplateLiterals.replace(\n    /'(?:\\\\'|\\\\\\\\|[^'])*'|\"(?:\\\\\"|\\\\\\\\|[^\"])*\"/g,\n    (match) => {\n      const placeholder = `__STRING_${stringCounter++}__`;\n      stringPlaceholders[placeholder] = match;\n      return placeholder;\n    }\n  );\n\n  // Handle JSX tags - more comprehensive approach for nested components\n  // First, capture JSX opening tags, self-closing tags, and closing tags\n  const withoutJSX = withoutStrings.replace(\n    /<([A-Z][a-zA-Z0-9]*|[a-z][a-z0-9]*)((?:\\s+[a-zA-Z0-9_]+(?:=(?:\"|'|\\{).*?(?:\"|'|\\}))?)*)\\s*(?:\\/)?>/g,\n    (match, tagName, attributes) => {\n      const placeholder = `__JSX_TAG_${stringCounter++}__`;\n      stringPlaceholders[placeholder] = match;\n\n      // Add the tag name as token\n      tokens.push(tagName);\n\n      // Extract and add attribute names\n      if (attributes) {\n        const attrMatches = attributes.match(/[a-zA-Z0-9_]+(?==)/g);\n        if (attrMatches) {\n          tokens.push(...attrMatches);\n        }\n      }\n\n      return placeholder;\n    }\n  );\n\n  // Handle JSX closing tags\n  const withoutJSXClosing = withoutJSX.replace(\n    /<\\/([A-Z][a-zA-Z0-9]*|[a-z][a-z0-9]*)>/g,\n    (match, tagName) => {\n      const placeholder = `__JSX_CLOSING_${stringCounter++}__`;\n      stringPlaceholders[placeholder] = match;\n      tokens.push(tagName);\n      return placeholder;\n    }\n  );\n\n  // Handle decorators with placeholder - more comprehensive for complex decorators\n  const withoutDecorators = withoutJSXClosing.replace(\n    /@([a-zA-Z][a-zA-Z0-9_]*)(?:\\((?:[^)(]*|\\([^)(]*\\))*\\))?/g,\n    (match, decoratorName) => {\n      const placeholder = `__DECORATOR_${stringCounter++}__`;\n      stringPlaceholders[placeholder] = match;\n\n      // Add the decorator name as token\n      tokens.push(decoratorName);\n\n      // If there are parameters to the decorator, tokenize them separately\n      const paramMatch = match.match(/\\((.*)\\)/);\n      if (paramMatch && paramMatch[1]) {\n        const paramTokens = tokenizeGeneric(paramMatch[1]);\n        tokens.push(...paramTokens);\n      }\n\n      return placeholder;\n    }\n  );\n\n  // Handle arrow functions specially\n  const withoutArrows = withoutDecorators.replace(/=>/g, (match) => {\n    tokens.push(\"arrow_function\"); // Use a special token for recognizing arrow functions\n    return \" => \"; // Preserve the token but with spaces for other tokenization\n  });\n\n  // Handle optional chaining and nullish coalescing\n  const withSpecialOps = withoutArrows\n    .replace(/\\?\\./g, (match) => {\n      tokens.push(\"optional_chaining\");\n      return \" ?. \"; // Space-separated for tokenization\n    })\n    .replace(/\\?\\?/g, (match) => {\n      tokens.push(\"nullish_coalescing\");\n      return \" ?? \"; // Space-separated for tokenization\n    });\n\n  // Handle import statements more robustly\n  const withoutImports = withSpecialOps.replace(\n    /import\\s+(?:{[^}]*}|\\*\\s+as\\s+[a-zA-Z][a-zA-Z0-9_]*|[a-zA-Z][a-zA-Z0-9_]*)\\s+from\\s+['\"][^'\"]*['\"]/g,\n    (match) => {\n      // Add import as a token\n      tokens.push(\"import\");\n\n      // Extract module name\n      const moduleMatch = match.match(/from\\s+['\"]([^'\"]*)['\"]/);\n      if (moduleMatch && moduleMatch[1]) {\n        tokens.push(moduleMatch[1]);\n      }\n\n      // Extract imported identifiers\n      const importedMatch = match.match(\n        /import\\s+({[^}]*}|\\*\\s+as\\s+[a-zA-Z][a-zA-Z0-9_]*|[a-zA-Z][a-zA-Z0-9_]*)/\n      );\n      if (importedMatch && importedMatch[1]) {\n        const importSection = importedMatch[1];\n\n        if (importSection.startsWith(\"{\")) {\n          // Named imports\n          const namedImports = importSection\n            .replace(/[{}]/g, \"\")\n            .split(\",\")\n            .map((part) => part.trim())\n            .filter((part) => part.length > 0);\n\n          tokens.push(...namedImports);\n        } else if (importSection.includes(\"* as\")) {\n          // Namespace import\n          const nsMatch = importSection.match(\n            /\\*\\s+as\\s+([a-zA-Z][a-zA-Z0-9_]*)/\n          );\n          if (nsMatch && nsMatch[1]) {\n            tokens.push(nsMatch[1]);\n          }\n        } else {\n          // Default import\n          tokens.push(importSection.trim());\n        }\n      }\n\n      return \" \"; // Replace with a space\n    }\n  );\n\n  // Split remaining text into tokens\n  let mainTokens = tokenizeGeneric(withoutImports);\n\n  // Handle camelCase and PascalCase by splitting them into separate tokens\n  const processedTokens = [];\n  for (const token of mainTokens) {\n    // Skip placeholder tokens (we'll handle them separately)\n    if (token.startsWith(\"__\") && token.endsWith(\"__\")) {\n      processedTokens.push(token);\n      continue;\n    }\n\n    // Skip operators we've already handled\n    if ([\"=>\", \"?.\", \"??\"].includes(token)) {\n      processedTokens.push(token);\n      continue;\n    }\n\n    // Split camelCase into separate tokens\n    const camelTokens = token\n      .replace(/([a-z])([A-Z])/g, \"$1 $2\")\n      .toLowerCase()\n      .split(\" \");\n\n    // Add original token and split tokens\n    processedTokens.push(token);\n    if (camelTokens.length > 1) {\n      processedTokens.push(...camelTokens);\n    }\n  }\n\n  // Replace placeholders with their original values\n  const finalTokens = [];\n  for (const token of processedTokens) {\n    if (stringPlaceholders[token]) {\n      // Add the original placeholder as a token (to preserve context)\n      if (token.startsWith(\"__REGEX_\")) {\n        finalTokens.push(\"regex_literal\");\n      } else if (token.startsWith(\"__JSX_\")) {\n        finalTokens.push(\"jsx_element\");\n      } else if (token.startsWith(\"__DECORATOR_\")) {\n        finalTokens.push(\"decorator\");\n      } else {\n        finalTokens.push(token);\n      }\n\n      // For string literals, also add their content as tokens\n      if (token.startsWith(\"__STRING_\") || token.startsWith(\"__TEMPLATE_\")) {\n        // Extract content and add relevant words\n        const content = stringPlaceholders[token];\n        // Remove quotes/backticks and tokenize content\n        const strContent = content.replace(/^[`'\"](.*)[`'\"]$/s, \"$1\");\n        const contentTokens = tokenizeGeneric(strContent);\n        finalTokens.push(...contentTokens);\n      }\n    } else if (commentPlaceholders[token]) {\n      // For comments, optionally extract keywords if needed\n      // Don't add the full comment as a token to avoid noise\n      finalTokens.push(\"code_comment\");\n\n      // Extract possible important terms from comments\n      const commentContent = commentPlaceholders[token]\n        .replace(/^\\/\\*|\\*\\/$/g, \"\") // Remove /* */\n        .replace(/^\\/\\//g, \"\"); // Remove //\n\n      // Only use alphanumeric words from comments, skip punctuation and symbols\n      const commentTokens = commentContent\n        .split(/\\s+/)\n        .filter((word) => /^[a-z0-9_]{3,}$/i.test(word))\n        .map((word) => word.toLowerCase());\n\n      finalTokens.push(...commentTokens);\n    } else {\n      finalTokens.push(token);\n    }\n  }\n\n  return [...new Set(finalTokens)]; // Remove duplicates\n}\n\n/**\n * Python-specific tokenization\n * Handles snake_case, decorators, f-strings, indentation significance\n *\n * @param {string} text - The Python text to tokenize\n * @returns {string[]} An array of tokens\n */\nfunction tokenizePython(text) {\n  let tokens = [];\n\n  // Preserve comments for content analysis but mark them specially\n  const commentPlaceholders = {};\n  let commentCounter = 0;\n\n  // Remove block comments first (triple-quoted strings when used as comments)\n  const withoutDocstrings = text.replace(\n    /(?:'''[\\s\\S]*?'''|\"\"\"[\\s\\S]*?\"\"\")/g,\n    (match) => {\n      const placeholder = `__PYCOMMENT_BLOCK_${commentCounter++}__`;\n      commentPlaceholders[placeholder] = match;\n      return placeholder;\n    }\n  );\n\n  // Remove line comments\n  const withoutComments = withoutDocstrings.replace(/#[^\\n]*/g, (match) => {\n    const placeholder = `__PYCOMMENT_LINE_${commentCounter++}__`;\n    commentPlaceholders[placeholder] = match;\n    return placeholder;\n  });\n\n  // Handle string literals\n  const stringPlaceholders = {};\n  let placeholderCounter = 0;\n\n  // Enhanced f-string handling - look for f, fr, rf prefixes and capture interpolation\n  const withoutFStrings = withoutComments.replace(\n    /(?:f|fr|rf)(?:'''[\\s\\S]*?'''|\"\"\"[\\s\\S]*?\"\"\"|'(?:\\\\'|\\\\\\\\|[^'])*'|\"(?:\\\\\"|\\\\\\\\|[^\"])*\")/g,\n    (match) => {\n      const placeholder = `__PYFSTRING_${placeholderCounter++}__`;\n      stringPlaceholders[placeholder] = match;\n\n      // Extract interpolated expressions from {...} and tokenize them separately\n      const expressions = [];\n      // Match {expression} but not escaped \\{\n      let expContent = match.match(/(?<!\\\\){([^{}]*)}/g);\n      if (expContent) {\n        expContent.forEach((exp) => {\n          expressions.push(exp.slice(1, -1)); // Remove { and }\n        });\n\n        // Tokenize each expression content\n        expressions.forEach((exp) => {\n          const expTokens = tokenizePython(exp); // Recursively tokenize expressions\n          tokens.push(...expTokens);\n        });\n      }\n\n      return placeholder;\n    }\n  );\n\n  // Handle other string literals (r-strings, normal strings)\n  const withoutSpecialStrings = withoutFStrings.replace(\n    /(?:r|b|rb|br)?(?:'''[\\s\\S]*?'''|\"\"\"[\\s\\S]*?\"\"\"|'(?:\\\\'|\\\\\\\\|[^'])*'|\"(?:\\\\\"|\\\\\\\\|[^\"])*\")/g,\n    (match) => {\n      const placeholder = `__PYSTRING_${placeholderCounter++}__`;\n      stringPlaceholders[placeholder] = match;\n      return placeholder;\n    }\n  );\n\n  // Handle decorators with placeholder - more comprehensive for complex decorators\n  const withoutDecorators = withoutSpecialStrings.replace(\n    /@([a-zA-Z][a-zA-Z0-9_.]*)(?:\\((?:[^)(]*|\\([^)(]*\\))*\\))?/g,\n    (match, decoratorName) => {\n      const placeholder = `__PYDECORATOR_${placeholderCounter++}__`;\n      stringPlaceholders[placeholder] = match;\n\n      // Add decorator name as token\n      tokens.push(decoratorName);\n\n      // If the decorator has parameters, extract and tokenize them\n      const paramMatch = match.match(/\\((.*)\\)/);\n      if (paramMatch && paramMatch[1]) {\n        const paramTokens = tokenizeGeneric(paramMatch[1]);\n        tokens.push(...paramTokens);\n      }\n\n      return placeholder;\n    }\n  );\n\n  // Handle Python-specific operators\n  const withSpecialOps = withoutDecorators\n    // Handle walrus operator :=\n    .replace(/:=/g, (match) => {\n      tokens.push(\"walrus_operator\");\n      return \" := \"; // Space-separated for tokenization\n    })\n    // Handle list splices with :\n    .replace(/\\[.*:.*\\]/g, (match) => {\n      tokens.push(\"slice_operation\");\n      // Process what's inside the brackets\n      const innerContent = match.slice(1, -1);\n      const sliceParts = innerContent.split(\":\");\n      sliceParts.forEach((part) => {\n        if (part.trim()) {\n          const partTokens = tokenizeGeneric(part.trim());\n          tokens.push(...partTokens);\n        }\n      });\n      return match; // Preserve for general tokenization\n    });\n\n  // Process lines with indentation awareness\n  const lines = withSpecialOps.split(\"\\n\");\n\n  // Track indentation levels\n  let previousIndentLevel = 0;\n\n  for (const line of lines) {\n    // Skip empty lines\n    if (line.trim() === \"\") continue;\n\n    // Count leading spaces/tabs to track indentation\n    const indentMatch = line.match(/^(\\s*)/);\n    const leadingSpaces = indentMatch ? indentMatch[1].length : 0;\n\n    if (leadingSpaces !== previousIndentLevel) {\n      if (leadingSpaces > previousIndentLevel) {\n        // Indentation increased - add token for indent\n        tokens.push(\"indent\");\n      } else {\n        // Indentation decreased - add token for dedent\n        // Add one dedent token for each level decreased\n        const dedentLevels = Math.floor(\n          (previousIndentLevel - leadingSpaces) / 4\n        );\n        for (let i = 0; i < dedentLevels; i++) {\n          tokens.push(\"dedent\");\n        }\n      }\n      previousIndentLevel = leadingSpaces;\n    }\n\n    // Tokenize the line content by first removing the leading whitespace\n    const lineContent = line.trim();\n    if (lineContent) {\n      // Check for keyword tokens\n      const pythonKeywords = [\n        \"def\",\n        \"class\",\n        \"lambda\",\n        \"return\",\n        \"yield\",\n        \"from\",\n        \"import\",\n        \"as\",\n        \"with\",\n        \"try\",\n        \"except\",\n        \"finally\",\n        \"raise\",\n        \"assert\",\n        \"if\",\n        \"elif\",\n        \"else\",\n        \"while\",\n        \"for\",\n        \"in\",\n        \"continue\",\n        \"break\",\n        \"pass\",\n        \"global\",\n        \"nonlocal\",\n        \"del\",\n        \"is\",\n        \"not\",\n        \"and\",\n        \"or\",\n        \"async\",\n        \"await\",\n        \"comprehension\",\n        \"self\",\n      ];\n\n      // Add line content keywords\n      for (const keyword of pythonKeywords) {\n        if (lineContent.includes(keyword)) {\n          const keywordRegex = new RegExp(`\\\\b${keyword}\\\\b`, \"g\");\n          if (keywordRegex.test(lineContent)) {\n            tokens.push(keyword);\n          }\n        }\n      }\n\n      // Now tokenize the whole line\n      const lineTokens = tokenizeGeneric(lineContent);\n      tokens.push(...lineTokens);\n    }\n  }\n\n  // Add keyword for Python-specific list operations\n  if (\n    withSpecialOps.includes(\"append(\") ||\n    withSpecialOps.includes(\".extend(\")\n  ) {\n    tokens.push(\"list_operation\");\n  }\n\n  // Add keyword for Python-specific dictionary operations\n  if (\n    withSpecialOps.includes(\".get(\") ||\n    withSpecialOps.includes(\".items()\") ||\n    withSpecialOps.includes(\".keys()\") ||\n    withSpecialOps.includes(\".values()\")\n  ) {\n    tokens.push(\"dict_operation\");\n  }\n\n  // Split snake_case identifiers\n  const snakeCaseTokens = [];\n  for (const token of tokens) {\n    // Skip placeholder tokens\n    if (token.startsWith(\"__\") && token.endsWith(\"__\")) {\n      snakeCaseTokens.push(token);\n      continue;\n    }\n\n    // Split snake_case\n    if (token.includes(\"_\")) {\n      const parts = token.split(\"_\").filter((part) => part.length > 0);\n      snakeCaseTokens.push(token); // Original token\n      snakeCaseTokens.push(...parts); // Parts of the token\n    } else {\n      snakeCaseTokens.push(token);\n    }\n  }\n\n  // Replace placeholders with their original values and process them\n  const finalTokens = [];\n  for (const token of snakeCaseTokens) {\n    if (stringPlaceholders[token]) {\n      if (token.startsWith(\"__PYFSTRING_\")) {\n        finalTokens.push(\"f_string\");\n      } else if (token.startsWith(\"__PYSTRING_\")) {\n        finalTokens.push(\"string_literal\");\n      } else if (token.startsWith(\"__PYDECORATOR_\")) {\n        finalTokens.push(\"decorator\");\n      } else {\n        finalTokens.push(token);\n      }\n\n      // For string placeholders, also tokenize their content\n      if (token.startsWith(\"__PYSTRING_\") || token.startsWith(\"__PYFSTRING_\")) {\n        const content = stringPlaceholders[token];\n        // Extract the content without prefix and quotes\n        let strContent = content;\n\n        // Handle different types of string literals\n        if (\n          strContent.startsWith(\"f\") ||\n          strContent.startsWith(\"r\") ||\n          strContent.startsWith(\"fr\") ||\n          strContent.startsWith(\"rf\") ||\n          strContent.startsWith(\"b\") ||\n          strContent.startsWith(\"rb\") ||\n          strContent.startsWith(\"br\")\n        ) {\n          const prefixLength = /^[a-z]+/.exec(strContent)[0].length;\n          strContent = strContent.substring(prefixLength);\n        }\n\n        // Remove quotes\n        strContent = strContent.replace(/^['\"]|['\"]$/g, \"\");\n        strContent = strContent.replace(/^'''|'''$/g, \"\");\n        strContent = strContent.replace(/^\"\"\"|\"\"\"$/g, \"\");\n\n        // Remove f-string interpolation markers\n        strContent = strContent.replace(/{[^{}]*}/g, \" \");\n\n        // Tokenize content\n        const contentTokens = tokenizeGeneric(strContent);\n        finalTokens.push(...contentTokens);\n      }\n    } else if (commentPlaceholders[token]) {\n      // Extract useful keywords from comments\n      finalTokens.push(\"code_comment\");\n\n      // Extract possible important terms from comments\n      const commentContent = commentPlaceholders[token]\n        .replace(/^#{1}/, \"\") // Remove #\n        .replace(/^'''|'''$/g, \"\") // Remove '''\n        .replace(/^\"\"\"|\"\"\"$/g, \"\"); // Remove \"\"\"\n\n      // Only use alphanumeric words from comments, skip punctuation and symbols\n      const commentTokens = commentContent\n        .split(/\\s+/)\n        .filter((word) => /^[a-z0-9_]{3,}$/i.test(word))\n        .map((word) => word.toLowerCase());\n\n      finalTokens.push(...commentTokens);\n    } else {\n      finalTokens.push(token);\n    }\n  }\n\n  return [...new Set(finalTokens)]; // Remove duplicates\n}\n\n/**\n * Java/C#-like language tokenization\n * Handles annotations, generics, access modifiers, lambda expressions\n *\n * @param {string} text - The Java or C# text to tokenize\n * @returns {string[]} An array of tokens\n */\nfunction tokenizeJavaLike(text) {\n  let tokens = [];\n\n  // Preserve comments for content analysis but mark them specially\n  const commentPlaceholders = {};\n  let commentCounter = 0;\n\n  // Remove block comments first\n  const withoutBlockComments = text.replace(/\\/\\*[\\s\\S]*?\\*\\//g, (match) => {\n    const placeholder = `__JAVA_COMMENT_BLOCK_${commentCounter++}__`;\n    commentPlaceholders[placeholder] = match;\n    return placeholder;\n  });\n\n  // Remove line comments\n  const withoutComments = withoutBlockComments.replace(\n    /\\/\\/[^\\n]*/g,\n    (match) => {\n      const placeholder = `__JAVA_COMMENT_LINE_${commentCounter++}__`;\n      commentPlaceholders[placeholder] = match;\n      return placeholder;\n    }\n  );\n\n  // Handle string literals with placeholders\n  const stringPlaceholders = {};\n  let placeholderCounter = 0;\n\n  // Handle string literals (support escaping)\n  const withoutStrings = withoutComments.replace(\n    /\"(?:[^\"\\\\]|\\\\.)*\"|'(?:[^'\\\\]|\\\\.)*'/g,\n    (match) => {\n      const placeholder = `__JAVASTRING_${placeholderCounter++}__`;\n      stringPlaceholders[placeholder] = match;\n      return placeholder;\n    }\n  );\n\n  // Handle annotations with parameters more comprehensively\n  const withoutAnnotations = withoutStrings.replace(\n    /@([a-zA-Z][a-zA-Z0-9_.]*)(?:\\s*\\((?:[^)(]*|\\([^)(]*\\))*\\))?/g,\n    (match, annotationName) => {\n      const placeholder = `__ANNOTATION_${placeholderCounter++}__`;\n      stringPlaceholders[placeholder] = match;\n\n      // Add annotation name as a token\n      tokens.push(\"annotation\");\n      tokens.push(annotationName.toLowerCase());\n\n      // Extract and process annotation parameters\n      const paramMatch = match.match(/\\((.*)\\)/);\n      if (paramMatch && paramMatch[1]) {\n        const params = paramMatch[1];\n\n        // Handle key-value pairs in annotations\n        const keyValuePairs = params.split(\",\");\n        for (const pair of keyValuePairs) {\n          const parts = pair.split(\"=\");\n          if (parts.length === 2) {\n            // Add parameter name as token\n            tokens.push(parts[0].trim());\n          }\n          // Tokenize the values\n          const valueTokens = tokenizeGeneric(pair);\n          tokens.push(...valueTokens);\n        }\n      }\n\n      return placeholder;\n    }\n  );\n\n  // Handle generics with better nesting support\n  // This pattern can handle nested generics like Map<String, List<Integer>>\n  const withoutGenerics = withoutAnnotations.replace(\n    /<([^<>]*(?:<[^<>]*(?:<[^<>]*>)*[^<>]*>)*[^<>]*)>/g,\n    (match) => {\n      const placeholder = `__GENERIC_${placeholderCounter++}__`;\n      stringPlaceholders[placeholder] = match;\n\n      // Add token for generic type usage\n      tokens.push(\"generic_type\");\n\n      // Process the content within the generic\n      // Remove the < and > delimiters\n      const content = match.slice(1, -1);\n\n      // Split by commas to get individual type parameters\n      const typeParams = content.split(/,(?![^<>]*>)/); // Split by commas not within angle brackets\n\n      // Process each type parameter\n      for (const param of typeParams) {\n        const paramTokens = tokenizeGeneric(param.trim());\n        tokens.push(...paramTokens);\n      }\n\n      return placeholder;\n    }\n  );\n\n  // Handle lambda expressions (Java: -> and C#: =>)\n  const withoutLambdas = withoutGenerics.replace(\n    /(?:\\(.*?\\)|[a-zA-Z_][a-zA-Z0-9_]*)\\s*(?:->|=>)\\s*(?:{[\\s\\S]*?}|[^;]*)/g,\n    (match) => {\n      const placeholder = `__LAMBDA_${placeholderCounter++}__`;\n      stringPlaceholders[placeholder] = match;\n\n      // Add token for lambda expression\n      tokens.push(\"lambda_expression\");\n\n      // Tokenize the entire lambda expression to extract parameter and body tokens\n      const lambdaTokens = tokenizeGeneric(match);\n      tokens.push(...lambdaTokens);\n\n      return placeholder;\n    }\n  );\n\n  // Extract and add access modifiers as specific tokens\n  const accessModifiers = [\n    \"public\",\n    \"private\",\n    \"protected\",\n    \"internal\",\n    \"static\",\n    \"final\",\n    \"abstract\",\n    \"override\",\n    \"virtual\",\n    \"readonly\",\n    \"const\",\n    \"sealed\",\n    \"partial\",\n    \"async\",\n    \"volatile\",\n    \"transient\",\n    \"synchronized\",\n    \"unsafe\",\n    \"extern\",\n  ];\n\n  let withAccessModifiers = withoutLambdas;\n  for (const modifier of accessModifiers) {\n    // Use word boundaries to match whole words\n    const regex = new RegExp(`\\\\b${modifier}\\\\b`, \"gi\");\n    withAccessModifiers = withAccessModifiers.replace(regex, (match) => {\n      tokens.push(match.toLowerCase());\n      tokens.push(\"access_modifier\");\n      return match;\n    });\n  }\n\n  // Handle package/namespace declarations\n  withAccessModifiers = withAccessModifiers.replace(\n    /\\b(?:package|namespace)\\s+([a-zA-Z_][a-zA-Z0-9_.]*)/g,\n    (match, packageName) => {\n      tokens.push(\"package_declaration\");\n\n      // Add the package name and its components\n      const packageParts = packageName.split(\".\");\n      tokens.push(packageName);\n      tokens.push(...packageParts);\n\n      return match;\n    }\n  );\n\n  // Handle import/using statements\n  withAccessModifiers = withAccessModifiers.replace(\n    /\\b(?:import|using)\\s+(?:static\\s+)?([a-zA-Z_][a-zA-Z0-9_.]*(?:\\.\\*)?)/g,\n    (match, importName) => {\n      tokens.push(\"import_statement\");\n\n      // Add the import name and its components\n      const importParts = importName.split(\".\");\n      tokens.push(importName);\n      // Remove wildcard * from last part if present\n      if (\n        importParts.length > 0 &&\n        importParts[importParts.length - 1] === \"*\"\n      ) {\n        importParts.pop();\n        tokens.push(\"wildcard_import\");\n      }\n      tokens.push(...importParts);\n\n      return match;\n    }\n  );\n\n  // Handle common C# LINQ expressions\n  if (/\\bfrom\\b.*\\bin\\b.*\\bselect\\b/i.test(withAccessModifiers)) {\n    tokens.push(\"linq_expression\");\n\n    // Extract common LINQ keywords\n    const linqKeywords = [\n      \"from\",\n      \"in\",\n      \"select\",\n      \"where\",\n      \"group\",\n      \"by\",\n      \"into\",\n      \"orderby\",\n      \"join\",\n      \"let\",\n      \"on\",\n      \"equals\",\n    ];\n\n    for (const keyword of linqKeywords) {\n      const regex = new RegExp(`\\\\b${keyword}\\\\b`, \"gi\");\n      if (regex.test(withAccessModifiers)) {\n        tokens.push(`linq_${keyword}`);\n      }\n    }\n  }\n\n  // Add remaining tokens\n  const mainTokens = tokenizeGeneric(withAccessModifiers);\n  tokens.push(...mainTokens);\n\n  // Handle camelCase and PascalCase with more specialized type names\n  const processedTokens = [];\n  for (const token of tokens) {\n    // Skip placeholder tokens\n    if (token.startsWith(\"__\") && token.endsWith(\"__\")) {\n      processedTokens.push(token);\n      continue;\n    }\n\n    // Check if token might be a fully qualified name (contains dots)\n    if (token.includes(\".\")) {\n      const parts = token.split(\".\");\n      processedTokens.push(token); // Add the full token\n      processedTokens.push(...parts); // Add individual parts\n      continue;\n    }\n\n    // Split PascalCase and camelCase\n    // Add original token first\n    processedTokens.push(token);\n\n    // Then add split tokens if there's a case change\n    if (/[a-z][A-Z]/.test(token)) {\n      const parts = token\n        .replace(/([a-z])([A-Z])/g, \"$1 $2\")\n        .toLowerCase()\n        .split(\" \");\n\n      if (parts.length > 1) {\n        processedTokens.push(...parts);\n      }\n    }\n  }\n\n  // Replace placeholders with their original values\n  const finalTokens = [];\n  for (const token of processedTokens) {\n    if (stringPlaceholders[token]) {\n      // Add information about what kind of structure this is\n      if (token.startsWith(\"__JAVASTRING_\")) {\n        finalTokens.push(\"string_literal\");\n      } else if (token.startsWith(\"__ANNOTATION_\")) {\n        finalTokens.push(\"annotation\");\n      } else if (token.startsWith(\"__GENERIC_\")) {\n        finalTokens.push(\"generic\");\n      } else if (token.startsWith(\"__LAMBDA_\")) {\n        finalTokens.push(\"lambda\");\n      } else {\n        finalTokens.push(token);\n      }\n\n      // For string literals, add their content as tokens\n      if (token.startsWith(\"__JAVASTRING_\")) {\n        const content = stringPlaceholders[token];\n        // Extract the content without the quotes\n        const strContent = content.replace(/^\"|\"$/g, \"\").replace(/^'|'$/g, \"\");\n\n        // Only tokenize non-empty content\n        if (strContent.trim().length > 0) {\n          const contentTokens = tokenizeGeneric(strContent);\n          finalTokens.push(...contentTokens);\n        }\n      }\n    } else if (commentPlaceholders[token]) {\n      // Extract useful keywords from comments\n      finalTokens.push(\"code_comment\");\n\n      // Extract possible important terms from comments\n      const commentContent = commentPlaceholders[token]\n        .replace(/^\\/\\*|\\*\\/$/g, \"\") // Remove /* */\n        .replace(/^\\/\\//g, \"\"); // Remove //\n\n      // Only use alphanumeric words from comments, skip punctuation and symbols\n      const commentTokens = commentContent\n        .split(/\\s+/)\n        .filter((word) => /^[a-z0-9_]{3,}$/i.test(word))\n        .map((word) => word.toLowerCase());\n\n      finalTokens.push(...commentTokens);\n    } else {\n      finalTokens.push(token);\n    }\n  }\n\n  return [...new Set(finalTokens)]; // Remove duplicates\n}\n\n/**\n * Ruby-specific tokenization\n * Handles symbols, blocks, string interpolation, and range operators\n *\n * @param {string} text - The Ruby text to tokenize\n * @returns {string[]} An array of tokens\n */\nfunction tokenizeRuby(text) {\n  let tokens = [];\n\n  // Preserve comments for content analysis but mark them specially\n  const commentPlaceholders = {};\n  let commentCounter = 0;\n\n  // Remove block comments (=begin...=end)\n  const withoutBlockComments = text.replace(/=begin[\\s\\S]*?=end/g, (match) => {\n    const placeholder = `__RUBY_COMMENT_BLOCK_${commentCounter++}__`;\n    commentPlaceholders[placeholder] = match;\n    return placeholder;\n  });\n\n  // Remove line comments\n  const withoutComments = withoutBlockComments.replace(/#[^\\n]*/g, (match) => {\n    const placeholder = `__RUBY_COMMENT_LINE_${commentCounter++}__`;\n    commentPlaceholders[placeholder] = match;\n    return placeholder;\n  });\n\n  // Handle string literals and placeholders\n  const stringPlaceholders = {};\n  let placeholderCounter = 0;\n\n  // Handle string interpolation (#{...}) in double-quoted strings\n  // This is similar to f-strings in Python\n  const withoutInterpolation = withoutComments.replace(\n    /\"(?:[^\"\\\\]|\\\\.|#\\{[^}]*\\})*\"/g,\n    (match) => {\n      const placeholder = `__RUBY_INTERPOLATED_STRING_${placeholderCounter++}__`;\n      stringPlaceholders[placeholder] = match;\n\n      // Extract interpolated expressions from #{...} and tokenize them separately\n      const expressions = [];\n      let expContent = match.match(/#\\{([^}]*)\\}/g);\n      if (expContent) {\n        expContent.forEach((exp) => {\n          expressions.push(exp.slice(2, -1)); // Remove #{ and }\n        });\n\n        // Tokenize each expression content\n        expressions.forEach((exp) => {\n          const expTokens = tokenizeRuby(exp); // Recursively tokenize expressions\n          tokens.push(...expTokens);\n        });\n      }\n\n      return placeholder;\n    }\n  );\n\n  // Handle other string literals (including %q, %Q, heredocs)\n  const withoutStrings = withoutInterpolation.replace(\n    /('(?:[^'\\\\]|\\\\.)*'|%[qQ]?\\{(?:[^\\\\}]|\\\\.)*\\}|%[qQ]?\\((?:[^\\\\)]|\\\\.)*\\)|%[qQ]?\\[(?:[^\\\\]]|\\\\.)*\\]|%[qQ]?<(?:[^\\\\>]|\\\\.)*>|<<-?(['\"]?)(\\w+)\\1[\\s\\S]*?\\2)/g,\n    (match) => {\n      const placeholder = `__RUBY_STRING_${placeholderCounter++}__`;\n      stringPlaceholders[placeholder] = match;\n      return placeholder;\n    }\n  );\n\n  // Handle regular expressions with placeholder\n  const withoutRegexps = withoutStrings.replace(\n    /\\/(?:[^\\/\\\\]|\\\\.)*\\/[iomxneus]*/g,\n    (match) => {\n      const placeholder = `__RUBY_REGEXP_${placeholderCounter++}__`;\n      stringPlaceholders[placeholder] = match;\n      tokens.push(\"regexp\");\n      return placeholder;\n    }\n  );\n\n  // Handle Ruby symbols with placeholder\n  const withoutSymbols = withoutRegexps.replace(\n    /:(?:@?[a-zA-Z_][a-zA-Z0-9_]*(?:[?!]|=(?!=))?|\"(?:[^\"\\\\]|\\\\.)*\"|'(?:[^'\\\\]|\\\\.)*'|\\S+)/g,\n    (match) => {\n      const placeholder = `__RUBY_SYMBOL_${placeholderCounter++}__`;\n      stringPlaceholders[placeholder] = match;\n\n      // Extract symbol name without the colon\n      const symbolName = match.substring(1);\n      tokens.push(\"symbol\");\n      tokens.push(`symbol_${symbolName}`);\n\n      // Add the base name without question mark or exclamation mark\n      if (symbolName.endsWith(\"?\") || symbolName.endsWith(\"!\")) {\n        tokens.push(`symbol_${symbolName.slice(0, -1)}`);\n      }\n\n      return placeholder;\n    }\n  );\n\n  // Handle blocks (do...end and {...}) with placeholder\n  let withoutBlocks = withoutSymbols;\n\n  // do...end blocks\n  withoutBlocks = withoutBlocks.replace(\n    /\\bdo\\s*(?:\\|[^|]*\\|)?[\\s\\S]*?\\bend\\b/g,\n    (match) => {\n      const placeholder = `__RUBY_BLOCK_DO_${placeholderCounter++}__`;\n      stringPlaceholders[placeholder] = match;\n\n      tokens.push(\"block_do_end\");\n\n      // Extract block parameters\n      const paramMatch = match.match(/\\|\\s*([^|]*)\\s*\\|/);\n      if (paramMatch && paramMatch[1]) {\n        const params = paramMatch[1].split(\",\");\n        params.forEach((param) => {\n          tokens.push(param.trim());\n        });\n      }\n\n      // Tokenize block content\n      const blockContent = match\n        .replace(/\\bdo\\s*(?:\\|[^|]*\\|)?/, \"\") // Remove 'do' and params\n        .replace(/\\bend\\b$/, \"\"); // Remove 'end'\n\n      const contentTokens = tokenizeGeneric(blockContent);\n      tokens.push(...contentTokens);\n\n      return placeholder;\n    }\n  );\n\n  // {...} blocks\n  withoutBlocks = withoutBlocks.replace(\n    /\\{(?:\\s*\\|[^|]*\\|\\s*)?[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}/g,\n    (match) => {\n      // Skip if it looks like a Hash literal rather than a block\n      if (/^\\{\\s*:/.test(match) || /^\\{\\s*['\"]/.test(match)) {\n        return match; // Process as a Hash literal (key-value pairs)\n      }\n\n      const placeholder = `__RUBY_BLOCK_BRACE_${placeholderCounter++}__`;\n      stringPlaceholders[placeholder] = match;\n\n      tokens.push(\"block_brace\");\n\n      // Extract block parameters\n      const paramMatch = match.match(/\\|\\s*([^|]*)\\s*\\|/);\n      if (paramMatch && paramMatch[1]) {\n        const params = paramMatch[1].split(\",\");\n        params.forEach((param) => {\n          tokens.push(param.trim());\n        });\n      }\n\n      // Process the content without parameters\n      let blockContent = match.slice(1, -1); // Remove { and }\n      if (paramMatch) {\n        blockContent = blockContent.replace(/\\|\\s*[^|]*\\s*\\|/, \"\");\n      }\n\n      const contentTokens = tokenizeGeneric(blockContent);\n      tokens.push(...contentTokens);\n\n      return placeholder;\n    }\n  );\n\n  // Handle range operators (.., ...)\n  let withRangeOps = withoutBlocks.replace(/\\.\\.(\\.)?/g, (match) => {\n    tokens.push(\n      match === \"..\" ? \"range_operator_inclusive\" : \"range_operator_exclusive\"\n    );\n    return \" \" + match + \" \"; // Space-padded for tokenization\n  });\n\n  // Handle Ruby method definition with symbols\n  withRangeOps = withRangeOps.replace(\n    /\\bdef\\s+(?:self\\.)?([a-zA-Z_][a-zA-Z0-9_]*[?!=]?)/g,\n    (match, methodName) => {\n      tokens.push(\"method_definition\");\n      tokens.push(methodName);\n\n      // Add method name without special character suffix\n      if (\n        methodName.endsWith(\"?\") ||\n        methodName.endsWith(\"!\") ||\n        methodName.endsWith(\"=\")\n      ) {\n        tokens.push(methodName.slice(0, -1));\n      }\n\n      return match;\n    }\n  );\n\n  // Handle class and module definitions\n  withRangeOps = withRangeOps.replace(\n    /\\b(?:class|module)\\s+([A-Z][a-zA-Z0-9_]*(?:::[A-Z][a-zA-Z0-9_]*)*)/g,\n    (match, className) => {\n      tokens.push(\n        match.startsWith(\"class\") ? \"class_definition\" : \"module_definition\"\n      );\n\n      // Add class/module name\n      tokens.push(className);\n\n      // Add namespace components\n      if (className.includes(\"::\")) {\n        const parts = className.split(\"::\");\n        tokens.push(...parts);\n      }\n\n      return match;\n    }\n  );\n\n  // Tokenize what's left with generic tokenization\n  const genericTokens = tokenizeGeneric(withRangeOps);\n\n  // Add common Ruby keywords if present\n  const rubyKeywords = [\n    \"if\",\n    \"unless\",\n    \"else\",\n    \"elsif\",\n    \"end\",\n    \"begin\",\n    \"rescue\",\n    \"ensure\",\n    \"while\",\n    \"until\",\n    \"for\",\n    \"break\",\n    \"next\",\n    \"redo\",\n    \"retry\",\n    \"return\",\n    \"super\",\n    \"self\",\n    \"nil\",\n    \"true\",\n    \"false\",\n    \"and\",\n    \"or\",\n    \"not\",\n    \"yield\",\n  ];\n\n  for (const keyword of rubyKeywords) {\n    const regex = new RegExp(`\\\\b${keyword}\\\\b`, \"g\");\n    if (regex.test(withRangeOps)) {\n      tokens.push(keyword);\n    }\n  }\n\n  tokens.push(...genericTokens);\n\n  // Process tokens for Ruby method calling convention\n  const processedTokens = [];\n  for (const token of tokens) {\n    // Skip placeholder tokens\n    if (token.startsWith(\"__RUBY_\")) {\n      processedTokens.push(token);\n      continue;\n    }\n\n    processedTokens.push(token);\n\n    // Also add version without trailing ? or !\n    if (token.endsWith(\"?\") || token.endsWith(\"!\")) {\n      processedTokens.push(token.slice(0, -1));\n    }\n\n    // Also add version without = for attribute setters\n    if (\n      token.endsWith(\"=\") &&\n      ![\"==\", \"!=\", \">=\", \"<=\", \"=>\"].includes(token)\n    ) {\n      processedTokens.push(token.slice(0, -1));\n    }\n  }\n\n  // Replace placeholders with their original values and extract content\n  const finalTokens = [];\n  for (const token of processedTokens) {\n    if (stringPlaceholders[token]) {\n      // Categorize by token type\n      if (\n        token.startsWith(\"__RUBY_STRING_\") ||\n        token.startsWith(\"__RUBY_INTERPOLATED_STRING_\")\n      ) {\n        finalTokens.push(\"string_literal\");\n\n        // Extract and tokenize string content\n        const content = stringPlaceholders[token];\n        let strContent = content;\n\n        // Remove quotes\n        if (strContent.startsWith(\"'\") && strContent.endsWith(\"'\")) {\n          strContent = strContent.slice(1, -1);\n        } else if (strContent.startsWith('\"') && strContent.endsWith('\"')) {\n          strContent = strContent.slice(1, -1);\n        } else if (strContent.startsWith(\"%q\") || strContent.startsWith(\"%Q\")) {\n          // Handle %q/%Q strings\n          strContent = strContent.slice(3, -1);\n        }\n\n        // Remove interpolation markers\n        strContent = strContent.replace(/#\\{[^}]*\\}/g, \" \");\n\n        // Tokenize content if not empty\n        if (strContent.trim()) {\n          const contentTokens = tokenizeGeneric(strContent);\n          finalTokens.push(...contentTokens);\n        }\n      } else if (token.startsWith(\"__RUBY_SYMBOL_\")) {\n        finalTokens.push(\"symbol\");\n      } else if (token.startsWith(\"__RUBY_BLOCK_\")) {\n        finalTokens.push(\"block\");\n      } else if (token.startsWith(\"__RUBY_REGEXP_\")) {\n        finalTokens.push(\"regexp\");\n      } else {\n        finalTokens.push(token);\n      }\n    } else if (commentPlaceholders[token]) {\n      // Extract useful keywords from comments\n      finalTokens.push(\"code_comment\");\n\n      // Extract possible important terms from comments\n      const commentContent = commentPlaceholders[token]\n        .replace(/^#/, \"\") // Remove # for line comments\n        .replace(/^=begin\\s*|\\s*=end$/g, \"\"); // Remove =begin/=end for block comments\n\n      // Only use alphanumeric words from comments, skip punctuation and symbols\n      const commentTokens = commentContent\n        .split(/\\s+/)\n        .filter((word) => /^[a-z0-9_]{3,}$/i.test(word))\n        .map((word) => word.toLowerCase());\n\n      finalTokens.push(...commentTokens);\n    } else {\n      finalTokens.push(token);\n    }\n  }\n\n  return [...new Set(finalTokens)]; // Remove duplicates\n}\n\n/**\n * Go-specific tokenization\n * Handles struct tags, goroutines, interfaces, and Go-specific operators\n *\n * @param {string} text - The Go text to tokenize\n * @returns {string[]} An array of tokens\n */\nfunction tokenizeGo(text) {\n  let tokens = [];\n\n  // Preserve comments for content analysis but mark them specially\n  const commentPlaceholders = {};\n  let commentCounter = 0;\n\n  // Remove block comments first\n  const withoutBlockComments = text.replace(/\\/\\*[\\s\\S]*?\\*\\//g, (match) => {\n    const placeholder = `__GO_COMMENT_BLOCK_${commentCounter++}__`;\n    commentPlaceholders[placeholder] = match;\n    return placeholder;\n  });\n\n  // Remove line comments\n  const withoutComments = withoutBlockComments.replace(\n    /\\/\\/[^\\n]*/g,\n    (match) => {\n      const placeholder = `__GO_COMMENT_LINE_${commentCounter++}__`;\n      commentPlaceholders[placeholder] = match;\n      return placeholder;\n    }\n  );\n\n  // Handle string literals and placeholders\n  const stringPlaceholders = {};\n  let placeholderCounter = 0;\n\n  // Handle raw string literals with backticks\n  const withoutRawStrings = withoutComments.replace(/`[^`]*`/g, (match) => {\n    const placeholder = `__GO_RAW_STRING_${placeholderCounter++}__`;\n    stringPlaceholders[placeholder] = match;\n    return placeholder;\n  });\n\n  // Handle regular string literals\n  const withoutStrings = withoutRawStrings.replace(\n    /\"(?:[^\"\\\\]|\\\\.)*\"/g,\n    (match) => {\n      const placeholder = `__GO_STRING_${placeholderCounter++}__`;\n      stringPlaceholders[placeholder] = match;\n      return placeholder;\n    }\n  );\n\n  // Handle rune literals\n  const withoutRunes = withoutStrings.replace(/'(?:[^'\\\\]|\\\\.)*'/g, (match) => {\n    const placeholder = `__GO_RUNE_${placeholderCounter++}__`;\n    stringPlaceholders[placeholder] = match;\n    return placeholder;\n  });\n\n  // Handle struct tags in field definitions\n  // These are special string literals used for annotations in struct fields\n  const withoutStructTags = withoutRunes.replace(\n    /`(?:[a-zA-Z0-9_]+:\"[^\"]*\")+`/g,\n    (match) => {\n      const placeholder = `__GO_STRUCT_TAG_${placeholderCounter++}__`;\n      stringPlaceholders[placeholder] = match;\n\n      // Process struct tag content to extract key information\n      tokens.push(\"struct_tag\");\n\n      // Extract tag keys and values\n      const tagPairs = match.slice(1, -1).split(\" \"); // Remove backticks and split by space\n      for (const pair of tagPairs) {\n        if (!pair.trim()) continue;\n\n        // Split by colon and extract key/value\n        const [key, quotedValue] = pair.split(\":\");\n        if (key && quotedValue) {\n          tokens.push(`tag_${key}`);\n\n          // Extract value without quotes\n          const value = quotedValue.replace(/^\"|\"$/g, \"\");\n          if (value) {\n            // If it's a comma-separated list, add each part\n            if (value.includes(\",\")) {\n              const valueParts = value.split(\",\");\n              tokens.push(...valueParts);\n            } else {\n              tokens.push(value);\n            }\n          }\n        }\n      }\n\n      return placeholder;\n    }\n  );\n\n  // Handle Go channel operations (<-, ->)\n  const withoutChannelOps = withoutStructTags.replace(/<-/g, (match) => {\n    tokens.push(\"channel_operation\");\n    return \" <- \"; // Space-separated for tokenization\n  });\n\n  // Handle goroutines (go keyword followed by function call or func literal)\n  const withoutGoroutines = withoutChannelOps.replace(\n    /\\bgo\\s+(?:func\\b|[a-zA-Z_][a-zA-Z0-9_]*\\s*\\()/g,\n    (match) => {\n      tokens.push(\"goroutine\");\n\n      // Extract function name if it's a function call\n      const funcCallMatch = match.match(/go\\s+([a-zA-Z_][a-zA-Z0-9_]*)/);\n      if (funcCallMatch && funcCallMatch[1]) {\n        tokens.push(funcCallMatch[1]);\n      }\n\n      return match;\n    }\n  );\n\n  // Handle select statement with cases\n  const withoutSelect = withoutGoroutines.replace(\n    /\\bselect\\s*{[\\s\\S]*?}/g,\n    (match) => {\n      tokens.push(\"select_statement\");\n\n      // Extract cases\n      const cases = match.match(/case\\s+[^:]+:/g);\n      if (cases) {\n        for (const caseStr of cases) {\n          // Tokenize case content\n          const caseContent = caseStr.slice(4, -1).trim(); // Remove \"case\" and \":\"\n          const caseTokens = tokenizeGeneric(caseContent);\n          tokens.push(...caseTokens);\n        }\n      }\n\n      return match;\n    }\n  );\n\n  // Handle defer statements\n  const withoutDefer = withoutSelect.replace(\n    /\\bdefer\\s+[a-zA-Z_][a-zA-Z0-9_]*\\s*\\(/g,\n    (match) => {\n      tokens.push(\"defer\");\n\n      // Extract function name\n      const funcMatch = match.match(/defer\\s+([a-zA-Z_][a-zA-Z0-9_]*)/);\n      if (funcMatch && funcMatch[1]) {\n        tokens.push(funcMatch[1]);\n      }\n\n      return match;\n    }\n  );\n\n  // Handle type declarations with interfaces and structs\n  const withoutTypeDecls = withoutDefer.replace(\n    /\\btype\\s+([a-zA-Z_][a-zA-Z0-9_]*)\\s+(?:struct|interface)\\s*{[\\s\\S]*?}/g,\n    (match, typeName) => {\n      tokens.push(\"type_declaration\");\n      tokens.push(typeName);\n\n      // Check if it's a struct or interface\n      if (match.includes(\"struct\")) {\n        tokens.push(\"struct_type\");\n\n        // Extract field names and types\n        const fieldMatches = match.match(\n          /([a-zA-Z_][a-zA-Z0-9_]*)\\s+([a-zA-Z_][a-zA-Z0-9_.*[\\]]*)/g\n        );\n        if (fieldMatches) {\n          for (const fieldMatch of fieldMatches) {\n            const parts = fieldMatch.trim().split(/\\s+/);\n            if (parts.length >= 2) {\n              tokens.push(parts[0]); // Field name\n              tokens.push(parts[1]); // Field type\n            }\n          }\n        }\n      } else if (match.includes(\"interface\")) {\n        tokens.push(\"interface_type\");\n\n        // Extract method signatures\n        const methodMatches = match.match(\n          /([a-zA-Z_][a-zA-Z0-9_]*)\\s*\\([\\s\\S]*?\\)(?:\\s*\\([\\s\\S]*?\\))?\\s*[,{]/g\n        );\n        if (methodMatches) {\n          for (const methodMatch of methodMatches) {\n            const methodName = methodMatch.match(/([a-zA-Z_][a-zA-Z0-9_]*)/);\n            if (methodName && methodName[1]) {\n              tokens.push(methodName[1]);\n            }\n          }\n        }\n      }\n\n      return match;\n    }\n  );\n\n  // Handle Go-specific builtins\n  let withBuiltins = withoutTypeDecls;\n  const goBuiltins = [\n    \"make\",\n    \"new\",\n    \"len\",\n    \"cap\",\n    \"append\",\n    \"copy\",\n    \"delete\",\n    \"close\",\n    \"complex\",\n    \"real\",\n    \"imag\",\n    \"panic\",\n    \"recover\",\n  ];\n\n  for (const builtin of goBuiltins) {\n    const regex = new RegExp(`\\\\b${builtin}\\\\s*\\\\(`, \"g\");\n    withBuiltins = withBuiltins.replace(regex, (match) => {\n      tokens.push(`builtin_${builtin}`);\n      return match;\n    });\n  }\n\n  // Add Go keywords and common patterns\n  const goKeywords = [\n    \"package\",\n    \"import\",\n    \"func\",\n    \"return\",\n    \"var\",\n    \"const\",\n    \"type\",\n    \"struct\",\n    \"interface\",\n    \"map\",\n    \"chan\",\n    \"go\",\n    \"select\",\n    \"case\",\n    \"default\",\n    \"defer\",\n    \"if\",\n    \"else\",\n    \"switch\",\n    \"for\",\n    \"range\",\n    \"continue\",\n    \"break\",\n    \"fallthrough\",\n    \"goto\",\n    \"nil\",\n    \"iota\",\n    \"true\",\n    \"false\",\n  ];\n\n  for (const keyword of goKeywords) {\n    const regex = new RegExp(`\\\\b${keyword}\\\\b`, \"g\");\n    if (regex.test(withBuiltins)) {\n      tokens.push(keyword);\n    }\n  }\n\n  // Process the remaining text with generic tokenization\n  const genericTokens = tokenizeGeneric(withBuiltins);\n  tokens.push(...genericTokens);\n\n  // Handle camelCase for method names which is common in Go\n  const processedTokens = [];\n  for (const token of tokens) {\n    // Skip placeholders\n    if (token.startsWith(\"__GO_\")) {\n      processedTokens.push(token);\n      continue;\n    }\n\n    processedTokens.push(token);\n\n    // Split camelCase\n    if (/[a-z][A-Z]/.test(token)) {\n      const parts = token\n        .replace(/([a-z])([A-Z])/g, \"$1 $2\")\n        .toLowerCase()\n        .split(\" \");\n      if (parts.length > 1) {\n        processedTokens.push(...parts);\n      }\n    }\n  }\n\n  // Replace placeholders with their original values and process content\n  const finalTokens = [];\n  for (const token of processedTokens) {\n    if (stringPlaceholders[token]) {\n      if (token.startsWith(\"__GO_STRING_\")) {\n        finalTokens.push(\"string_literal\");\n\n        // Extract and tokenize string content\n        const content = stringPlaceholders[token];\n        // Remove quotes and tokenize content\n        const strContent = content.slice(1, -1);\n\n        // Only tokenize non-empty content\n        if (strContent.trim().length > 0) {\n          const contentTokens = tokenizeGeneric(strContent);\n          finalTokens.push(...contentTokens);\n        }\n      } else if (token.startsWith(\"__GO_RAW_STRING_\")) {\n        finalTokens.push(\"raw_string_literal\");\n\n        // Extract content from raw string\n        const content = stringPlaceholders[token];\n        // Remove backticks\n        const rawContent = content.slice(1, -1);\n\n        // Handle multiline raw strings specially\n        if (rawContent.includes(\"\\n\")) {\n          // Process each line separately\n          const lines = rawContent.split(\"\\n\");\n          for (const line of lines) {\n            if (line.trim()) {\n              const lineTokens = tokenizeGeneric(line.trim());\n              finalTokens.push(...lineTokens);\n            }\n          }\n        } else if (rawContent.trim()) {\n          const contentTokens = tokenizeGeneric(rawContent);\n          finalTokens.push(...contentTokens);\n        }\n      } else if (token.startsWith(\"__GO_STRUCT_TAG_\")) {\n        finalTokens.push(\"struct_tag\");\n      } else if (token.startsWith(\"__GO_RUNE_\")) {\n        finalTokens.push(\"rune_literal\");\n      } else {\n        finalTokens.push(token);\n      }\n    } else if (commentPlaceholders[token]) {\n      // Extract useful keywords from comments\n      finalTokens.push(\"code_comment\");\n\n      // Extract possible important terms from comments\n      const commentContent = commentPlaceholders[token]\n        .replace(/^\\/\\*|\\*\\/$/g, \"\") // Remove /* */\n        .replace(/^\\/\\//g, \"\"); // Remove //\n\n      // Only use alphanumeric words from comments, skip punctuation and symbols\n      const commentTokens = commentContent\n        .split(/\\s+/)\n        .filter((word) => /^[a-z0-9_]{3,}$/i.test(word))\n        .map((word) => word.toLowerCase());\n\n      finalTokens.push(...commentTokens);\n    } else {\n      finalTokens.push(token);\n    }\n  }\n\n  return [...new Set(finalTokens)]; // Remove duplicates\n}\n\n/**\n * Simple word stemming function that handles common suffixes\n *\n * @param {string} word - The word to stem\n * @returns {string} The stemmed word\n */\nexport function stem(word) {\n  // Make sure input is a string and lowercase\n  if (typeof word !== \"string\") return \"\";\n  const lowerWord = word.toLowerCase();\n\n  // Handle empty strings\n  if (lowerWord.length <= 2) return lowerWord;\n\n  // Simple suffix removal rules\n  if (lowerWord.endsWith(\"ing\")) {\n    // ending -> end, running -> run\n    const stemmed = lowerWord.slice(0, -3);\n    if (stemmed.length > 2) return stemmed;\n    return lowerWord;\n  }\n\n  if (lowerWord.endsWith(\"ed\")) {\n    // ended -> end, created -> creat\n    const stemmed = lowerWord.slice(0, -2);\n    if (stemmed.length > 2) return stemmed;\n    return lowerWord;\n  }\n\n  if (lowerWord.endsWith(\"s\") && !lowerWord.endsWith(\"ss\")) {\n    // files -> file, classes -> class\n    return lowerWord.slice(0, -1);\n  }\n\n  if (lowerWord.endsWith(\"es\")) {\n    // classes -> class, boxes -> box\n    return lowerWord.slice(0, -2);\n  }\n\n  if (lowerWord.endsWith(\"ly\")) {\n    // quickly -> quick\n    return lowerWord.slice(0, -2);\n  }\n\n  if (lowerWord.endsWith(\"er\")) {\n    // faster -> fast\n    return lowerWord.slice(0, -2);\n  }\n\n  // Default: return word as is\n  return lowerWord;\n}\n", "/**\n * ContextIndexerLogic.js\n *\n * Provides functions for indexing code files and extracting structured information\n * about code entities and their relationships.\n */\n\nimport { v4 as uuidv4 } from \"uuid\";\nimport crypto from \"crypto\";\nimport path from \"path\";\nimport * as acorn from \"acorn\";\nimport { executeQuery } from \"../db.js\";\nimport { tokenize, extractKeywords } from \"./TextTokenizerLogic.js\";\nimport { addRelationship } from \"./RelationshipContextManagerLogic.js\";\nimport { buildAST } from \"./CodeStructureAnalyzerLogic.js\";\n\n/**\n * Calculate SHA-256 hash of content\n *\n * @param {string} content - Content to hash\n * @returns {string} SHA-256 hash as hex string\n */\nfunction calculateContentHash(content) {\n  return crypto.createHash(\"sha256\").update(content).digest(\"hex\");\n}\n\n/**\n * Extract filename from path\n *\n * @param {string} filePath - Path to file\n * @returns {string} Filename without directory\n */\nfunction extractFilename(filePath) {\n  return path.basename(filePath);\n}\n\n/**\n * Detect language from file extension if not provided\n *\n * @param {string} filePath - Path to file\n * @param {string} languageHint - Language hint\n * @returns {string} Detected language\n */\nfunction detectLanguage(filePath, languageHint) {\n  if (languageHint) {\n    return languageHint.toLowerCase();\n  }\n\n  const extension = path.extname(filePath).toLowerCase();\n\n  const extensionMap = {\n    \".js\": \"javascript\",\n    \".jsx\": \"javascript\",\n    \".ts\": \"typescript\",\n    \".tsx\": \"typescript\",\n    \".py\": \"python\",\n    \".rb\": \"ruby\",\n    \".java\": \"java\",\n    \".go\": \"go\",\n    \".rs\": \"rust\",\n    \".php\": \"php\",\n    \".c\": \"c\",\n    \".cpp\": \"cpp\",\n    \".h\": \"c\",\n    \".hpp\": \"cpp\",\n    \".cs\": \"csharp\",\n    \".swift\": \"swift\",\n    \".kt\": \"kotlin\",\n    \".html\": \"html\",\n    \".css\": \"css\",\n    \".scss\": \"scss\",\n    \".json\": \"json\",\n    \".md\": \"markdown\",\n    \".xml\": \"xml\",\n    \".yaml\": \"yaml\",\n    \".yml\": \"yaml\",\n  };\n\n  return extensionMap[extension] || \"unknown\";\n}\n\n/**\n * Extract line number from character position\n *\n * @param {string} content - File content\n * @param {number} position - Character position\n * @returns {number} Line number\n */\nfunction getLineFromPosition(content, position) {\n  const lines = content.substring(0, position).split(\"\\n\");\n  return lines.length;\n}\n\n/**\n * Extract code entities using regex for languages without AST support\n *\n * @param {string} content - File content\n * @param {string} language - Language of the file\n * @returns {Array} Extracted entities\n */\nfunction extractEntitiesWithRegex(content, language) {\n  const entities = [];\n\n  // Common patterns for different languages\n  const patterns = {\n    // Function patterns\n    function: {\n      python: /def\\s+([a-zA-Z_][a-zA-Z0-9_]*)\\s*\\([^)]*\\)\\s*:/g,\n      ruby: /def\\s+([a-zA-Z_][a-zA-Z0-9_]*)\\s*(\\([^)]*\\))?\\s*(do|\\n)/g,\n      java: /(public|private|protected|static|\\s) +[\\w\\<\\>\\[\\]]+\\s+(\\w+) *\\([^\\)]*\\) *(\\{?|[^;])/g,\n      go: /func\\s+([a-zA-Z_][a-zA-Z0-9_]*)\\s*\\([^)]*\\)\\s*(?:\\([^)]*\\))?\\s*\\{/g,\n      php: /function\\s+([a-zA-Z_][a-zA-Z0-9_]*)\\s*\\([^)]*\\)\\s*\\{/g,\n      default: /function\\s+([a-zA-Z_][a-zA-Z0-9_]*)\\s*\\([^)]*\\)\\s*\\{/g,\n    },\n    // Class patterns\n    class: {\n      python: /class\\s+([a-zA-Z_][a-zA-Z0-9_]*)\\s*(\\([^)]*\\))?\\s*:/g,\n      ruby: /class\\s+([a-zA-Z_][a-zA-Z0-9_]*)\\s*((<|::)\\s*[A-Za-z0-9_:]*)?/g,\n      java: /class\\s+([a-zA-Z_][a-zA-Z0-9_]*)\\s*(extends\\s+[A-Za-z0-9_]+)?\\s*(implements\\s+[A-Za-z0-9_,\\s]+)?\\s*\\{/g,\n      go: /type\\s+([a-zA-Z_][a-zA-Z0-9_]*)\\s+struct\\s*\\{/g,\n      php: /class\\s+([a-zA-Z_][a-zA-Z0-9_]*)\\s*(extends\\s+[A-Za-z0-9_]+)?\\s*(implements\\s+[A-Za-z0-9_,\\s]+)?\\s*\\{/g,\n      default:\n        /class\\s+([a-zA-Z_][a-zA-Z0-9_]*)\\s*(extends\\s+[A-Za-z0-9_]+)?\\s*\\{/g,\n    },\n    // Variable/constant patterns\n    variable: {\n      python: /(^|\\s)([a-zA-Z_][a-zA-Z0-9_]*)\\s*=\\s*(?!==)/g,\n      ruby: /(^|\\s)([a-zA-Z_][a-zA-Z0-9_]*)\\s*=\\s*(?!=)/g,\n      java: /(private|protected|public|static|\\s) +[\\w\\<\\>\\[\\]]+\\s+([a-zA-Z_][a-zA-Z0-9_]*)\\s*=\\s*[^;]+;/g,\n      go: /var\\s+([a-zA-Z_][a-zA-Z0-9_]*)\\s+[\\w\\[\\]]+(\\s*=\\s*[^;]+)?/g,\n      php: /(\\$[a-zA-Z_][a-zA-Z0-9_]*)\\s*=\\s*(?!=)/g,\n      default: /(const|let|var)\\s+([a-zA-Z_][a-zA-Z0-9_]*)\\s*=\\s*[^;]+;/g,\n    },\n  };\n\n  // Extract functions\n  const functionPattern =\n    patterns.function[language] || patterns.function.default;\n  let match;\n  while ((match = functionPattern.exec(content)) !== null) {\n    const name = match[1] || match[2]; // Some patterns capture name in different groups\n    const startPosition = match.index;\n\n    // Find the end of the function\n    // This is a simplification - would need language-specific logic for accurate ending\n    const startLine = getLineFromPosition(content, startPosition);\n    let endLine = startLine + 10; // Assume small functions for simplicity\n\n    entities.push({\n      type: \"function\",\n      name,\n      start_position: startPosition,\n      start_line: startLine,\n      end_line: endLine, // Approximation\n      raw_content: content.substring(\n        startPosition,\n        startPosition + match[0].length + 100\n      ), // Approximate content\n    });\n  }\n\n  // Extract classes with similar approach\n  const classPattern = patterns.class[language] || patterns.class.default;\n  while ((match = classPattern.exec(content)) !== null) {\n    const name = match[1];\n    const startPosition = match.index;\n    const startLine = getLineFromPosition(content, startPosition);\n    let endLine = startLine + 20; // Assume larger for classes\n\n    entities.push({\n      type: \"class\",\n      name,\n      start_position: startPosition,\n      start_line: startLine,\n      end_line: endLine, // Approximation\n      raw_content: content.substring(\n        startPosition,\n        startPosition + match[0].length + 500\n      ), // Approximate content\n    });\n  }\n\n  // Could continue with variables, methods, etc.\n\n  return entities;\n}\n\n/**\n * Extract code entities from JavaScript/TypeScript AST\n *\n * @param {Object} ast - Abstract Syntax Tree\n * @param {string} content - File content\n * @returns {Object} Extracted entities and relationships\n */\nfunction extractEntitiesFromAST(ast, content) {\n  const entities = [];\n  const relationships = [];\n  const idMap = new Map(); // Maps node to entity for relationship tracking\n\n  // Track visited nodes to prevent infinite recursion\n  const visitedNodes = new WeakSet();\n\n  /**\n   * Create a new entity object\n   *\n   * @param {string} type - Entity type\n   * @param {string} name - Entity name\n   * @param {number} startPosition - Start position in source\n   * @param {number} endPosition - End position in source\n   * @param {number} startLine - Start line number\n   * @param {number} endLine - End line number\n   * @param {string} rawContent - Raw content of the entity\n   * @param {Object|null} parentEntity - Parent entity if exists\n   * @param {Object} customMetadata - Additional metadata\n   * @returns {Object} The created entity\n   */\n  function createEntity(\n    type,\n    name,\n    startPosition,\n    endPosition,\n    startLine,\n    endLine,\n    rawContent,\n    parentEntity = null,\n    customMetadata = {}\n  ) {\n    const entity = {\n      type,\n      name,\n      start_position: startPosition,\n      end_position: endPosition,\n      start_line: startLine,\n      end_line: endLine,\n      raw_content: rawContent,\n      custom_metadata: customMetadata,\n    };\n\n    entities.push(entity);\n\n    // Set up parent-child relationship\n    if (parentEntity) {\n      relationships.push({\n        source: parentEntity,\n        target: entity,\n        type: \"contains\",\n      });\n    }\n\n    return entity;\n  }\n\n  /**\n   * Process a node to extract relevant entity information\n   */\n  function visit(node, parentNode = null, parentEntity = null, scope = null) {\n    if (!node || typeof node !== \"object\" || visitedNodes.has(node)) {\n      return;\n    }\n\n    visitedNodes.add(node);\n\n    // Skip if node doesn't have location data\n    if (!node.loc) {\n      return;\n    }\n\n    // Get line information\n    const startLine = node.loc?.start?.line;\n    const endLine = node.loc?.end?.line;\n    const startPosition = node.start;\n    const endPosition = node.end;\n    const rawContent = content.substring(startPosition, endPosition);\n\n    // Create currentEntity to track the entity for this node\n    let currentEntity = null;\n\n    // Extract entities based on node type\n    switch (node.type) {\n      case \"FunctionDeclaration\": {\n        const name = node.id?.name || \"anonymous\";\n        const params = node.params?.map((p) =>\n          p.type === \"Identifier\" ? p.name : \"param\"\n        );\n\n        currentEntity = createEntity(\n          \"function\",\n          name,\n          startPosition,\n          endPosition,\n          startLine,\n          endLine,\n          rawContent,\n          parentEntity,\n          {\n            ast_node_type: node.type,\n            params: params || [],\n            is_async: node.async || false,\n            is_generator: node.generator || false,\n          }\n        );\n\n        // Store in idMap for relationship tracking\n        idMap.set(node, currentEntity);\n\n        // Visit function body with this entity as parent\n        if (node.body) {\n          visit(node.body, node, currentEntity, name);\n        }\n        break;\n      }\n\n      case \"FunctionExpression\":\n      case \"ArrowFunctionExpression\": {\n        // Try to infer name from parent if this is a variable declaration\n        let name = \"anonymous\";\n        let functionType = \"function_expression\";\n\n        if (\n          parentNode &&\n          parentNode.type === \"VariableDeclarator\" &&\n          parentNode.id\n        ) {\n          name = parentNode.id.name;\n          functionType = \"function\";\n        } else if (\n          parentNode &&\n          parentNode.type === \"AssignmentExpression\" &&\n          parentNode.left\n        ) {\n          if (parentNode.left.type === \"Identifier\") {\n            name = parentNode.left.name;\n            functionType = \"function\";\n          } else if (\n            parentNode.left.type === \"MemberExpression\" &&\n            parentNode.left.property\n          ) {\n            name = parentNode.left.property.name;\n            functionType = \"method\";\n          }\n        } else if (\n          parentNode &&\n          parentNode.type === \"Property\" &&\n          parentNode.key\n        ) {\n          name = parentNode.key.name || parentNode.key.value || \"anonymous\";\n          functionType = \"method\";\n        } else if (\n          parentNode &&\n          parentNode.type === \"MethodDefinition\" &&\n          parentNode.key\n        ) {\n          name = parentNode.key.name || \"anonymous\";\n          functionType = \"method\";\n        }\n\n        const params = node.params?.map((p) =>\n          p.type === \"Identifier\" ? p.name : \"param\"\n        );\n\n        currentEntity = createEntity(\n          functionType,\n          name,\n          startPosition,\n          endPosition,\n          startLine,\n          endLine,\n          rawContent,\n          parentEntity,\n          {\n            ast_node_type: node.type,\n            params: params || [],\n            is_async: node.async || false,\n            is_generator: node.generator || false,\n            is_arrow: node.type === \"ArrowFunctionExpression\",\n          }\n        );\n\n        idMap.set(node, currentEntity);\n\n        // Visit function body with this entity as parent\n        if (node.body) {\n          visit(node.body, node, currentEntity, name);\n        }\n        break;\n      }\n\n      case \"ClassDeclaration\": {\n        const name = node.id?.name || \"anonymous\";\n\n        currentEntity = createEntity(\n          \"class\",\n          name,\n          startPosition,\n          endPosition,\n          startLine,\n          endLine,\n          rawContent,\n          parentEntity,\n          {\n            ast_node_type: node.type,\n          }\n        );\n\n        idMap.set(node, currentEntity);\n\n        // If this class extends another, record the relationship\n        if (node.superClass) {\n          if (node.superClass.type === \"Identifier\") {\n            relationships.push({\n              source: currentEntity,\n              target: { name: node.superClass.name, type: \"class\" },\n              type: \"extends\",\n            });\n          }\n        }\n\n        // Visit class body with this entity as parent\n        if (node.body) {\n          visit(node.body, node, currentEntity, name);\n        }\n        break;\n      }\n\n      case \"ClassExpression\": {\n        // Try to infer name from parent if possible\n        let name = node.id?.name || \"anonymous\";\n        if (\n          parentNode &&\n          parentNode.type === \"VariableDeclarator\" &&\n          parentNode.id\n        ) {\n          name = parentNode.id.name;\n        }\n\n        currentEntity = createEntity(\n          \"class\",\n          name,\n          startPosition,\n          endPosition,\n          startLine,\n          endLine,\n          rawContent,\n          parentEntity,\n          {\n            ast_node_type: node.type,\n          }\n        );\n\n        idMap.set(node, currentEntity);\n\n        // If this class extends another, record the relationship\n        if (node.superClass) {\n          if (node.superClass.type === \"Identifier\") {\n            relationships.push({\n              source: currentEntity,\n              target: { name: node.superClass.name, type: \"class\" },\n              type: \"extends\",\n            });\n          }\n        }\n\n        // Visit class body with this entity as parent\n        if (node.body) {\n          visit(node.body, node, currentEntity, name);\n        }\n        break;\n      }\n\n      case \"MethodDefinition\": {\n        const name = node.key?.name || node.key?.value || \"anonymous\";\n        const kind = node.kind || \"method\"; // \"method\", \"constructor\", \"get\", \"set\"\n\n        currentEntity = createEntity(\n          kind === \"constructor\" ? \"constructor\" : \"method\",\n          name,\n          startPosition,\n          endPosition,\n          startLine,\n          endLine,\n          rawContent,\n          parentEntity,\n          {\n            ast_node_type: node.type,\n            kind: kind,\n            is_static: !!node.static,\n            is_async: node.value?.async || false,\n            is_generator: node.value?.generator || false,\n          }\n        );\n\n        idMap.set(node, currentEntity);\n\n        // Visit method value/body with this entity as parent\n        if (node.value) {\n          visit(node.value, node, currentEntity, name);\n        }\n        break;\n      }\n\n      case \"VariableDeclaration\": {\n        // Don't create an entity for the declaration itself, just visit the declarators\n        node.declarations.forEach((declarator) => {\n          visit(declarator, node, parentEntity, scope);\n        });\n        break;\n      }\n\n      case \"VariableDeclarator\": {\n        if (node.id && node.id.type === \"Identifier\") {\n          const name = node.id.name;\n\n          // Don't create entities for simple variable assignments to primitives\n          // unless they have a function or object expression as initializer\n          let shouldCreateEntity = false;\n          let entityType = \"variable\";\n\n          if (!node.init) {\n            shouldCreateEntity = true; // Declarations without initializers\n          } else if (\n            [\n              \"FunctionExpression\",\n              \"ArrowFunctionExpression\",\n              \"ClassExpression\",\n              \"ObjectExpression\",\n              \"NewExpression\",\n            ].includes(node.init.type)\n          ) {\n            shouldCreateEntity = true;\n            if (node.init.type === \"ObjectExpression\") {\n              entityType = \"object\";\n            }\n          } else if (\n            node.init.type === \"Literal\" &&\n            typeof node.init.value === \"object\"\n          ) {\n            shouldCreateEntity = true;\n            entityType = \"object\";\n          } else if (parentEntity && parentEntity.type !== \"variable\") {\n            // Always create if inside a function/class\n            shouldCreateEntity = true;\n          }\n\n          if (shouldCreateEntity) {\n            currentEntity = createEntity(\n              entityType,\n              name,\n              startPosition,\n              endPosition,\n              startLine,\n              endLine,\n              rawContent,\n              parentEntity,\n              {\n                ast_node_type: node.type,\n                variable_kind: parentNode?.kind || \"var\", // 'var', 'let', or 'const'\n              }\n            );\n\n            idMap.set(node, currentEntity);\n          }\n        }\n\n        // Visit initializer\n        if (node.init) {\n          visit(node.init, node, parentEntity || currentEntity, scope);\n        }\n        break;\n      }\n\n      case \"ImportDeclaration\": {\n        // Create an entity for the import statement\n        const source = node.source.value;\n        const specifiers = node.specifiers.map((specifier) => {\n          if (specifier.type === \"ImportDefaultSpecifier\") {\n            return { type: \"default\", name: specifier.local.name };\n          } else if (specifier.type === \"ImportNamespaceSpecifier\") {\n            return { type: \"namespace\", name: specifier.local.name };\n          } else {\n            return {\n              type: \"named\",\n              name: specifier.local.name,\n              imported: specifier.imported?.name || specifier.local.name,\n            };\n          }\n        });\n\n        currentEntity = createEntity(\n          \"import\",\n          source,\n          startPosition,\n          endPosition,\n          startLine,\n          endLine,\n          rawContent,\n          parentEntity,\n          {\n            ast_node_type: node.type,\n            specifiers: specifiers,\n          }\n        );\n\n        idMap.set(node, currentEntity);\n\n        // Add relationships for the imports\n        specifiers.forEach((spec) => {\n          relationships.push({\n            source: currentEntity,\n            target: { name: spec.name, type: \"imported\" },\n            type: \"imports\",\n            metadata: {\n              source_module: source,\n              import_type: spec.type,\n              original_name: spec.imported,\n            },\n          });\n        });\n\n        break;\n      }\n\n      case \"ExportNamedDeclaration\": {\n        // Create an entity for the export statement\n        let name = \"named_export\";\n        if (node.declaration) {\n          if (\n            node.declaration.type === \"FunctionDeclaration\" ||\n            node.declaration.type === \"ClassDeclaration\"\n          ) {\n            name = node.declaration.id?.name || \"anonymous\";\n          } else if (\n            node.declaration.type === \"VariableDeclaration\" &&\n            node.declaration.declarations.length > 0\n          ) {\n            name = node.declaration.declarations[0].id?.name || \"anonymous\";\n          }\n        } else if (node.specifiers && node.specifiers.length > 0) {\n          name = node.specifiers\n            .map((s) => s.exported?.name || s.local?.name || \"anonymous\")\n            .join(\",\");\n        }\n\n        currentEntity = createEntity(\n          \"export\",\n          name,\n          startPosition,\n          endPosition,\n          startLine,\n          endLine,\n          rawContent,\n          parentEntity,\n          {\n            ast_node_type: node.type,\n            source: node.source?.value,\n          }\n        );\n\n        idMap.set(node, currentEntity);\n\n        // Visit the declaration\n        if (node.declaration) {\n          visit(node.declaration, node, parentEntity, scope);\n        }\n\n        // Add relationships for the exports\n        if (node.specifiers) {\n          node.specifiers.forEach((spec) => {\n            if (spec.local && spec.exported) {\n              relationships.push({\n                source: currentEntity,\n                target: { name: spec.local.name, type: \"exported\" },\n                type: \"exports\",\n                metadata: {\n                  exported_as: spec.exported.name,\n                  source_module: node.source?.value,\n                },\n              });\n            }\n          });\n        }\n\n        break;\n      }\n\n      case \"ExportDefaultDeclaration\": {\n        // Get name from declaration if possible\n        let name = \"default\";\n        if (node.declaration) {\n          if (\n            node.declaration.type === \"FunctionDeclaration\" ||\n            node.declaration.type === \"ClassDeclaration\"\n          ) {\n            name = node.declaration.id?.name || \"default\";\n          } else if (node.declaration.type === \"Identifier\") {\n            name = node.declaration.name;\n          }\n        }\n\n        currentEntity = createEntity(\n          \"export\",\n          name,\n          startPosition,\n          endPosition,\n          startLine,\n          endLine,\n          rawContent,\n          parentEntity,\n          {\n            ast_node_type: node.type,\n            is_default: true,\n          }\n        );\n\n        idMap.set(node, currentEntity);\n\n        // Visit the declaration\n        if (node.declaration) {\n          visit(node.declaration, node, parentEntity, scope);\n        }\n\n        // Add relationship\n        relationships.push({\n          source: currentEntity,\n          target: { name: name, type: \"exported\" },\n          type: \"exports\",\n          metadata: { is_default: true },\n        });\n\n        break;\n      }\n\n      case \"InterfaceDeclaration\": {\n        // TypeScript interface\n        const name = node.id?.name || \"anonymous\";\n\n        currentEntity = createEntity(\n          \"interface\",\n          name,\n          startPosition,\n          endPosition,\n          startLine,\n          endLine,\n          rawContent,\n          parentEntity,\n          {\n            ast_node_type: node.type,\n          }\n        );\n\n        idMap.set(node, currentEntity);\n\n        // Add extends relationships\n        if (node.extends) {\n          node.extends.forEach((ext) => {\n            if (ext.expression && ext.expression.type === \"Identifier\") {\n              relationships.push({\n                source: currentEntity,\n                target: { name: ext.expression.name, type: \"interface\" },\n                type: \"extends\",\n              });\n            }\n          });\n        }\n\n        // Visit the interface body\n        if (node.body) {\n          visit(node.body, node, currentEntity, name);\n        }\n\n        break;\n      }\n\n      case \"TypeAliasDeclaration\": {\n        // TypeScript type alias\n        const name = node.id?.name || \"anonymous\";\n\n        currentEntity = createEntity(\n          \"type_alias\",\n          name,\n          startPosition,\n          endPosition,\n          startLine,\n          endLine,\n          rawContent,\n          parentEntity,\n          {\n            ast_node_type: node.type,\n          }\n        );\n\n        idMap.set(node, currentEntity);\n\n        // Visit the type annotation\n        if (node.typeAnnotation) {\n          visit(node.typeAnnotation, node, currentEntity, name);\n        }\n\n        break;\n      }\n\n      case \"EnumDeclaration\": {\n        // TypeScript enum\n        const name = node.id?.name || \"anonymous\";\n\n        currentEntity = createEntity(\n          \"enum\",\n          name,\n          startPosition,\n          endPosition,\n          startLine,\n          endLine,\n          rawContent,\n          parentEntity,\n          {\n            ast_node_type: node.type,\n          }\n        );\n\n        idMap.set(node, currentEntity);\n\n        // Visit the enum members\n        if (node.members) {\n          node.members.forEach((member) => {\n            visit(member, node, currentEntity, name);\n          });\n        }\n\n        break;\n      }\n\n      case \"CallExpression\": {\n        // Record function call relationships\n        if (parentEntity) {\n          if (node.callee.type === \"Identifier\") {\n            relationships.push({\n              source: parentEntity,\n              target: { name: node.callee.name, type: \"function\" },\n              type: \"calls\",\n            });\n          } else if (node.callee.type === \"MemberExpression\") {\n            if (\n              node.callee.property &&\n              node.callee.property.type === \"Identifier\"\n            ) {\n              relationships.push({\n                source: parentEntity,\n                target: { name: node.callee.property.name, type: \"method\" },\n                type: \"calls\",\n                metadata: {\n                  object:\n                    node.callee.object.type === \"Identifier\"\n                      ? node.callee.object.name\n                      : null,\n                },\n              });\n            }\n          }\n        }\n\n        // Visit callee and arguments\n        if (node.callee) {\n          visit(node.callee, node, parentEntity, scope);\n        }\n\n        if (node.arguments) {\n          node.arguments.forEach((arg) => {\n            visit(arg, node, parentEntity, scope);\n          });\n        }\n\n        break;\n      }\n\n      // For nodes without explicit handlers, recursively visit child properties\n      default: {\n        for (const key in node) {\n          const child = node[key];\n\n          // Skip non-AST properties\n          if (\n            key === \"type\" ||\n            key === \"loc\" ||\n            key === \"range\" ||\n            key === \"parent\"\n          ) {\n            continue;\n          }\n\n          if (Array.isArray(child)) {\n            // For arrays (like body), visit each element\n            for (const item of child) {\n              visit(item, node, parentEntity || currentEntity, scope);\n            }\n          } else if (child && typeof child === \"object\") {\n            // Visit child node\n            visit(child, node, parentEntity || currentEntity, scope);\n          }\n        }\n      }\n    }\n  }\n\n  // Start the traversal from the root node\n  visit(ast);\n\n  return { entities, relationships };\n}\n\n/**\n * Stores file and its code entities in the database\n *\n * @param {string} filePath - Path to the file\n * @param {string} fileContent - Content of the file\n * @param {string} languageHint - Programming language hint\n * @returns {Promise<void>}\n */\nexport async function indexCodeFile(filePath, fileContent, languageHint) {\n  try {\n    // Calculate content hash\n    const contentHash = calculateContentHash(fileContent);\n\n    // Extract filename\n    const filename = extractFilename(filePath);\n\n    // Detect or use provided language\n    const language = detectLanguage(filePath, languageHint);\n\n    // Check if file already exists and is unchanged\n    const existingFileQuery = `\n      SELECT entity_id, content_hash \n      FROM code_entities \n      WHERE file_path = ? AND entity_type = 'file'\n    `;\n\n    const existingFile = await executeQuery(existingFileQuery, [filePath]);\n\n    let fileEntityId;\n\n    if (existingFile && existingFile.length > 0) {\n      fileEntityId = existingFile[0].entity_id;\n\n      // If content hash matches, file is unchanged\n      if (existingFile[0].content_hash === contentHash) {\n        console.log(`File ${filePath} is unchanged, skipping indexing`);\n        return;\n      }\n\n      // Update existing file entity\n      await executeQuery(\n        `\n        UPDATE code_entities\n        SET raw_content = ?, content_hash = ?, language = ?, last_modified_at = CURRENT_TIMESTAMP\n        WHERE entity_id = ?\n      `,\n        [fileContent, contentHash, language, fileEntityId]\n      );\n\n      // Delete existing sub-entities for re-indexing\n      await executeQuery(\n        `\n        DELETE FROM code_entities\n        WHERE parent_entity_id = ?\n      `,\n        [fileEntityId]\n      );\n\n      // Delete keywords for the file\n      await executeQuery(\n        `\n        DELETE FROM entity_keywords\n        WHERE entity_id = ?\n      `,\n        [fileEntityId]\n      );\n    } else {\n      // Create new file entity\n      fileEntityId = uuidv4();\n\n      await executeQuery(\n        `\n        INSERT INTO code_entities (\n          entity_id, file_path, entity_type, name, content_hash, raw_content, language, created_at, last_modified_at\n        ) VALUES (?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)\n      `,\n        [\n          fileEntityId,\n          filePath,\n          \"file\",\n          filename,\n          contentHash,\n          fileContent,\n          language,\n        ]\n      );\n    }\n\n    // Process file content based on language\n    let codeEntities = [];\n    let relationships = [];\n\n    // For JavaScript/TypeScript, use AST-based extraction\n    if (language === \"javascript\" || language === \"typescript\") {\n      const ast = await buildAST(fileContent, language);\n\n      if (ast && !ast.error) {\n        const extracted = extractEntitiesFromAST(ast, fileContent);\n        codeEntities = extracted.entities;\n        relationships = extracted.relationships;\n      } else {\n        console.error(\n          `Error building AST for ${filePath}:`,\n          ast?.error || \"Unknown error\"\n        );\n        // Fallback to regex-based extraction for JS/TS with parsing errors\n        codeEntities = extractEntitiesWithRegex(fileContent, language);\n      }\n    }\n    // For other languages, use regex-based extraction\n    else {\n      codeEntities = extractEntitiesWithRegex(fileContent, language);\n    }\n\n    // Store each code entity\n    for (const entity of codeEntities) {\n      const entityId = uuidv4();\n\n      // Convert custom_metadata to JSON string if it exists\n      const customMetadataJson = entity.custom_metadata\n        ? JSON.stringify(entity.custom_metadata)\n        : null;\n\n      // Insert entity into database\n      await executeQuery(\n        `\n        INSERT INTO code_entities (\n          entity_id, parent_entity_id, file_path, entity_type, name, \n          start_line, end_line, raw_content, language, custom_metadata,\n          created_at, last_modified_at\n        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)\n      `,\n        [\n          entityId,\n          fileEntityId, // All sub-entities have the file as parent by default\n          filePath,\n          entity.type,\n          entity.name,\n          entity.start_line,\n          entity.end_line,\n          entity.raw_content,\n          language,\n          customMetadataJson,\n        ]\n      );\n\n      // Process content for keywords\n      const tokens = tokenize(entity.raw_content);\n      const keywords = extractKeywords(tokens, 20, language);\n\n      // Store keywords\n      for (const keyword of keywords) {\n        await executeQuery(\n          `\n          INSERT INTO entity_keywords (\n            entity_id, keyword, term_frequency, weight, keyword_type\n          ) VALUES (?, ?, ?, ?, ?)\n        `,\n          [\n            entityId,\n            keyword.keyword,\n            keyword.score || 1.0,\n            keyword.score || 1.0,\n            \"term\",\n          ]\n        );\n      }\n\n      // Store entity reference in memory for relationship processing\n      entity.db_entity_id = entityId;\n    }\n\n    // Store parent-child relationships (e.g., method inside class, function inside function)\n    // This second pass is needed because all entities now have their db_entity_id\n    for (const rel of relationships) {\n      // Skip incomplete relationships\n      if (!rel.source || !rel.target) continue;\n\n      const sourceId = rel.source.db_entity_id;\n      const targetId = rel.target.db_entity_id;\n\n      // If relationship is \"contains\", update the parent_entity_id in code_entities\n      if (rel.type === \"contains\" && sourceId && targetId) {\n        await executeQuery(\n          `\n          UPDATE code_entities\n          SET parent_entity_id = ?\n          WHERE entity_id = ?\n          `,\n          [sourceId, targetId]\n        );\n      }\n      // For other relationships (calls, extends, imports, etc.), use the relationship table\n      else if (sourceId && targetId) {\n        await addRelationship(\n          sourceId,\n          targetId,\n          rel.type,\n          1.0,\n          rel.metadata || {}\n        );\n      }\n      // For target entities that might be in other files (calls, extends)\n      else if (sourceId && !targetId && rel.target.name) {\n        // Try to find the target entity by name and type\n        const targetQuery = `\n          SELECT entity_id \n          FROM code_entities \n          WHERE name = ? AND entity_type = ?\n        `;\n\n        const targetEntity = await executeQuery(targetQuery, [\n          rel.target.name,\n          rel.target.type,\n        ]);\n\n        if (targetEntity && targetEntity.length > 0) {\n          await addRelationship(\n            sourceId,\n            targetEntity[0].entity_id,\n            rel.type,\n            1.0,\n            rel.metadata || {}\n          );\n        }\n      }\n    }\n\n    console.log(`Successfully indexed file ${filePath}`);\n  } catch (error) {\n    console.error(`Error indexing file ${filePath}:`, error);\n    throw error;\n  }\n}\n\n/**\n * Message object type definition\n * @typedef {Object} MessageObject\n * @property {string} messageId - Unique identifier for the message\n * @property {string} conversationId - ID of the conversation this message belongs to\n * @property {string} role - Role of the message sender (e.g., 'user', 'assistant')\n * @property {string} content - Content of the message\n * @property {Date} timestamp - When the message was sent\n * @property {string[]} [relatedContextEntityIds] - IDs of related code entities\n * @property {string} [summary] - Summary of the message content\n * @property {string} [userIntent] - Inferred user intent\n * @property {string} [topicSegmentId] - ID of topic segment this message belongs to\n * @property {string[]} [semanticMarkers] - Semantic markers for enhanced retrieval\n * @property {Object} [sentimentIndicators] - Sentiment analysis results\n */\n\n/**\n * Indexes a conversation message for later retrieval\n *\n * @param {MessageObject} message - Message object to index\n * @returns {Promise<void>}\n */\nexport async function indexConversationMessage(message) {\n  try {\n    // Validate required message properties\n    if (\n      !message.message_id ||\n      !message.conversation_id ||\n      !message.role ||\n      !message.content\n    ) {\n      throw new Error(\"Message object missing required properties\");\n    }\n\n    console.log(\"===== INDEX MESSAGE - START =====\");\n    console.log(\"Input parameters:\");\n    console.log(\"- message_id:\", message.message_id);\n    console.log(\"- conversation_id:\", message.conversation_id);\n    console.log(\"- role:\", message.role);\n    console.log(\n      \"- content:\",\n      message.content &&\n        message.content.substring(0, 50) +\n          (message.content.length > 50 ? \"...\" : \"\")\n    );\n    console.log(\"- timestamp:\", message.timestamp);\n\n    // Convert arrays and objects to JSON strings for storage\n    const relatedContextEntityIds = message.relatedContextEntityIds\n      ? message.relatedContextEntityIds\n      : null;\n\n    const semanticMarkers = message.semantic_markers\n      ? message.semantic_markers\n      : null;\n\n    const sentimentIndicators = message.sentiment_indicators\n      ? message.sentiment_indicators\n      : null;\n\n    // Format timestamp\n    const timestamp =\n      message.timestamp instanceof Date\n        ? message.timestamp.toISOString()\n        : message.timestamp || new Date().toISOString();\n\n    // Check if message already exists\n    const existingMessageQuery = `\n      SELECT message_id FROM conversation_history \n      WHERE message_id = ?\n    `;\n\n    console.log(\"Checking if message exists:\", message.message_id);\n    const existingMessage = await executeQuery(existingMessageQuery, [\n      message.message_id,\n    ]);\n\n    console.log(\n      \"Existing message check result:\",\n      JSON.stringify(existingMessage)\n    );\n\n    if (\n      existingMessage &&\n      existingMessage.rows &&\n      existingMessage.rows.length > 0\n    ) {\n      console.log(\"Updating existing message:\", message.message_id);\n      // Update existing message\n      try {\n        const updateQuery = `UPDATE conversation_history \n         SET content = ?, \n             summary = ?, \n             user_intent = ?, \n             topic_segment_id = ?, \n             related_context_entity_ids = ?, \n             semantic_markers = ?, \n             sentiment_indicators = ?\n         WHERE message_id = ?`;\n\n        const updateParams = [\n          message.content,\n          message.summary || null,\n          message.userIntent || null,\n          message.topicSegmentId || null,\n          relatedContextEntityIds,\n          semanticMarkers,\n          sentimentIndicators,\n          message.message_id,\n        ];\n\n        console.log(\"Update query parameters:\", {\n          message_id: message.message_id,\n          content_length: message.content ? message.content.length : 0,\n        });\n\n        const updateResult = await executeQuery(updateQuery, updateParams);\n        console.log(\"Message update result:\", JSON.stringify(updateResult));\n      } catch (updateError) {\n        console.error(\"Update error:\", updateError);\n        throw updateError;\n      }\n    } else {\n      console.log(\"Inserting new message:\", message.message_id);\n      // Insert new message\n      try {\n        const insertQuery = `INSERT INTO conversation_history (\n          message_id, \n          conversation_id, \n          role, \n          content, \n          timestamp, \n          summary, \n          user_intent, \n          topic_segment_id, \n          related_context_entity_ids, \n          semantic_markers, \n          sentiment_indicators\n        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)`;\n\n        const insertParams = [\n          message.message_id,\n          message.conversation_id,\n          message.role,\n          message.content,\n          timestamp,\n          message.summary || null,\n          message.userIntent || null,\n          message.topicSegmentId || null,\n          relatedContextEntityIds,\n          semanticMarkers,\n          sentimentIndicators,\n        ];\n\n        console.log(\"Insert query parameters:\", {\n          message_id: message.message_id,\n          conversation_id: message.conversation_id,\n          role: message.role,\n          timestamp: timestamp,\n        });\n\n        const insertResult = await executeQuery(insertQuery, insertParams);\n        console.log(\"Message insert result:\", JSON.stringify(insertResult));\n      } catch (insertError) {\n        console.error(\"Insert error:\", insertError);\n        console.error(\"Error stack:\", insertError.stack);\n        throw insertError;\n      }\n    }\n\n    // Process message content for keywords\n    const tokens = tokenize(message.content);\n    const keywords = extractKeywords(tokens);\n\n    console.log(\"===== INDEX MESSAGE - COMPLETE =====\");\n    console.log(\"Successfully indexed message:\", message.message_id);\n\n    return {\n      messageId: message.message_id,\n      keywords: keywords,\n    };\n  } catch (error) {\n    console.error(\"===== INDEX MESSAGE - ERROR =====\");\n    console.error(`Error indexing message ${message?.message_id}:`, error);\n    console.error(\"Error stack:\", error.stack);\n    throw error;\n  }\n}\n", "/**\n * CodeStructureAnalyzerLogic.js\n *\n * Provides code structure analysis capabilities by parsing code into an AST\n * (Abstract Syntax Tree) for further analysis and understanding.\n */\n\nimport * as acorn from \"acorn\";\nimport { executeQuery } from \"../db.js\";\n\n/**\n * Builds an abstract syntax tree (AST) from code content\n *\n * @param {string} content - The code content to parse\n * @param {string} language - The programming language of the code\n * @returns {Promise<Object|null>} The AST node (for JS/TS) or null for unsupported languages\n */\nexport async function buildAST(content, language) {\n  // Handle empty content\n  if (!content || content.trim() === \"\") {\n    console.warn(\"Empty code content provided to buildAST\");\n    return null;\n  }\n\n  // Normalize language to lowercase\n  const normalizedLanguage = language.toLowerCase();\n\n  // Handle JavaScript and TypeScript parsing with acorn\n  if (\n    [\"javascript\", \"typescript\", \"js\", \"ts\", \"jsx\", \"tsx\"].includes(\n      normalizedLanguage\n    )\n  ) {\n    try {\n      // Configure acorn parser options\n      const options = {\n        ecmaVersion: \"latest\",\n        sourceType: \"module\",\n        locations: true,\n        ranges: true,\n        // Enable JSX parsing if the language includes 'jsx' or 'tsx'\n        allowAwaitOutsideFunction: true,\n        allowImportExportEverywhere: true,\n        allowReserved: true,\n        allowReturnOutsideFunction: false,\n        allowSuperOutsideMethod: false,\n      };\n\n      // Parse the content using acorn\n      const ast = acorn.parse(content, options);\n      return ast;\n    } catch (error) {\n      console.error(`Error parsing ${normalizedLanguage} code:`, error.message);\n      // Return a structured error object instead of null\n      return {\n        error: true,\n        message: error.message,\n        location: error.loc,\n        type: \"AST_PARSE_ERROR\",\n      };\n    }\n  } else {\n    // Log a message for unsupported languages\n    console.log(\n      `AST generation is not yet supported for ${normalizedLanguage}`\n    );\n    return null;\n  }\n}\n\n/**\n * Extracts structural features from an Abstract Syntax Tree\n *\n * @param {Object} ast - The AST node (output from buildAST)\n * @returns {Object} Object containing features array and complexity number\n */\nexport function extractStructuralFeatures(ast) {\n  // Handle null or invalid AST\n  if (!ast || ast.error) {\n    return { features: [], complexity: 0 };\n  }\n\n  // Initialize the result object\n  const result = {\n    features: [],\n    complexity: 1, // Base complexity is 1\n  };\n\n  // Track maximum nesting depth\n  let maxNestingDepth = 0;\n  let currentNestingDepth = 0;\n\n  // Track visited nodes to prevent infinite recursion in case of circular references\n  const visitedNodes = new WeakSet();\n\n  // Visit function to traverse the AST\n  function visit(node, parentNode = null, currentScope = \"global\") {\n    // Skip if node is null, undefined, or already visited\n    if (!node || visitedNodes.has(node)) {\n      return;\n    }\n\n    visitedNodes.add(node);\n\n    // Skip non-object nodes (primitive values)\n    if (typeof node !== \"object\") {\n      return;\n    }\n\n    // Get line number if available\n    const line = node.loc?.start?.line;\n\n    // Process node based on its type\n    switch (node.type) {\n      // Control flow statements\n      case \"IfStatement\":\n        result.features.push({\n          type: \"control_flow\",\n          statement: \"if\",\n          line,\n          nesting: currentNestingDepth,\n        });\n        result.complexity++; // Add to cyclomatic complexity\n        break;\n\n      case \"ForStatement\":\n      case \"ForInStatement\":\n      case \"ForOfStatement\":\n        result.features.push({\n          type: \"control_flow\",\n          statement: \"for\",\n          line,\n          nesting: currentNestingDepth,\n        });\n        result.complexity++; // Add to cyclomatic complexity\n        break;\n\n      case \"WhileStatement\":\n      case \"DoWhileStatement\":\n        result.features.push({\n          type: \"control_flow\",\n          statement: \"while\",\n          line,\n          nesting: currentNestingDepth,\n        });\n        result.complexity++; // Add to cyclomatic complexity\n        break;\n\n      case \"SwitchStatement\":\n        result.features.push({\n          type: \"control_flow\",\n          statement: \"switch\",\n          line,\n          nesting: currentNestingDepth,\n        });\n        // Each case adds to complexity\n        const caseCount = node.cases?.length || 0;\n        result.complexity += caseCount > 0 ? caseCount - 1 : 0;\n        break;\n\n      case \"TryStatement\":\n        result.features.push({\n          type: \"control_flow\",\n          statement: \"try\",\n          line,\n          nesting: currentNestingDepth,\n        });\n        // Catch and finally don't add to complexity, but we record them\n        break;\n\n      case \"ConditionalExpression\": // ternary operator: a ? b : c\n        result.features.push({\n          type: \"control_flow\",\n          statement: \"conditional\",\n          line,\n          nesting: currentNestingDepth,\n        });\n        result.complexity++; // Add to cyclomatic complexity\n        break;\n\n      case \"LogicalExpression\":\n        if (node.operator === \"&&\" || node.operator === \"||\") {\n          result.features.push({\n            type: \"control_flow\",\n            statement: \"logical\",\n            operator: node.operator,\n            line,\n            nesting: currentNestingDepth,\n          });\n          result.complexity++; // Logical expressions add to complexity\n        }\n        break;\n\n      // Function declarations and expressions\n      case \"FunctionDeclaration\":\n        result.features.push({\n          type: \"function_declaration\",\n          name: node.id?.name || \"anonymous\",\n          params: node.params?.length || 0,\n          line,\n          async: node.async || false,\n          generator: node.generator || false,\n        });\n        // Enter a new scope\n        currentScope = node.id?.name || \"anonymous\";\n        break;\n\n      case \"FunctionExpression\":\n      case \"ArrowFunctionExpression\":\n        result.features.push({\n          type: \"function_expression\",\n          name: node.id?.name || \"anonymous\",\n          params: node.params?.length || 0,\n          line,\n          async: node.async || false,\n          generator:\n            node.type === \"FunctionExpression\"\n              ? node.generator || false\n              : false,\n          arrow: node.type === \"ArrowFunctionExpression\",\n        });\n        // Enter an anonymous scope\n        currentScope = node.id?.name || \"anonymous\";\n        break;\n\n      // Function calls\n      case \"CallExpression\":\n        let callName = \"unknown\";\n\n        // Determine the function name being called\n        if (node.callee.type === \"Identifier\") {\n          callName = node.callee.name;\n        } else if (node.callee.type === \"MemberExpression\") {\n          // For expressions like object.method()\n          if (\n            node.callee.property &&\n            node.callee.property.type === \"Identifier\"\n          ) {\n            callName = node.callee.property.name;\n            // If we can determine the object name, include it\n            if (\n              node.callee.object &&\n              node.callee.object.type === \"Identifier\"\n            ) {\n              callName = `${node.callee.object.name}.${callName}`;\n            }\n          }\n        }\n\n        result.features.push({\n          type: \"function_call\",\n          name: callName,\n          arguments: node.arguments?.length || 0,\n          line,\n        });\n        break;\n\n      // Variable declarations\n      case \"VariableDeclaration\":\n        // Process each declarator separately\n        node.declarations.forEach((declarator) => {\n          if (declarator.id && declarator.id.type === \"Identifier\") {\n            result.features.push({\n              type: \"variable_declaration\",\n              name: declarator.id.name,\n              kind: node.kind, // 'var', 'let', or 'const'\n              scope: currentScope,\n              line,\n              initialized: declarator.init !== null,\n            });\n          }\n        });\n        break;\n\n      // Class declarations\n      case \"ClassDeclaration\":\n        result.features.push({\n          type: \"class_declaration\",\n          name: node.id?.name || \"anonymous\",\n          extends: node.superClass ? node.superClass.name || \"unknown\" : null,\n          line,\n        });\n        break;\n\n      // Import / Export statements\n      case \"ImportDeclaration\":\n        result.features.push({\n          type: \"import\",\n          source: node.source?.value,\n          line,\n        });\n        break;\n\n      case \"ExportNamedDeclaration\":\n      case \"ExportDefaultDeclaration\":\n        result.features.push({\n          type: \"export\",\n          default: node.type === \"ExportDefaultDeclaration\",\n          line,\n        });\n        break;\n    }\n\n    // Track nesting depth for block statements\n    if (node.type === \"BlockStatement\") {\n      currentNestingDepth++;\n      maxNestingDepth = Math.max(maxNestingDepth, currentNestingDepth);\n    }\n\n    // Recursively visit all child nodes\n    for (const key in node) {\n      const child = node[key];\n\n      // Skip special properties and non-AST properties\n      if (\n        key === \"type\" ||\n        key === \"loc\" ||\n        key === \"range\" ||\n        key === \"parent\" ||\n        key === \"leadingComments\" ||\n        key === \"trailingComments\"\n      ) {\n        continue;\n      }\n\n      if (Array.isArray(child)) {\n        // For arrays (like body), visit each element\n        for (const item of child) {\n          visit(item, node, currentScope);\n        }\n      } else if (child && typeof child === \"object\") {\n        // Visit child node\n        visit(child, node, currentScope);\n      }\n    }\n\n    // Decrement nesting depth when leaving a block\n    if (node.type === \"BlockStatement\") {\n      currentNestingDepth--;\n    }\n  }\n\n  // Start traversal from the root node\n  visit(ast);\n\n  // Add overall nesting depth as a feature\n  result.features.push({\n    type: \"metadata\",\n    name: \"max_nesting_depth\",\n    value: maxNestingDepth,\n  });\n\n  return result;\n}\n\n/**\n * Stores structural metadata for a code entity in the database\n *\n * @param {string} entityId - ID of the code entity\n * @param {Array} features - Array of structural features from extractStructuralFeatures\n * @returns {Promise<void>} Promise that resolves when the update is complete\n */\nexport async function storeStructuralMetadata(entityId, features) {\n  if (!entityId) {\n    throw new Error(\"Entity ID is required for storing structural metadata\");\n  }\n\n  if (!features || !Array.isArray(features)) {\n    throw new Error(\"Features must be a valid array\");\n  }\n\n  try {\n    // First, retrieve the current custom_metadata to merge with it\n    const currentMetadataQuery = `\n      SELECT custom_metadata \n      FROM code_entities \n      WHERE id = ?\n    `;\n\n    const result = await executeQuery(currentMetadataQuery, [entityId]);\n\n    // Parse existing metadata or initialize as empty object\n    let existingMetadata = {};\n\n    if (result && result.length > 0 && result[0].custom_metadata) {\n      try {\n        // Handle the case where custom_metadata might be stored as a string or object\n        const metadataValue = result[0].custom_metadata;\n        existingMetadata =\n          typeof metadataValue === \"string\"\n            ? JSON.parse(metadataValue)\n            : metadataValue;\n      } catch (parseError) {\n        console.warn(\n          `Error parsing existing metadata for entity ${entityId}:`,\n          parseError\n        );\n        // Continue with empty object if parsing fails\n      }\n    }\n\n    // Create the updated metadata by merging with existing\n    // Overwrite specifically the structural_features key\n    const updatedMetadata = {\n      ...existingMetadata,\n      structural_features: features,\n      // Add timestamp of when this analysis was performed\n      structural_analysis_timestamp: new Date().toISOString(),\n    };\n\n    // Prepare the UPDATE query\n    const updateQuery = `\n      UPDATE code_entities \n      SET \n        custom_metadata = ?,\n        updated_at = CURRENT_TIMESTAMP\n      WHERE id = ?\n    `;\n\n    // Execute the query with the serialized metadata\n    await executeQuery(updateQuery, [\n      JSON.stringify(updatedMetadata),\n      entityId,\n    ]);\n\n    console.log(`Structural metadata updated for entity ${entityId}`);\n  } catch (error) {\n    console.error(\n      `Error storing structural metadata for entity ${entityId}:`,\n      error\n    );\n    throw error; // Re-throw to allow caller to handle\n  }\n}\n\n/**\n * Compares two Abstract Syntax Trees to determine structural similarity\n *\n * @param {Object} ast1 - First AST node\n * @param {Object} ast2 - Second AST node\n * @returns {Object} Object containing similarity score and array of differences\n */\nexport function compareStructures(ast1, ast2) {\n  // Handle null or invalid ASTs\n  if (!ast1 || ast1.error || !ast2 || ast2.error) {\n    return {\n      similarity: 0,\n      differences: [\n        {\n          type: \"invalid_ast\",\n          description: \"One or both ASTs are null or invalid\",\n        },\n      ],\n    };\n  }\n\n  // Array to collect structural differences\n  const differences = [];\n\n  // Extract node type distributions from both ASTs\n  const dist1 = extractNodeTypeDistribution(ast1);\n  const dist2 = extractNodeTypeDistribution(ast2);\n\n  // Compare basic tree structure statistics\n  const basicStats1 = extractBasicStats(ast1);\n  const basicStats2 = extractBasicStats(ast2);\n\n  // Generate structural fingerprints for comparison\n  const fingerprint1 = generateStructuralFingerprint(ast1);\n  const fingerprint2 = generateStructuralFingerprint(ast2);\n\n  // Calculate similarity based on various metrics\n  const typeSimilarity = calculateTypeDistributionSimilarity(dist1, dist2);\n  const statsSimilarity = calculateStatsSimilarity(basicStats1, basicStats2);\n  const fingerprintSimilarity = calculateFingerprintSimilarity(\n    fingerprint1,\n    fingerprint2\n  );\n\n  // Detect key structural differences\n  detectStructuralDifferences(ast1, ast2, differences);\n\n  // Weight the different similarity components for a final score\n  // Fingerprint similarity should have the highest weight as it captures structure\n  const similarity =\n    typeSimilarity * 0.3 + statsSimilarity * 0.2 + fingerprintSimilarity * 0.5;\n\n  // Return bounded similarity score and differences\n  return {\n    similarity: Math.max(0, Math.min(1, similarity)), // Ensure between 0 and 1\n    differences,\n  };\n}\n\n/**\n * Extracts the distribution of node types in an AST\n *\n * @param {Object} ast - The AST to analyze\n * @returns {Object} Map of node types to their frequency\n * @private\n */\nfunction extractNodeTypeDistribution(ast) {\n  const distribution = {};\n  const visitedNodes = new WeakSet();\n\n  function visit(node) {\n    if (!node || typeof node !== \"object\" || visitedNodes.has(node)) {\n      return;\n    }\n\n    visitedNodes.add(node);\n\n    if (node.type) {\n      distribution[node.type] = (distribution[node.type] || 0) + 1;\n    }\n\n    // Recursively visit child nodes\n    for (const key in node) {\n      const child = node[key];\n\n      if (key === \"type\" || key === \"loc\" || key === \"range\") {\n        continue;\n      }\n\n      if (Array.isArray(child)) {\n        for (const item of child) {\n          visit(item);\n        }\n      } else if (child && typeof child === \"object\") {\n        visit(child);\n      }\n    }\n  }\n\n  visit(ast);\n  return distribution;\n}\n\n/**\n * Extracts basic structural statistics from an AST\n *\n * @param {Object} ast - The AST to analyze\n * @returns {Object} Basic structure statistics\n * @private\n */\nfunction extractBasicStats(ast) {\n  const stats = {\n    nodeCount: 0,\n    maxDepth: 0,\n    leafCount: 0,\n    blockCount: 0,\n    functionCount: 0,\n    expressionCount: 0,\n  };\n\n  const visitedNodes = new WeakSet();\n\n  function visit(node, depth = 0) {\n    if (!node || typeof node !== \"object\" || visitedNodes.has(node)) {\n      return;\n    }\n\n    visitedNodes.add(node);\n    stats.nodeCount++;\n    stats.maxDepth = Math.max(stats.maxDepth, depth);\n\n    let hasChildren = false;\n\n    // Count specific node types\n    if (node.type) {\n      if (node.type === \"BlockStatement\") {\n        stats.blockCount++;\n      } else if (\n        node.type === \"FunctionDeclaration\" ||\n        node.type === \"FunctionExpression\" ||\n        node.type === \"ArrowFunctionExpression\"\n      ) {\n        stats.functionCount++;\n      } else if (node.type.includes(\"Expression\")) {\n        stats.expressionCount++;\n      }\n    }\n\n    // Recursively visit child nodes\n    for (const key in node) {\n      const child = node[key];\n\n      if (key === \"type\" || key === \"loc\" || key === \"range\") {\n        continue;\n      }\n\n      if (Array.isArray(child)) {\n        if (child.length > 0) {\n          hasChildren = true;\n          for (const item of child) {\n            visit(item, depth + 1);\n          }\n        }\n      } else if (child && typeof child === \"object\") {\n        hasChildren = true;\n        visit(child, depth + 1);\n      }\n    }\n\n    if (!hasChildren) {\n      stats.leafCount++;\n    }\n  }\n\n  visit(ast);\n  return stats;\n}\n\n/**\n * Generates a structural fingerprint for an AST\n * This creates a simplified representation of the AST structure\n *\n * @param {Object} ast - The AST to fingerprint\n * @returns {Object} Structural fingerprint with depth-based node sequences\n * @private\n */\nfunction generateStructuralFingerprint(ast) {\n  const fingerprint = {\n    // Store sequences of node types at each depth\n    sequencesByDepth: {},\n    // Store parent-child type relationships\n    relationships: {},\n    // Top-level node structure\n    topLevel: [],\n  };\n\n  const visitedNodes = new WeakSet();\n\n  function visit(node, depth = 0, path = \"\") {\n    if (!node || typeof node !== \"object\" || visitedNodes.has(node)) {\n      return;\n    }\n\n    visitedNodes.add(node);\n\n    if (node.type) {\n      // Add to sequence at this depth\n      if (!fingerprint.sequencesByDepth[depth]) {\n        fingerprint.sequencesByDepth[depth] = [];\n      }\n      fingerprint.sequencesByDepth[depth].push(node.type);\n\n      // For root level nodes, capture more detail\n      if (depth === 1 && node.type) {\n        fingerprint.topLevel.push(node.type);\n      }\n    }\n\n    // Recursively visit child nodes\n    for (const key in node) {\n      const child = node[key];\n\n      if (key === \"type\" || key === \"loc\" || key === \"range\") {\n        continue;\n      }\n\n      if (Array.isArray(child)) {\n        for (let i = 0; i < child.length; i++) {\n          const item = child[i];\n          const newPath = `${path}.${key}[${i}]`;\n\n          if (item && item.type && node.type) {\n            const relationship = `${node.type}->${item.type}`;\n            fingerprint.relationships[relationship] =\n              (fingerprint.relationships[relationship] || 0) + 1;\n          }\n\n          visit(item, depth + 1, newPath);\n        }\n      } else if (child && typeof child === \"object\") {\n        const newPath = `${path}.${key}`;\n\n        if (child.type && node.type) {\n          const relationship = `${node.type}->${child.type}`;\n          fingerprint.relationships[relationship] =\n            (fingerprint.relationships[relationship] || 0) + 1;\n        }\n\n        visit(child, depth + 1, newPath);\n      }\n    }\n  }\n\n  visit(ast, 0, \"root\");\n  return fingerprint;\n}\n\n/**\n * Calculates similarity between two node type distributions\n *\n * @param {Object} dist1 - First node type distribution\n * @param {Object} dist2 - Second node type distribution\n * @returns {number} Similarity score between 0 and 1\n * @private\n */\nfunction calculateTypeDistributionSimilarity(dist1, dist2) {\n  // Get all unique node types\n  const allTypes = new Set([...Object.keys(dist1), ...Object.keys(dist2)]);\n\n  if (allTypes.size === 0) {\n    return 0;\n  }\n\n  // Calculate cosine similarity\n  let dotProduct = 0;\n  let magnitude1 = 0;\n  let magnitude2 = 0;\n\n  for (const type of allTypes) {\n    const count1 = dist1[type] || 0;\n    const count2 = dist2[type] || 0;\n\n    dotProduct += count1 * count2;\n    magnitude1 += count1 * count1;\n    magnitude2 += count2 * count2;\n  }\n\n  magnitude1 = Math.sqrt(magnitude1);\n  magnitude2 = Math.sqrt(magnitude2);\n\n  if (magnitude1 === 0 || magnitude2 === 0) {\n    return 0;\n  }\n\n  return dotProduct / (magnitude1 * magnitude2);\n}\n\n/**\n * Calculates similarity between two sets of basic structure statistics\n *\n * @param {Object} stats1 - First structure statistics\n * @param {Object} stats2 - Second structure statistics\n * @returns {number} Similarity score between 0 and 1\n * @private\n */\nfunction calculateStatsSimilarity(stats1, stats2) {\n  // Normalize and compare key statistics\n  const metrics = [\n    \"nodeCount\",\n    \"maxDepth\",\n    \"leafCount\",\n    \"blockCount\",\n    \"functionCount\",\n    \"expressionCount\",\n  ];\n\n  let totalSimilarity = 0;\n\n  for (const metric of metrics) {\n    // If both values are 0, consider them perfectly similar for this metric\n    if (stats1[metric] === 0 && stats2[metric] === 0) {\n      totalSimilarity += 1;\n      continue;\n    }\n\n    // Calculate ratio of smaller to larger value\n    const ratio =\n      Math.min(stats1[metric], stats2[metric]) /\n      Math.max(stats1[metric], stats2[metric]);\n\n    totalSimilarity += ratio;\n  }\n\n  return totalSimilarity / metrics.length;\n}\n\n/**\n * Calculates similarity between two structural fingerprints\n *\n * @param {Object} fp1 - First fingerprint\n * @param {Object} fp2 - Second fingerprint\n * @returns {number} Similarity score between 0 and 1\n * @private\n */\nfunction calculateFingerprintSimilarity(fp1, fp2) {\n  // Compare sequences at each depth (with higher weight for lower depths)\n  let sequenceSimilarity = 0;\n  let totalWeight = 0;\n\n  // Find the maximum depth across both fingerprints\n  const maxDepth = Math.max(\n    ...Object.keys(fp1.sequencesByDepth).map(Number),\n    ...Object.keys(fp2.sequencesByDepth).map(Number)\n  );\n\n  for (let depth = 0; depth <= maxDepth; depth++) {\n    const seq1 = fp1.sequencesByDepth[depth] || [];\n    const seq2 = fp2.sequencesByDepth[depth] || [];\n\n    // Skip if both sequences are empty\n    if (seq1.length === 0 && seq2.length === 0) {\n      continue;\n    }\n\n    // Calculate longest common subsequence length\n    const lcsLength = longestCommonSubsequenceLength(seq1, seq2);\n\n    // Calculate sequence similarity as ratio of LCS to max length\n    const maxSeqLength = Math.max(seq1.length, seq2.length);\n    const seqSimilarity = maxSeqLength > 0 ? lcsLength / maxSeqLength : 0;\n\n    // Weight decreases with depth (root nodes are more important)\n    const weight = 1 / (depth + 1);\n    sequenceSimilarity += seqSimilarity * weight;\n    totalWeight += weight;\n  }\n\n  const normalizedSequenceSimilarity =\n    totalWeight > 0 ? sequenceSimilarity / totalWeight : 0;\n\n  // Compare top-level structure (higher weight)\n  const topLevelSimilarity = compareArrays(fp1.topLevel, fp2.topLevel);\n\n  // Compare parent-child relationships\n  const relationshipSimilarity = compareRelationships(\n    fp1.relationships,\n    fp2.relationships\n  );\n\n  // Combine with weights\n  return (\n    normalizedSequenceSimilarity * 0.4 +\n    topLevelSimilarity * 0.4 +\n    relationshipSimilarity * 0.2\n  );\n}\n\n/**\n * Compares arrays by finding elements in common\n *\n * @param {Array} arr1 - First array\n * @param {Array} arr2 - Second array\n * @returns {number} Similarity score between 0 and 1\n * @private\n */\nfunction compareArrays(arr1, arr2) {\n  if (arr1.length === 0 && arr2.length === 0) {\n    return 1; // Both empty means they're identical\n  }\n\n  if (arr1.length === 0 || arr2.length === 0) {\n    return 0; // One empty means no similarity\n  }\n\n  // Count elements in common\n  const set1 = new Set(arr1);\n  const set2 = new Set(arr2);\n\n  let common = 0;\n  for (const item of set1) {\n    if (set2.has(item)) {\n      common++;\n    }\n  }\n\n  // Jaccard similarity: intersection size / union size\n  const union = set1.size + set2.size - common;\n  return union > 0 ? common / union : 0;\n}\n\n/**\n * Compares relationship maps between two fingerprints\n *\n * @param {Object} rel1 - First relationship map\n * @param {Object} rel2 - Second relationship map\n * @returns {number} Similarity score between 0 and 1\n * @private\n */\nfunction compareRelationships(rel1, rel2) {\n  const allRelationships = new Set([\n    ...Object.keys(rel1),\n    ...Object.keys(rel2),\n  ]);\n\n  if (allRelationships.size === 0) {\n    return 0;\n  }\n\n  let similarity = 0;\n\n  for (const rel of allRelationships) {\n    const count1 = rel1[rel] || 0;\n    const count2 = rel2[rel] || 0;\n\n    // Ratio of smaller to larger count\n    const ratio = Math.min(count1, count2) / Math.max(count1, count2);\n    similarity += ratio;\n  }\n\n  return similarity / allRelationships.size;\n}\n\n/**\n * Finds the length of the longest common subsequence of two arrays\n *\n * @param {Array} arr1 - First array\n * @param {Array} arr2 - Second array\n * @returns {number} Length of longest common subsequence\n * @private\n */\nfunction longestCommonSubsequenceLength(arr1, arr2) {\n  if (!arr1.length || !arr2.length) {\n    return 0;\n  }\n\n  // Initialize DP table\n  const dp = Array(arr1.length + 1)\n    .fill()\n    .map(() => Array(arr2.length + 1).fill(0));\n\n  // Fill the DP table\n  for (let i = 1; i <= arr1.length; i++) {\n    for (let j = 1; j <= arr2.length; j++) {\n      if (arr1[i - 1] === arr2[j - 1]) {\n        dp[i][j] = dp[i - 1][j - 1] + 1;\n      } else {\n        dp[i][j] = Math.max(dp[i - 1][j], dp[i][j - 1]);\n      }\n    }\n  }\n\n  return dp[arr1.length][arr2.length];\n}\n\n/**\n * Detects structural differences between two ASTs and adds them to the differences array\n *\n * @param {Object} ast1 - First AST\n * @param {Object} ast2 - Second AST\n * @param {Array} differences - Array to store detected differences\n * @private\n */\nfunction detectStructuralDifferences(ast1, ast2, differences) {\n  // Extract basic stats for comparison\n  const stats1 = extractBasicStats(ast1);\n  const stats2 = extractBasicStats(ast2);\n\n  // Compare node counts\n  const nodeDiff = Math.abs(stats1.nodeCount - stats2.nodeCount);\n  const nodeRatio =\n    stats1.nodeCount === 0 && stats2.nodeCount === 0\n      ? 1\n      : Math.min(stats1.nodeCount, stats2.nodeCount) /\n        Math.max(stats1.nodeCount, stats2.nodeCount);\n\n  if (nodeRatio < 0.7) {\n    differences.push({\n      type: \"node_count_mismatch\",\n      description: `Node count differs significantly: ${stats1.nodeCount} vs ${stats2.nodeCount}`,\n      severity: \"high\",\n    });\n  }\n\n  // Compare depth\n  if (Math.abs(stats1.maxDepth - stats2.maxDepth) > 2) {\n    differences.push({\n      type: \"depth_mismatch\",\n      description: `Tree depth differs: ${stats1.maxDepth} vs ${stats2.maxDepth}`,\n      severity: \"medium\",\n    });\n  }\n\n  // Compare function counts\n  if (stats1.functionCount !== stats2.functionCount) {\n    differences.push({\n      type: \"function_count_mismatch\",\n      description: `Function count differs: ${stats1.functionCount} vs ${stats2.functionCount}`,\n      severity: \"medium\",\n    });\n  }\n\n  // Compare root structure\n  const rootType1 = ast1.type;\n  const rootType2 = ast2.type;\n\n  if (rootType1 !== rootType2) {\n    differences.push({\n      type: \"root_type_mismatch\",\n      description: `Root node type differs: ${rootType1} vs ${rootType2}`,\n      severity: \"high\",\n    });\n  }\n\n  // Analyze top-level structure\n  const body1 = ast1.body || [];\n  const body2 = ast2.body || [];\n\n  if (body1.length !== body2.length) {\n    differences.push({\n      type: \"program_body_length_mismatch\",\n      description: `Program body length differs: ${body1.length} vs ${body2.length}`,\n      severity: \"medium\",\n    });\n  }\n\n  // Compare top-level statement types\n  const topLevelTypes1 = (body1 || []).map((node) => node.type);\n  const topLevelTypes2 = (body2 || []).map((node) => node.type);\n\n  // Find missing types in each direction\n  const missingInAst2 = topLevelTypes1.filter(\n    (type) => !topLevelTypes2.includes(type)\n  );\n  const missingInAst1 = topLevelTypes2.filter(\n    (type) => !topLevelTypes1.includes(type)\n  );\n\n  if (missingInAst2.length > 0) {\n    differences.push({\n      type: \"missing_node_types\",\n      description: `Node types in AST1 but missing in AST2: ${missingInAst2.join(\n        \", \"\n      )}`,\n      severity: \"medium\",\n    });\n  }\n\n  if (missingInAst1.length > 0) {\n    differences.push({\n      type: \"missing_node_types\",\n      description: `Node types in AST2 but missing in AST1: ${missingInAst1.join(\n        \", \"\n      )}`,\n      severity: \"medium\",\n    });\n  }\n}\n\n/**\n * Finds entities with similar structural features to the source entity\n *\n * @param {string} entityId - ID of the source entity\n * @param {number} threshold - Similarity threshold (0 to 1), defaults to 0.7\n * @returns {Promise<Array>} Array of similar entities with similarity scores\n */\nexport async function findStructurallySimilarEntities(\n  entityId,\n  threshold = 0.7\n) {\n  if (!entityId) {\n    throw new Error(\"Entity ID is required for finding similar entities\");\n  }\n\n  // Validate threshold\n  if (threshold < 0 || threshold > 1) {\n    throw new Error(\"Threshold must be between 0 and 1\");\n  }\n\n  try {\n    // 1. Retrieve the source entity's metadata including features\n    const sourceQuery = `\n      SELECT id, type, path, custom_metadata \n      FROM code_entities \n      WHERE id = ?\n    `;\n\n    const sourceResult = await executeQuery(sourceQuery, [entityId]);\n\n    if (!sourceResult || sourceResult.length === 0) {\n      throw new Error(`Source entity with ID ${entityId} not found`);\n    }\n\n    const sourceEntity = sourceResult[0];\n    let sourceFeatures = null;\n\n    // Try to get structural features from custom_metadata\n    if (sourceEntity.custom_metadata) {\n      try {\n        const metadata =\n          typeof sourceEntity.custom_metadata === \"string\"\n            ? JSON.parse(sourceEntity.custom_metadata)\n            : sourceEntity.custom_metadata;\n\n        if (metadata.structural_features) {\n          sourceFeatures = metadata.structural_features;\n        }\n      } catch (parseError) {\n        console.warn(\n          `Error parsing metadata for source entity ${entityId}:`,\n          parseError\n        );\n      }\n    }\n\n    if (\n      !sourceFeatures ||\n      !Array.isArray(sourceFeatures) ||\n      sourceFeatures.length === 0\n    ) {\n      console.warn(\n        `No structural features found for entity ${entityId}. Cannot compare similarity.`\n      );\n      return [];\n    }\n\n    // 2. Find candidate entities (same type as source for more relevant comparisons)\n    const candidatesQuery = `\n      SELECT id, type, path, custom_metadata \n      FROM code_entities \n      WHERE id != ? \n      AND type = ?\n    `;\n\n    const candidateEntities = await executeQuery(candidatesQuery, [\n      entityId,\n      sourceEntity.type,\n    ]);\n\n    if (!candidateEntities || candidateEntities.length === 0) {\n      console.log(\n        `No candidate entities found for comparison with ${entityId}`\n      );\n      return [];\n    }\n\n    // 3. Compare source features with each candidate's features\n    const similarEntities = [];\n\n    for (const candidate of candidateEntities) {\n      let candidateFeatures = null;\n\n      // Extract candidate features from metadata\n      if (candidate.custom_metadata) {\n        try {\n          const metadata =\n            typeof candidate.custom_metadata === \"string\"\n              ? JSON.parse(candidate.custom_metadata)\n              : candidate.custom_metadata;\n\n          if (metadata.structural_features) {\n            candidateFeatures = metadata.structural_features;\n          }\n        } catch (parseError) {\n          console.warn(\n            `Error parsing metadata for candidate entity ${candidate.id}:`,\n            parseError\n          );\n          continue; // Skip this candidate\n        }\n      }\n\n      // Skip if candidate has no features\n      if (\n        !candidateFeatures ||\n        !Array.isArray(candidateFeatures) ||\n        candidateFeatures.length === 0\n      ) {\n        continue;\n      }\n\n      // Calculate similarity score between source and candidate features\n      const similarityScore = calculateFeatureSimilarity(\n        sourceFeatures,\n        candidateFeatures\n      );\n\n      // Add to results if above threshold\n      if (similarityScore >= threshold) {\n        similarEntities.push({\n          entityId: candidate.id,\n          path: candidate.path,\n          type: candidate.type,\n          similarity: similarityScore,\n        });\n      }\n    }\n\n    // 4. Sort by similarity score (descending)\n    similarEntities.sort((a, b) => b.similarity - a.similarity);\n\n    return similarEntities;\n  } catch (error) {\n    console.error(\n      `Error finding structurally similar entities for ${entityId}:`,\n      error\n    );\n    throw error;\n  }\n}\n\n/**\n * Calculates similarity between two sets of structural features\n *\n * @param {Array} features1 - First set of features\n * @param {Array} features2 - Second set of features\n * @returns {number} Similarity score between 0 and 1\n * @private\n */\nfunction calculateFeatureSimilarity(features1, features2) {\n  if (!features1 || !features2 || !features1.length || !features2.length) {\n    return 0;\n  }\n\n  // Extract feature types for a simple Jaccard similarity\n  const types1 = new Set(\n    features1.map((f) => `${f.type}:${f.statement || f.name || \"\"}`)\n  );\n  const types2 = new Set(\n    features2.map((f) => `${f.type}:${f.statement || f.name || \"\"}`)\n  );\n\n  // Find complexity value if present\n  const getComplexity = (features) => {\n    const metadataFeature = features.find(\n      (f) => f.type === \"metadata\" && f.name === \"max_nesting_depth\"\n    );\n    return metadataFeature ? metadataFeature.value : 0;\n  };\n\n  const nestingDepth1 = getComplexity(features1);\n  const nestingDepth2 = getComplexity(features2);\n\n  // Calculate count of each feature type\n  const countByType1 = countFeaturesByType(features1);\n  const countByType2 = countFeaturesByType(features2);\n\n  // Calculate different similarity components\n\n  // 1. Jaccard similarity of feature types (40% weight)\n  const intersection = new Set([...types1].filter((x) => types2.has(x)));\n  const union = new Set([...types1, ...types2]);\n  const jaccardSimilarity = intersection.size / union.size;\n\n  // 2. Feature count similarity (30% weight)\n  const countSimilarity = calculateCountSimilarity(countByType1, countByType2);\n\n  // 3. Nesting depth similarity (30% weight)\n  const maxNesting = Math.max(nestingDepth1, nestingDepth2);\n  const nestingSimilarity =\n    maxNesting === 0\n      ? 1 // If both have zero nesting, they're similar\n      : 1 - Math.abs(nestingDepth1 - nestingDepth2) / maxNesting;\n\n  // Weighted combination of similarity measures\n  return (\n    jaccardSimilarity * 0.4 + countSimilarity * 0.3 + nestingSimilarity * 0.3\n  );\n}\n\n/**\n * Count features by type in a feature array\n *\n * @param {Array} features - Array of features\n * @returns {Object} Map of feature types to counts\n * @private\n */\nfunction countFeaturesByType(features) {\n  const counts = {};\n\n  for (const feature of features) {\n    const type = feature.type;\n    counts[type] = (counts[type] || 0) + 1;\n  }\n\n  return counts;\n}\n\n/**\n * Calculate similarity between feature type counts\n *\n * @param {Object} counts1 - Feature counts for first entity\n * @param {Object} counts2 - Feature counts for second entity\n * @returns {number} Similarity score between 0 and 1\n * @private\n */\nfunction calculateCountSimilarity(counts1, counts2) {\n  // Get all unique feature types\n  const allTypes = new Set([...Object.keys(counts1), ...Object.keys(counts2)]);\n\n  if (allTypes.size === 0) {\n    return 0;\n  }\n\n  let similarity = 0;\n\n  for (const type of allTypes) {\n    const count1 = counts1[type] || 0;\n    const count2 = counts2[type] || 0;\n\n    // Skip if both counts are 0\n    if (count1 === 0 && count2 === 0) {\n      continue;\n    }\n\n    // Calculate ratio (smaller/larger)\n    const ratio = Math.min(count1, count2) / Math.max(count1, count2);\n    similarity += ratio;\n  }\n\n  return similarity / allTypes.size;\n}\n\n/**\n * Generates a data flow graph from a function's Abstract Syntax Tree\n *\n * @param {Object} functionAst - AST node representing a function\n * @returns {Object} Data flow graph with nodes and edges\n */\nexport function getDataFlowGraph(functionAst) {\n  // Return empty graph for invalid input\n  if (!functionAst || functionAst.error) {\n    return { nodes: [], edges: [] };\n  }\n\n  // Verify that the AST node is a function\n  const isFunctionNode =\n    functionAst.type === \"FunctionDeclaration\" ||\n    functionAst.type === \"FunctionExpression\" ||\n    functionAst.type === \"ArrowFunctionExpression\";\n\n  if (!isFunctionNode) {\n    return { nodes: [], edges: [] };\n  }\n\n  // Initialize graph components\n  const nodes = [];\n  const edges = [];\n\n  // Track node IDs to avoid duplicates\n  const nodeIds = new Set();\n\n  // Track variable declarations and their scopes\n  const variables = new Map();\n\n  // Track current scope and parent scope chain\n  const scopeChain = [];\n  let currentScope = \"function\";\n\n  // Helper to add a node if it doesn't already exist\n  function addNode(id, type) {\n    if (!nodeIds.has(id)) {\n      nodes.push({ id, type });\n      nodeIds.add(id);\n    }\n  }\n\n  // Helper to add an edge between nodes\n  function addEdge(source, target, type) {\n    // Ensure both nodes exist first\n    if (nodeIds.has(source) && nodeIds.has(target)) {\n      edges.push({ source, target, type });\n    }\n  }\n\n  // Add function parameters as nodes\n  functionAst.params?.forEach((param) => {\n    if (param.type === \"Identifier\") {\n      const paramId = param.name;\n      addNode(paramId, \"parameter\");\n      variables.set(paramId, currentScope);\n    } else if (\n      param.type === \"AssignmentPattern\" &&\n      param.left.type === \"Identifier\"\n    ) {\n      // Handle default parameters: function(a = 1)\n      const paramId = param.left.name;\n      addNode(paramId, \"parameter\");\n      variables.set(paramId, currentScope);\n\n      // Add default value node and edge\n      if (param.right) {\n        const defaultValueId = `default_${paramId}`;\n        addNode(defaultValueId, \"literal\");\n        addEdge(defaultValueId, paramId, \"default_value\");\n      }\n    }\n  });\n\n  // Begin AST traversal for the function body\n  const visitedNodes = new WeakSet();\n\n  function visit(node, parentNode = null) {\n    // Skip if node is null, undefined, or already visited\n    if (!node || typeof node !== \"object\" || visitedNodes.has(node)) {\n      return;\n    }\n\n    visitedNodes.add(node);\n\n    // Handle variable declarations\n    if (node.type === \"VariableDeclaration\") {\n      node.declarations.forEach((declarator) => {\n        if (declarator.id && declarator.id.type === \"Identifier\") {\n          const varId = declarator.id.name;\n          addNode(varId, \"variable\");\n          variables.set(varId, currentScope);\n\n          // If there's an initializer, create a data flow edge\n          if (declarator.init) {\n            // For literals, create a literal node\n            if (declarator.init.type === \"Literal\") {\n              const literalId = `literal_${varId}`;\n              addNode(literalId, \"literal\");\n              addEdge(literalId, varId, \"assignment\");\n            }\n            // For identifiers (another variable)\n            else if (declarator.init.type === \"Identifier\") {\n              const sourceId = declarator.init.name;\n              // Only create edge if source exists as a node\n              if (nodeIds.has(sourceId)) {\n                addEdge(sourceId, varId, \"assignment\");\n              }\n            }\n            // For binary expressions (e.g., a + b)\n            else if (declarator.init.type === \"BinaryExpression\") {\n              processExpression(declarator.init, varId);\n            }\n            // For function calls\n            else if (declarator.init.type === \"CallExpression\") {\n              processCallExpression(declarator.init, varId);\n            }\n          }\n        }\n      });\n    }\n\n    // Handle assignments\n    else if (node.type === \"AssignmentExpression\") {\n      if (node.left.type === \"Identifier\") {\n        const targetId = node.left.name;\n\n        // Make sure the target exists as a node\n        if (!nodeIds.has(targetId)) {\n          addNode(targetId, \"variable\");\n          variables.set(targetId, currentScope);\n        }\n\n        // For identifier on right side (another variable)\n        if (node.right.type === \"Identifier\") {\n          const sourceId = node.right.name;\n          if (nodeIds.has(sourceId)) {\n            addEdge(sourceId, targetId, \"assignment\");\n          }\n        }\n        // For literals\n        else if (node.right.type === \"Literal\") {\n          const literalId = `literal_${targetId}_${node.start}`;\n          addNode(literalId, \"literal\");\n          addEdge(literalId, targetId, \"assignment\");\n        }\n        // For binary expressions\n        else if (node.right.type === \"BinaryExpression\") {\n          processExpression(node.right, targetId);\n        }\n        // For function calls\n        else if (node.right.type === \"CallExpression\") {\n          processCallExpression(node.right, targetId);\n        }\n      }\n    }\n\n    // Handle return statements\n    else if (node.type === \"ReturnStatement\") {\n      if (node.argument) {\n        const returnId = `return_${node.start}`;\n        addNode(returnId, \"return\");\n\n        // Create flow from returned value to return node\n        if (node.argument.type === \"Identifier\") {\n          const sourceId = node.argument.name;\n          if (nodeIds.has(sourceId)) {\n            addEdge(sourceId, returnId, \"return_value\");\n          }\n        }\n        // For literals in return\n        else if (node.argument.type === \"Literal\") {\n          const literalId = `literal_return_${node.start}`;\n          addNode(literalId, \"literal\");\n          addEdge(literalId, returnId, \"return_value\");\n        }\n        // For expressions in return\n        else if (node.argument.type === \"BinaryExpression\") {\n          processExpression(node.argument, returnId, \"return_value\");\n        }\n        // For function calls in return\n        else if (node.argument.type === \"CallExpression\") {\n          processCallExpression(node.argument, returnId, \"return_value\");\n        }\n      }\n    }\n\n    // Handle function calls not covered in other cases\n    else if (\n      node.type === \"CallExpression\" &&\n      parentNode?.type !== \"VariableDeclarator\" &&\n      parentNode?.type !== \"AssignmentExpression\" &&\n      parentNode?.type !== \"ReturnStatement\"\n    ) {\n      processCallExpression(node);\n    }\n\n    // Handle entering new block scope\n    if (node.type === \"BlockStatement\") {\n      scopeChain.push(currentScope);\n      currentScope = `block_${node.start}`;\n    }\n\n    // Recursively visit all child nodes\n    for (const key in node) {\n      const child = node[key];\n\n      // Skip special properties\n      if (\n        key === \"type\" ||\n        key === \"loc\" ||\n        key === \"range\" ||\n        key === \"parent\"\n      ) {\n        continue;\n      }\n\n      if (Array.isArray(child)) {\n        // For arrays (like body), visit each element\n        for (const item of child) {\n          visit(item, node);\n        }\n      } else if (child && typeof child === \"object\") {\n        // Visit child node\n        visit(child, node);\n      }\n    }\n\n    // Handle exiting block scope\n    if (node.type === \"BlockStatement\") {\n      currentScope = scopeChain.pop();\n    }\n  }\n\n  // Helper to process expressions and their effect on data flow\n  function processExpression(expression, targetId, edgeType = \"assignment\") {\n    // For binary expressions like a + b, track flow from operands to result\n    if (expression.left?.type === \"Identifier\") {\n      const leftId = expression.left.name;\n      if (nodeIds.has(leftId)) {\n        addEdge(leftId, targetId, edgeType);\n      }\n    }\n\n    if (expression.right?.type === \"Identifier\") {\n      const rightId = expression.right.name;\n      if (nodeIds.has(rightId)) {\n        addEdge(rightId, targetId, edgeType);\n      }\n    }\n\n    // If operands are themselves expressions, process them recursively\n    if (expression.left?.type === \"BinaryExpression\") {\n      processExpression(expression.left, targetId, edgeType);\n    }\n\n    if (expression.right?.type === \"BinaryExpression\") {\n      processExpression(expression.right, targetId, edgeType);\n    }\n\n    // If operands are literals, create nodes for them\n    if (expression.left?.type === \"Literal\") {\n      const literalId = `literal_left_${expression.left.start}`;\n      addNode(literalId, \"literal\");\n      addEdge(literalId, targetId, edgeType);\n    }\n\n    if (expression.right?.type === \"Literal\") {\n      const literalId = `literal_right_${expression.right.start}`;\n      addNode(literalId, \"literal\");\n      addEdge(literalId, targetId, edgeType);\n    }\n  }\n\n  // Helper to process function calls and their effect on data flow\n  function processCallExpression(\n    callNode,\n    targetId = null,\n    edgeType = \"assignment\"\n  ) {\n    const callId = `call_${callNode.start}`;\n    let functionName = \"unknown\";\n\n    // Try to determine the function name\n    if (callNode.callee.type === \"Identifier\") {\n      functionName = callNode.callee.name;\n    } else if (\n      callNode.callee.type === \"MemberExpression\" &&\n      callNode.callee.property.type === \"Identifier\"\n    ) {\n      functionName = callNode.callee.property.name;\n    }\n\n    addNode(callId, \"function_call\");\n\n    // Connect parameters/arguments to the function call\n    callNode.arguments.forEach((arg, index) => {\n      if (arg.type === \"Identifier\") {\n        const argId = arg.name;\n        if (nodeIds.has(argId)) {\n          addEdge(argId, callId, \"call_argument\");\n        }\n      } else if (arg.type === \"Literal\") {\n        const literalId = `literal_arg_${index}_${callNode.start}`;\n        addNode(literalId, \"literal\");\n        addEdge(literalId, callId, \"call_argument\");\n      }\n    });\n\n    // If this call's result is assigned to a variable\n    if (targetId) {\n      addEdge(callId, targetId, edgeType);\n    }\n  }\n\n  // Start traversal from the function body\n  if (functionAst.body) {\n    visit(functionAst.body);\n  }\n\n  return { nodes, edges };\n}\n", "/**\n * ConversationSegmenter.js\n *\n * Provides functionality to detect topic shifts and segment conversations\n * into coherent topics for better context management.\n */\n\nimport * as TextTokenizerLogic from \"./TextTokenizerLogic.js\";\nimport { executeQuery } from \"../db.js\";\nimport { v4 as uuidv4 } from \"uuid\";\nimport * as ContextCompressorLogic from \"./ContextCompressorLogic.js\";\n\n/**\n * @typedef {Object} Message\n * @property {string} content - The content of the message\n * @property {string} role - The role of the sender (e.g., 'user', 'assistant')\n * @property {Date} [timestamp] - Optional timestamp of the message\n * @property {string[]} [entity_ids] - Optional array of referenced entity IDs\n */\n\n// Conversational shift marker phrases\nconst TOPIC_SHIFT_MARKERS = [\n  \"anyway\",\n  \"moving on\",\n  \"changing subject\",\n  \"regarding\",\n  \"switching to\",\n  \"on another note\",\n  \"back to\",\n  \"speaking of\",\n  \"about\",\n  \"let's talk about\",\n  \"with respect to\",\n  \"turning to\",\n  \"shifting to\",\n  \"let's discuss\",\n  \"instead\",\n];\n\n// Question starters that might indicate new topics\nconst QUESTION_STARTERS = [\n  \"what\",\n  \"how\",\n  \"why\",\n  \"can\",\n  \"could\",\n  \"would\",\n  \"should\",\n  \"is\",\n  \"are\",\n  \"do\",\n  \"does\",\n  \"did\",\n  \"have\",\n  \"has\",\n  \"will\",\n];\n\n/**\n * Detects if a new message represents a significant topic shift\n * compared to the conversation history\n *\n * @param {Message} newMessage - The new message to evaluate\n * @param {Message[]} conversationHistory - Previous messages in the conversation\n * @returns {Promise<boolean>} True if a topic shift is detected, false otherwise\n */\nexport async function detectTopicShift(newMessage, conversationHistory) {\n  try {\n    if (\n      !newMessage?.content ||\n      !conversationHistory ||\n      conversationHistory.length === 0\n    ) {\n      return false;\n    }\n\n    // Only look at recent history (last 5 messages) for comparison\n    const recentHistory = conversationHistory.slice(-5);\n\n    // 1. Keyword novelty detection\n    const keywordNoveltyScore = calculateKeywordNovelty(\n      newMessage,\n      recentHistory\n    );\n\n    // 2. Entity reference shift detection\n    const entityShiftScore = calculateEntityShift(newMessage, recentHistory);\n\n    // 3. Conversational markers detection\n    const hasConversationalMarkers = detectConversationalMarkers(\n      newMessage.content\n    );\n\n    // 4. Question-answer completion detection\n    const questionShiftScore = detectQuestionAnswerShift(\n      newMessage,\n      recentHistory\n    );\n\n    // Combine heuristics with appropriate weights\n    const topicShiftScore =\n      keywordNoveltyScore * 0.4 +\n      entityShiftScore * 0.3 +\n      (hasConversationalMarkers ? 0.8 : 0) * 0.2 +\n      questionShiftScore * 0.1;\n\n    // Return true if the combined score exceeds a threshold\n    return topicShiftScore > 0.45;\n  } catch (error) {\n    console.error(\"Error detecting topic shift:\", error);\n    return false; // Default to no shift on error\n  }\n}\n\n/**\n * Calculates keyword novelty by comparing tokens in new message with recent history\n *\n * @param {Message} newMessage - The new message\n * @param {Message[]} recentHistory - Recent conversation messages\n * @returns {number} Novelty score between 0 and 1\n */\nfunction calculateKeywordNovelty(newMessage, recentHistory) {\n  // Tokenize the new message\n  const newTokens = TextTokenizerLogic.tokenize(newMessage.content);\n\n  // Extract significant keywords from the new message\n  const newKeywords = TextTokenizerLogic.extractKeywords(newTokens, 10);\n  const newKeywordSet = new Set(newKeywords);\n\n  if (newKeywordSet.size === 0) {\n    return 0; // No significant keywords to compare\n  }\n\n  // Build a set of all keywords from recent history\n  const historyKeywordSet = new Set();\n  for (const message of recentHistory) {\n    const historyTokens = TextTokenizerLogic.tokenize(message.content);\n    const historyKeywords = TextTokenizerLogic.extractKeywords(\n      historyTokens,\n      10\n    );\n    historyKeywords.forEach((keyword) => historyKeywordSet.add(keyword));\n  }\n\n  // Count how many new keywords are novel (not in history)\n  let novelKeywordCount = 0;\n  for (const keyword of newKeywordSet) {\n    if (!historyKeywordSet.has(keyword)) {\n      novelKeywordCount++;\n    }\n  }\n\n  // Calculate novelty ratio: novel keywords / total keywords\n  return novelKeywordCount / newKeywordSet.size;\n}\n\n/**\n * Calculates entity reference shift by comparing entity IDs mentioned\n * in new message vs. recent history\n *\n * @param {Message} newMessage - The new message\n * @param {Message[]} recentHistory - Recent conversation messages\n * @returns {number} Entity shift score between 0 and 1\n */\nfunction calculateEntityShift(newMessage, recentHistory) {\n  // If entity_ids are not available, return 0\n  if (\n    !newMessage.entity_ids ||\n    !Array.isArray(newMessage.entity_ids) ||\n    newMessage.entity_ids.length === 0\n  ) {\n    return 0;\n  }\n\n  // Build a set of all entity IDs from recent history\n  const historyEntitySet = new Set();\n  for (const message of recentHistory) {\n    if (message.entity_ids && Array.isArray(message.entity_ids)) {\n      message.entity_ids.forEach((id) => historyEntitySet.add(id));\n    }\n  }\n\n  // If no entities in history, any entity in new message is a shift\n  if (historyEntitySet.size === 0) {\n    return newMessage.entity_ids.length > 0 ? 1 : 0;\n  }\n\n  // Count new entities not present in history\n  let newEntityCount = 0;\n  for (const entityId of newMessage.entity_ids) {\n    if (!historyEntitySet.has(entityId)) {\n      newEntityCount++;\n    }\n  }\n\n  // Calculate entity shift ratio: new entities / total entities\n  return newEntityCount / newMessage.entity_ids.length;\n}\n\n/**\n * Detects conversational markers indicating topic shifts\n *\n * @param {string} messageContent - The content of the message\n * @returns {boolean} True if shift markers are found\n */\nfunction detectConversationalMarkers(messageContent) {\n  if (!messageContent) return false;\n\n  const lowerContent = messageContent.toLowerCase();\n\n  // Check for topic shift marker phrases\n  for (const marker of TOPIC_SHIFT_MARKERS) {\n    // Look for the marker as a whole word\n    const regex = new RegExp(`\\\\b${marker}\\\\b`, \"i\");\n    if (regex.test(lowerContent)) {\n      return true;\n    }\n  }\n\n  return false;\n}\n\n/**\n * Detects if there's a shift in question patterns, indicating topic change\n *\n * @param {Message} newMessage - The new message\n * @param {Message[]} recentHistory - Recent conversation messages\n * @returns {number} Question shift score between 0 and 1\n */\nfunction detectQuestionAnswerShift(newMessage, recentHistory) {\n  // Check if new message is a question\n  const isNewMessageQuestion = isQuestion(newMessage.content);\n\n  if (!isNewMessageQuestion) {\n    return 0; // Not a question, so no question shift\n  }\n\n  // Check recent conversation flow\n  let previousQuestionCount = 0;\n  let questionAnswerPairCount = 0;\n\n  // Evaluate if we have a sequence of Q&A pairs\n  for (let i = 0; i < recentHistory.length - 1; i++) {\n    if (\n      recentHistory[i].role === \"user\" &&\n      isQuestion(recentHistory[i].content)\n    ) {\n      previousQuestionCount++;\n\n      // Check if next message is an answer (from assistant)\n      if (\n        i + 1 < recentHistory.length &&\n        recentHistory[i + 1].role === \"assistant\"\n      ) {\n        questionAnswerPairCount++;\n      }\n    }\n  }\n\n  // If we've had a series of Q&A exchanges and a new question appears,\n  // it's more likely to be a topic shift\n  if (previousQuestionCount > 0 && questionAnswerPairCount > 0) {\n    // Compare the question type/subject of the new question vs. previous questions\n    const lastUserQuestionIndex = findLastIndex(\n      recentHistory,\n      (msg) => msg.role === \"user\" && isQuestion(msg.content)\n    );\n\n    if (lastUserQuestionIndex >= 0) {\n      const lastUserQuestion = recentHistory[lastUserQuestionIndex].content;\n      return calculateQuestionDifference(newMessage.content, lastUserQuestion);\n    }\n  }\n\n  return 0.2; // Default modest score if it's a new question\n}\n\n/**\n * Determines if a message is a question\n *\n * @param {string} content - Message content\n * @returns {boolean} True if it appears to be a question\n */\nfunction isQuestion(content) {\n  if (!content) return false;\n\n  // Check for question marks\n  if (content.includes(\"?\")) {\n    return true;\n  }\n\n  // Check for question starter words\n  const lowerContent = content.toLowerCase().trim();\n  for (const starter of QUESTION_STARTERS) {\n    if (lowerContent.startsWith(starter + \" \")) {\n      return true;\n    }\n  }\n\n  return false;\n}\n\n/**\n * Calculates the difference between two questions to detect topic shift\n *\n * @param {string} newQuestion - The new question\n * @param {string} previousQuestion - A previous question from history\n * @returns {number} Difference score between 0 and 1\n */\nfunction calculateQuestionDifference(newQuestion, previousQuestion) {\n  const newTokens = TextTokenizerLogic.tokenize(newQuestion);\n  const prevTokens = TextTokenizerLogic.tokenize(previousQuestion);\n\n  // Use Jaccard similarity to compare question content\n  const newSet = new Set(newTokens);\n  const prevSet = new Set(prevTokens);\n\n  // Calculate intersection size\n  let intersectionSize = 0;\n  for (const token of newSet) {\n    if (prevSet.has(token)) {\n      intersectionSize++;\n    }\n  }\n\n  // Calculate union size\n  const unionSize = newSet.size + prevSet.size - intersectionSize;\n\n  // Jaccard similarity: intersection size / union size\n  const similarity = unionSize > 0 ? intersectionSize / unionSize : 0;\n\n  // Return difference (1 - similarity)\n  return 1 - similarity;\n}\n\n/**\n * Custom implementation of findLastIndex for compatibility\n *\n * @param {Array} array - The array to search\n * @param {Function} predicate - The predicate function\n * @returns {number} The last matching index or -1 if not found\n */\nfunction findLastIndex(array, predicate) {\n  for (let i = array.length - 1; i >= 0; i--) {\n    if (predicate(array[i])) {\n      return i;\n    }\n  }\n  return -1;\n}\n\n/**\n * Creates a new topic segment in the conversation\n *\n * @param {string} conversationId - ID of the conversation\n * @param {string} startMessageId - ID of the message where the topic starts\n * @param {Object} topicInfo - Information about the topic\n * @param {string} [topicInfo.name] - Optional name for the topic\n * @param {string} [topicInfo.description] - Optional description of the topic\n * @param {string[]} [topicInfo.primaryEntities] - Optional list of primary entity IDs for this topic\n * @param {string[]} [topicInfo.keywords] - Optional list of keywords characterizing this topic\n * @returns {Promise<string>} The ID of the newly created topic segment\n */\nexport async function createNewTopicSegment(\n  conversationId,\n  startMessageId,\n  topicInfo = {}\n) {\n  try {\n    // 1. Generate UUID for the topic\n    const topic_id = uuidv4();\n\n    // 2. Determine topic name\n    let topic_name = topicInfo.name;\n    if (!topic_name) {\n      // If no name provided, use a timestamp-based placeholder\n      // In a real implementation, we'd call generateTopicName() here\n      topic_name = `New Topic ${new Date().toISOString()}`;\n\n      // Alternatively, try to extract from the start message content\n      try {\n        const messageQuery =\n          \"SELECT content FROM conversation_history WHERE message_id = ?\";\n        const messageResult = await executeQuery(messageQuery, [\n          startMessageId,\n        ]);\n\n        if (messageResult && messageResult.length > 0) {\n          const content = messageResult[0].content;\n          // Use first few words (up to 5) as a generic name\n          const words = content.split(/\\s+/).slice(0, 5).join(\" \");\n          if (words.length > 3) {\n            topic_name = `Topic: ${words}${\n              words.length < content.length ? \"...\" : \"\"\n            }`;\n          }\n        }\n      } catch (error) {\n        console.warn(\n          \"Could not fetch message content for topic naming:\",\n          error\n        );\n        // Fall back to the timestamp-based name already set\n      }\n    }\n\n    // 3. Prepare entities and keywords as JSON strings\n    const primary_entities = topicInfo.primaryEntities\n      ? JSON.stringify(topicInfo.primaryEntities)\n      : \"[]\";\n\n    const keywords = topicInfo.keywords\n      ? JSON.stringify(topicInfo.keywords)\n      : \"[]\";\n\n    // 4. Get current timestamp for start_timestamp\n    const start_timestamp = new Date().toISOString();\n\n    // 5. Insert the new topic into the database\n    // Try to disable foreign key constraints temporarily\n    await executeQuery(\"PRAGMA foreign_keys = OFF;\");\n\n    const insertQuery = `\n      INSERT INTO conversation_topics (\n        topic_id,\n        conversation_id,\n        topic_name,\n        description,\n        start_message_id,\n        start_timestamp,\n        primary_entities,\n        keywords\n      ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n    `;\n\n    const params = [\n      topic_id,\n      conversationId,\n      topic_name,\n      topicInfo.description || \"\",\n      startMessageId,\n      start_timestamp,\n      primary_entities,\n      keywords,\n    ];\n\n    await executeQuery(insertQuery, params);\n\n    // Re-enable foreign key constraints\n    await executeQuery(\"PRAGMA foreign_keys = ON;\");\n\n    console.log(`Created new topic segment: ${topic_name} (${topic_id})`);\n\n    // 6. Return the topic_id\n    return topic_id;\n  } catch (error) {\n    console.error(\"Error creating new topic segment:\", error);\n    throw new Error(`Failed to create new topic segment: ${error.message}`);\n  }\n}\n\n/**\n * Closes a topic segment by setting its end message and timestamp\n *\n * @param {string} topicId - ID of the topic segment to close\n * @param {string} endMessageId - ID of the message where the topic ends\n * @returns {Promise<void>}\n */\nexport async function closeTopicSegment(topicId, endMessageId) {\n  try {\n    // 1. Get current timestamp for end_timestamp\n    const end_timestamp = new Date().toISOString();\n\n    // 2. Try to get message timestamp if available\n    let messageTimestamp = end_timestamp;\n    try {\n      const messageQuery =\n        \"SELECT timestamp FROM conversation_history WHERE message_id = ?\";\n      const messageResult = await executeQuery(messageQuery, [endMessageId]);\n\n      if (\n        messageResult &&\n        messageResult.length > 0 &&\n        messageResult[0].timestamp\n      ) {\n        messageTimestamp = messageResult[0].timestamp;\n      }\n    } catch (error) {\n      console.warn(\n        \"Could not fetch message timestamp, using current timestamp:\",\n        error\n      );\n      // Continue with the current timestamp that we've already set\n    }\n\n    // 3. Update the topic record in the database\n    const updateQuery = `\n      UPDATE conversation_topics\n      SET end_message_id = ?,\n          end_timestamp = ?\n      WHERE topic_id = ?\n    `;\n\n    const params = [endMessageId, messageTimestamp, topicId];\n\n    await executeQuery(updateQuery, params);\n\n    console.log(\n      `Closed topic segment: ${topicId} with end message: ${endMessageId}`\n    );\n\n    // 4. Generate topic summary (Note: Implementation not provided yet)\n    // This would be a good place to call a function like:\n    // await summarizeTopicSegment(topicId);\n  } catch (error) {\n    console.error(`Error closing topic segment ${topicId}:`, error);\n    throw new Error(`Failed to close topic segment: ${error.message}`);\n  }\n}\n\n/**\n * @typedef {Object} Topic\n * @property {string} topic_id - Unique identifier for the topic\n * @property {string} conversation_id - ID of the conversation this topic belongs to\n * @property {string} topic_name - Name of the topic\n * @property {string} description - Description of the topic\n * @property {string} start_message_id - ID of the message where the topic starts\n * @property {string} start_timestamp - Timestamp when the topic started\n * @property {string|null} end_message_id - ID of the message where the topic ends (null if active)\n * @property {string|null} end_timestamp - Timestamp when the topic ended (null if active)\n * @property {string[]} primary_entities - List of primary entity IDs for this topic\n * @property {string[]} keywords - List of keywords characterizing this topic\n */\n\n/**\n * Gets the currently active topic for a conversation\n *\n * @param {string} conversationId - ID of the conversation\n * @returns {Promise<Topic|null>} The active topic or null if no active topic exists\n */\nexport async function getActiveTopicForConversation(conversationId) {\n  try {\n    // Query for the active topic (where end_message_id is NULL)\n    const query = `\n      SELECT * FROM conversation_topics\n      WHERE conversation_id = ?\n        AND end_message_id IS NULL\n      ORDER BY start_timestamp DESC\n      LIMIT 1\n    `;\n\n    const result = await executeQuery(query, [conversationId]);\n\n    // If no active topic found, return null\n    if (!result || result.length === 0) {\n      return null;\n    }\n\n    // Get the active topic (the most recent one if multiple exist)\n    const activeTopic = result[0];\n\n    // Parse JSON fields\n    try {\n      // Parse primary_entities JSON string to array\n      activeTopic.primary_entities = activeTopic.primary_entities\n        ? JSON.parse(activeTopic.primary_entities)\n        : [];\n\n      // Parse keywords JSON string to array\n      activeTopic.keywords = activeTopic.keywords\n        ? JSON.parse(activeTopic.keywords)\n        : [];\n    } catch (jsonError) {\n      console.warn(\n        `Error parsing JSON fields for topic ${activeTopic.topic_id}:`,\n        jsonError\n      );\n      // Provide default empty arrays if JSON parsing fails\n      activeTopic.primary_entities = activeTopic.primary_entities || [];\n      activeTopic.keywords = activeTopic.keywords || [];\n    }\n\n    return activeTopic;\n  } catch (error) {\n    console.error(\n      `Error getting active topic for conversation ${conversationId}:`,\n      error\n    );\n    throw new Error(`Failed to get active topic: ${error.message}`);\n  }\n}\n\n/**\n * Generates a summary for a topic segment and updates the topic record\n *\n * @param {string} topicId - ID of the topic to summarize\n * @returns {Promise<string>} The generated summary\n */\nexport async function summarizeTopicSegment(topicId) {\n  try {\n    // 1. Get all messages belonging to this topic\n    const messages = await getTopicSegmentMessages(topicId);\n\n    if (!messages || messages.length === 0) {\n      const noMessagesWarning = \"No messages found for topic summarization\";\n      console.warn(noMessagesWarning);\n      return noMessagesWarning;\n    }\n\n    // 2. Concatenate the content of these messages\n    const concatenatedContent = messages\n      .map((msg) => {\n        // Format each message with role information\n        return `${msg.role}: ${msg.content}`;\n      })\n      .join(\"\\n\\n\");\n\n    // 3. Use ContextCompressorLogic to generate a summary\n    const summary = await ContextCompressorLogic.summarizeText(\n      concatenatedContent,\n      {\n        targetLength: 150, // Target 150 characters for the summary\n        preserveKeyPoints: true,\n      }\n    );\n\n    // 4. Update the topic record with the summary\n    const updateQuery = `\n      UPDATE conversation_topics\n      SET summary = ?\n      WHERE topic_id = ?\n    `;\n\n    await executeQuery(updateQuery, [summary, topicId]);\n\n    console.log(\n      `Topic ${topicId} summary generated and stored: ${summary.substring(\n        0,\n        50\n      )}...`\n    );\n\n    // 5. Return the generated summary\n    return summary;\n  } catch (error) {\n    console.error(`Error summarizing topic segment ${topicId}:`, error);\n    throw new Error(`Failed to summarize topic segment: ${error.message}`);\n  }\n}\n\n/**\n * Gets all messages belonging to a topic segment\n *\n * @param {string} topicId - ID of the topic\n * @returns {Promise<Array<Message>>} Array of messages with parsed JSON fields\n */\nexport async function getTopicSegmentMessages(topicId) {\n  try {\n    // Query conversation_history table directly using topic_segment_id\n    const messagesQuery = `\n      SELECT * FROM conversation_history \n      WHERE topic_segment_id = ? \n      ORDER BY timestamp ASC\n    `;\n\n    const messages = await executeQuery(messagesQuery, [topicId]);\n\n    if (!messages || messages.length === 0) {\n      return [];\n    }\n\n    // Parse JSON fields for each message\n    return messages.map((message) => {\n      try {\n        // Parse related_context_entity_ids JSON field\n        if (message.related_context_entity_ids) {\n          message.related_context_entity_ids = JSON.parse(\n            message.related_context_entity_ids\n          );\n        } else {\n          message.related_context_entity_ids = [];\n        }\n\n        // Parse semantic_markers JSON field\n        if (message.semantic_markers) {\n          message.semantic_markers = JSON.parse(message.semantic_markers);\n        } else {\n          message.semantic_markers = [];\n        }\n\n        // Parse sentiment_indicators JSON field\n        if (message.sentiment_indicators) {\n          message.sentiment_indicators = JSON.parse(\n            message.sentiment_indicators\n          );\n        } else {\n          message.sentiment_indicators = {};\n        }\n\n        return message;\n      } catch (jsonError) {\n        console.warn(\n          `Error parsing JSON fields for message ${message.message_id}:`,\n          jsonError\n        );\n        // Return message with default empty values for JSON fields\n        return {\n          ...message,\n          related_context_entity_ids: message.related_context_entity_ids || [],\n          semantic_markers: message.semantic_markers || [],\n          sentiment_indicators: message.sentiment_indicators || {},\n        };\n      }\n    });\n  } catch (error) {\n    console.error(`Error getting messages for topic ${topicId}:`, error);\n    throw new Error(`Failed to get topic messages: ${error.message}`);\n  }\n}\n\n/**\n * Generates a concise, descriptive topic name from a set of messages\n *\n * @param {string[]} messageIds - Array of message IDs to generate a topic name from\n * @returns {Promise<string>} A concise topic name\n */\nexport async function generateTopicName(messageIds) {\n  try {\n    if (!messageIds || messageIds.length === 0) {\n      return \"Untitled Topic\";\n    }\n\n    // 1. Fetch the content of messages\n    const placeholders = messageIds.map(() => \"?\").join(\",\");\n    const messagesQuery = `\n      SELECT content, related_context_entity_ids\n      FROM conversation_history \n      WHERE message_id IN (${placeholders})\n      ORDER BY timestamp ASC\n    `;\n\n    const messages = await executeQuery(messagesQuery, messageIds);\n\n    if (!messages || messages.length === 0) {\n      return \"Untitled Topic\";\n    }\n\n    // 2. Concatenate message contents\n    const concatenatedContent = messages.map((msg) => msg.content).join(\" \");\n\n    // 3. Tokenize the content\n    const tokens = TextTokenizerLogic.tokenize(concatenatedContent);\n\n    // 4. Extract keywords from the content\n    const keywords = TextTokenizerLogic.extractKeywords(tokens, 3);\n\n    // 5. Check for entity references in the messages\n    const entityReferences = new Set();\n    for (const message of messages) {\n      if (message.related_context_entity_ids) {\n        let entityIds;\n        try {\n          entityIds =\n            typeof message.related_context_entity_ids === \"string\"\n              ? JSON.parse(message.related_context_entity_ids)\n              : message.related_context_entity_ids;\n\n          if (Array.isArray(entityIds) && entityIds.length > 0) {\n            // Get entity names for up to 2 entities\n            const entityIds = entityIds.slice(0, 2);\n            const entityQuery = `\n              SELECT name FROM code_entities WHERE id IN (${entityIds\n                .map(() => \"?\")\n                .join(\",\")})\n            `;\n\n            const entities = await executeQuery(entityQuery, entityIds);\n            if (entities && entities.length > 0) {\n              entities.forEach((entity) => entityReferences.add(entity.name));\n            }\n          }\n        } catch (err) {\n          console.warn(\"Error parsing entity IDs\", err);\n        }\n      }\n    }\n\n    // 6. Formulate a topic name\n    let topicName;\n\n    // If we have entity references, prioritize those\n    if (entityReferences.size > 0) {\n      const entityNames = Array.from(entityReferences).slice(0, 2);\n      topicName = `Discussion about ${entityNames.join(\" and \")}`;\n    }\n    // Otherwise use the keywords\n    else if (keywords.length > 0) {\n      topicName = `Topic: ${keywords.join(\", \")}`;\n    }\n    // Fallback to using the first message\n    else {\n      // Get first few significant words from the initial message\n      const firstMsg = messages[0].content;\n      const firstFewWords = firstMsg.split(/\\s+/).slice(0, 5).join(\" \");\n      topicName = `Topic: ${firstFewWords}${\n        firstMsg.length > firstFewWords.length ? \"...\" : \"\"\n      }`;\n    }\n\n    // 7. Ensure topic name is not too long\n    if (topicName.length > 50) {\n      topicName = topicName.substring(0, 47) + \"...\";\n    }\n\n    return topicName;\n  } catch (error) {\n    console.error(`Error generating topic name:`, error);\n    return \"Untitled Topic\";\n  }\n}\n\n/**\n * Builds a hierarchical representation of topics in a conversation\n *\n * @param {string} conversationId - ID of the conversation\n * @returns {Promise<{rootTopics: Topic[], topicMap: Record<string, Topic>}>} Hierarchical topic structure\n */\nexport async function buildTopicHierarchy(conversationId) {\n  try {\n    // 1. Fetch all topics for the conversation, ordered by start_timestamp\n    const query = `\n      SELECT * FROM conversation_topics\n      WHERE conversation_id = ?\n      ORDER BY start_timestamp ASC\n    `;\n\n    const topics = await executeQuery(query, [conversationId]);\n\n    if (!topics || topics.length === 0) {\n      return { rootTopics: [], topicMap: {} };\n    }\n\n    // 2. Create the topic map and parse JSON fields\n    const topicMap = {};\n\n    for (const topic of topics) {\n      // Parse JSON fields\n      try {\n        // Parse primary_entities JSON string to array\n        topic.primary_entities = topic.primary_entities\n          ? JSON.parse(topic.primary_entities)\n          : [];\n\n        // Parse keywords JSON string to array\n        topic.keywords = topic.keywords ? JSON.parse(topic.keywords) : [];\n\n        // Add children array to each topic\n        topic.children = [];\n\n        // Add to topic map\n        topicMap[topic.topic_id] = topic;\n      } catch (jsonError) {\n        console.warn(\n          `Error parsing JSON fields for topic ${topic.topic_id}:`,\n          jsonError\n        );\n        // Provide default empty arrays if JSON parsing fails\n        topic.primary_entities = [];\n        topic.keywords = [];\n        topic.children = [];\n\n        // Still add to map even with parse error\n        topicMap[topic.topic_id] = topic;\n      }\n    }\n\n    // 3. Build the hierarchy by connecting parents and children\n    const rootTopics = [];\n\n    for (const topic of topics) {\n      if (topic.parent_topic_id && topicMap[topic.parent_topic_id]) {\n        // Add this topic as a child of its parent\n        topicMap[topic.parent_topic_id].children.push(topic);\n      } else {\n        // This is a root topic (no parent or parent not in the map)\n        rootTopics.push(topic);\n      }\n    }\n\n    return { rootTopics, topicMap };\n  } catch (error) {\n    console.error(\n      `Error building topic hierarchy for conversation ${conversationId}:`,\n      error\n    );\n    throw new Error(`Failed to build topic hierarchy: ${error.message}`);\n  }\n}\n", "/**\n * ContextCompressorLogic.js\n *\n * Logic for compressing and summarizing text content to fit within\n * specific size constraints while preserving important information.\n */\n\nimport { tokenize } from \"./TextTokenizerLogic.js\";\n\n/**\n * @typedef {Object} ScoredSnippet\n * @property {Object} entity - The code entity object\n * @property {number} score - Relevance score\n * @property {string} [content] - Optional pre-provided content\n */\n\n/**\n * @typedef {Object} ProcessedSnippet\n * @property {string} entity_id - ID of the entity\n * @property {string} summarizedContent - Final summarized content\n * @property {number} originalScore - Original relevance score\n */\n\n/**\n * Manage token budget distribution across context snippets\n *\n * @param {ScoredSnippet[]} contextSnippets - Array of scored context snippets\n * @param {number} budget - Total character budget to distribute\n * @param {string[]} [queryKeywords] - Optional query keywords for targeted summarization\n * @returns {ProcessedSnippet[]} Array of processed snippets with summarized content\n */\nexport function manageTokenBudget(contextSnippets, budget, queryKeywords = []) {\n  if (!contextSnippets || contextSnippets.length === 0) {\n    return [];\n  }\n\n  // Initialize result array\n  const processedSnippets = [];\n\n  // Track remaining budget\n  let remainingBudget = budget;\n\n  // Calculate initial budget allocation based on scores\n  const totalScore = contextSnippets.reduce(\n    (sum, snippet) => sum + snippet.score,\n    0\n  );\n  const budgetAllocations = contextSnippets.map((snippet) => {\n    // Allocate budget proportionally to score, with a minimum of 100 chars\n    return Math.max(100, Math.floor((snippet.score / totalScore) * budget));\n  });\n\n  // Process snippets in order of score/relevance (assume they're already sorted)\n  for (let i = 0; i < contextSnippets.length; i++) {\n    const snippet = contextSnippets[i];\n\n    // Get content from snippet, preferring pre-provided content\n    const content = snippet.content || snippet.entity.raw_content || \"\";\n\n    // Skip if no content\n    if (!content) {\n      continue;\n    }\n\n    // Get allocated budget for this snippet\n    let snippetBudget = Math.min(budgetAllocations[i], remainingBudget);\n\n    // If remaining budget is too small, skip this snippet\n    if (snippetBudget < 50) {\n      continue;\n    }\n\n    // Check if content fits within allocated budget\n    if (content.length <= snippetBudget) {\n      // Content fits as-is\n      processedSnippets.push({\n        entity_id: snippet.entity.entity_id,\n        summarizedContent: content,\n        originalScore: snippet.score,\n      });\n\n      remainingBudget -= content.length;\n    } else {\n      // Need to summarize content\n      let summarizedContent;\n\n      // Summarize based on entity type\n      if (snippet.entity.entity_type) {\n        summarizedContent = summarizeCodeEntity(\n          snippet.entity,\n          snippetBudget,\n          queryKeywords\n        );\n      } else {\n        summarizedContent = summarizeText(content, snippetBudget);\n      }\n\n      // Add to processed snippets if summarization succeeded\n      if (summarizedContent) {\n        processedSnippets.push({\n          entity_id: snippet.entity.entity_id,\n          summarizedContent,\n          originalScore: snippet.score,\n        });\n\n        remainingBudget -= summarizedContent.length;\n      }\n    }\n\n    // Stop if budget is exhausted\n    if (remainingBudget <= 50) {\n      break;\n    }\n\n    // Redistribute remaining budget to future snippets\n    if (i < contextSnippets.length - 1) {\n      const remainingSnippets = contextSnippets.length - i - 1;\n      const remainingScores = contextSnippets\n        .slice(i + 1)\n        .reduce((sum, s) => sum + s.score, 0);\n\n      // Recalculate budget allocations for remaining snippets\n      for (let j = i + 1; j < contextSnippets.length; j++) {\n        budgetAllocations[j] = Math.max(\n          100,\n          Math.floor(\n            (contextSnippets[j].score / remainingScores) * remainingBudget\n          )\n        );\n      }\n    }\n  }\n\n  // If we have significant remaining budget and processed snippets,\n  // try to use it to expand summaries\n  if (remainingBudget > 200 && processedSnippets.length > 0) {\n    redistributeRemainingBudget(\n      processedSnippets,\n      contextSnippets,\n      remainingBudget,\n      queryKeywords\n    );\n  }\n\n  return processedSnippets;\n}\n\n/**\n * Redistribute remaining budget to expand summaries\n *\n * @param {ProcessedSnippet[]} processedSnippets - Already processed snippets\n * @param {ScoredSnippet[]} originalSnippets - Original scored snippets\n * @param {number} remainingBudget - Remaining character budget\n * @param {string[]} queryKeywords - Query keywords for summarization\n */\nfunction redistributeRemainingBudget(\n  processedSnippets,\n  originalSnippets,\n  remainingBudget,\n  queryKeywords\n) {\n  // Create a map of processed snippets for quick lookup\n  const processedMap = new Map();\n  processedSnippets.forEach((ps) => {\n    processedMap.set(ps.entity_id, ps);\n  });\n\n  // Filter original snippets to only include those that were processed\n  // and sort by score (highest first)\n  const snippetsToExpand = originalSnippets\n    .filter((s) => processedMap.has(s.entity.entity_id))\n    .sort((a, b) => b.score - a.score);\n\n  // Calculate additional budget per snippet\n  const additionalBudgetPerSnippet = Math.floor(\n    remainingBudget / snippetsToExpand.length\n  );\n\n  // Expand each snippet with additional budget\n  for (const snippet of snippetsToExpand) {\n    const processedSnippet = processedMap.get(snippet.entity.entity_id);\n    const currentLength = processedSnippet.summarizedContent.length;\n    const newBudget = currentLength + additionalBudgetPerSnippet;\n\n    // Get content from snippet\n    const content = snippet.content || snippet.entity.raw_content || \"\";\n\n    // If original content fits in new budget, use it\n    if (content.length <= newBudget) {\n      processedSnippet.summarizedContent = content;\n      remainingBudget -= content.length - currentLength;\n    } else {\n      // Otherwise, re-summarize with expanded budget\n      let expandedContent;\n\n      if (snippet.entity.entity_type) {\n        expandedContent = summarizeCodeEntity(\n          snippet.entity,\n          newBudget,\n          queryKeywords\n        );\n      } else {\n        expandedContent = summarizeText(content, newBudget);\n      }\n\n      if (expandedContent && expandedContent.length > currentLength) {\n        remainingBudget -= expandedContent.length - currentLength;\n        processedSnippet.summarizedContent = expandedContent;\n      }\n    }\n\n    // Stop if remaining budget gets too small\n    if (remainingBudget < 100) {\n      break;\n    }\n  }\n}\n\n/**\n * Summarize text to fit within a maximum length\n *\n * @param {string} text - The text to summarize\n * @param {number} maxLength - Maximum character length for the summary\n * @param {'rule-based' | 'ml-light'} [method='rule-based'] - Summarization method to use\n * @returns {string} Summarized text\n */\nexport function summarizeText(text, maxLength, method = \"rule-based\") {\n  // Validate inputs\n  if (!text) return \"\";\n  if (text.length <= maxLength) return text;\n\n  // Check method and apply fallback if necessary\n  if (method === \"ml-light\") {\n    console.log(\n      \"ML-light summarization not fully implemented, falling back to rule-based method\"\n    );\n    method = \"rule-based\";\n  }\n\n  // Apply rule-based summarization\n  return ruleBased(text, maxLength);\n}\n\n/**\n * Summarize a code entity based on its type and content\n *\n * @param {Object} entity - Code entity object from code_entities table\n * @param {number} budget - Maximum characters for the summary\n * @param {string[]} [queryKeywords] - Optional keywords to highlight in the summary\n * @returns {string} Summarized entity content\n */\nexport function summarizeCodeEntity(entity, budget, queryKeywords = []) {\n  // Check if entity is valid\n  if (!entity) return \"\";\n\n  // Use existing summary if available and within budget\n  if (entity.summary && entity.summary.length <= budget) {\n    return entity.summary;\n  }\n\n  // If raw content is empty, return entity name\n  if (!entity.raw_content) {\n    return `${entity.name} (${entity.entity_type})`;\n  }\n\n  // If raw content fits within budget, return it directly\n  if (entity.raw_content.length <= budget) {\n    return entity.raw_content;\n  }\n\n  // Generate summary based on entity type\n  const entityType = (entity.entity_type || \"\").toLowerCase();\n\n  switch (entityType) {\n    case \"function\":\n    case \"method\":\n      return summarizeFunction(entity, budget, queryKeywords);\n\n    case \"class\":\n      return summarizeClass(entity, budget, queryKeywords);\n\n    case \"file\":\n      return summarizeFile(entity, budget, queryKeywords);\n\n    default:\n      // For other entity types, use generic text summarization\n      return summarizeText(entity.raw_content, budget);\n  }\n}\n\n/**\n * Summarize a function or method\n *\n * @param {Object} entity - Function entity\n * @param {number} budget - Character budget\n * @param {string[]} queryKeywords - Keywords to prioritize\n * @returns {string} Function summary\n */\nfunction summarizeFunction(entity, budget, queryKeywords) {\n  const content = entity.raw_content;\n  const lines = content.split(\"\\n\");\n\n  // Extract function signature\n  const signatureLine = extractFunctionSignature(lines);\n\n  // If we can only fit the signature, return just that\n  if (signatureLine.length >= budget - 10) {\n    return truncateToMaxLength(signatureLine, budget);\n  }\n\n  // Score lines by importance\n  const scoredLines = scoreCodeLines(lines, queryKeywords, \"function\");\n\n  // Begin with the signature\n  let summary = signatureLine;\n  let remainingBudget = budget - signatureLine.length;\n\n  // Add comment block if it exists\n  const commentBlock = extractCommentBlock(lines);\n  if (commentBlock && commentBlock.length < remainingBudget * 0.4) {\n    summary += \"\\n\" + commentBlock;\n    remainingBudget -= commentBlock.length;\n  }\n\n  // Add important lines\n  summary += \"\\n\" + selectImportantLines(scoredLines, remainingBudget);\n\n  // Ensure we're within budget\n  return truncateToMaxLength(summary, budget);\n}\n\n/**\n * Summarize a class\n *\n * @param {Object} entity - Class entity\n * @param {number} budget - Character budget\n * @param {string[]} queryKeywords - Keywords to prioritize\n * @returns {string} Class summary\n */\nfunction summarizeClass(entity, budget, queryKeywords) {\n  const content = entity.raw_content;\n  const lines = content.split(\"\\n\");\n\n  // Extract class signature and method list\n  const classSignature = extractClassSignature(lines);\n  const methodList = extractMethodList(lines);\n\n  // Start with class signature\n  let summary = classSignature;\n  let remainingBudget = budget - classSignature.length;\n\n  // Add method list if it fits\n  if (methodList && methodList.length < remainingBudget) {\n    summary += \"\\n\" + methodList;\n    remainingBudget -= methodList.length;\n  }\n\n  // If we still have budget, add important lines\n  if (remainingBudget > 50) {\n    const scoredLines = scoreCodeLines(lines, queryKeywords, \"class\");\n    summary += \"\\n\" + selectImportantLines(scoredLines, remainingBudget);\n  }\n\n  return truncateToMaxLength(summary, budget);\n}\n\n/**\n * Summarize a file\n *\n * @param {Object} entity - File entity\n * @param {number} budget - Character budget\n * @param {string[]} queryKeywords - Keywords to prioritize\n * @returns {string} File summary\n */\nfunction summarizeFile(entity, budget, queryKeywords) {\n  const content = entity.raw_content;\n  const lines = content.split(\"\\n\");\n\n  // Check if it's a README or documentation file\n  const isDocFile =\n    (entity.name || \"\").toLowerCase().includes(\"readme\") ||\n    (entity.name || \"\").toLowerCase().includes(\"doc\");\n\n  if (isDocFile) {\n    // For documentation files, use text summarization\n    return summarizeText(content, budget);\n  }\n\n  // Extract import/require statements\n  const importStatements = lines\n    .filter(\n      (line) =>\n        line.trim().startsWith(\"import \") ||\n        line.trim().startsWith(\"require(\") ||\n        line.trim().startsWith(\"from \") ||\n        line.trim().includes(\" from \")\n    )\n    .join(\"\\n\");\n\n  // Extract export statements\n  const exportStatements = lines\n    .filter(\n      (line) =>\n        line.trim().startsWith(\"export \") ||\n        line.trim().startsWith(\"module.exports\")\n    )\n    .join(\"\\n\");\n\n  // Start building summary\n  let summary = `// File: ${entity.name || \"Unnamed\"}\\n`;\n\n  // Add imports if they fit\n  if (importStatements && importStatements.length < budget * 0.3) {\n    summary += `// Imports:\\n${importStatements}\\n`;\n  }\n\n  // Add exports if they fit\n  const remainingAfterImports = budget - summary.length;\n  if (\n    exportStatements &&\n    exportStatements.length < remainingAfterImports * 0.3\n  ) {\n    summary += `// Exports:\\n${exportStatements}\\n`;\n  }\n\n  // Score and add other important lines\n  const remainingBudget = budget - summary.length;\n  if (remainingBudget > 100) {\n    const scoredLines = scoreCodeLines(lines, queryKeywords, \"file\");\n    summary += `// Key sections:\\n${selectImportantLines(\n      scoredLines,\n      remainingBudget\n    )}`;\n  }\n\n  return truncateToMaxLength(summary, budget);\n}\n\n/**\n * Extract function signature from code lines\n *\n * @param {string[]} lines - Code lines\n * @returns {string} Function signature\n */\nfunction extractFunctionSignature(lines) {\n  // Look for function declarations\n  for (let i = 0; i < lines.length; i++) {\n    const line = lines[i].trim();\n    if (\n      line.match(\n        /^(async\\s+)?(function\\s+\\w+|\\w+\\s*=\\s*(async\\s+)?function|\\w+\\s*:\\s*(async\\s+)?function|const\\s+\\w+\\s*=\\s*(async\\s+)?(\\([^)]*\\)|[^=]*)\\s*=>)/\n      )\n    ) {\n      // Function found, get signature and opening bracket\n      let signature = line;\n\n      // If the line doesn't contain an opening brace, look for it\n      if (!line.includes(\"{\") && !line.includes(\"=>\")) {\n        let j = i + 1;\n        while (j < lines.length && !lines[j].includes(\"{\")) {\n          signature += \" \" + lines[j].trim();\n          j++;\n        }\n        if (j < lines.length) {\n          signature += \" \" + lines[j].trim().split(\"{\")[0] + \"{ ... }\";\n        }\n      } else if (line.includes(\"{\")) {\n        signature = signature.split(\"{\")[0] + \"{ ... }\";\n      } else if (line.includes(\"=>\")) {\n        const arrowParts = signature.split(\"=>\");\n        signature = arrowParts[0] + \"=> { ... }\";\n      }\n\n      return signature;\n    }\n  }\n\n  // If no function signature found, return a placeholder\n  return \"function() { ... }\";\n}\n\n/**\n * Extract class signature from code lines\n *\n * @param {string[]} lines - Code lines\n * @returns {string} Class signature\n */\nfunction extractClassSignature(lines) {\n  // Look for class declarations\n  for (let i = 0; i < lines.length; i++) {\n    const line = lines[i].trim();\n    if (line.startsWith(\"class \")) {\n      // Class found, get signature and opening bracket\n      let signature = line;\n\n      // If the line doesn't contain an opening brace, look for it\n      if (!line.includes(\"{\")) {\n        let j = i + 1;\n        while (j < lines.length && !lines[j].includes(\"{\")) {\n          signature += \" \" + lines[j].trim();\n          j++;\n        }\n        if (j < lines.length) {\n          signature += \" \" + lines[j].trim().split(\"{\")[0] + \"{ ... }\";\n        }\n      } else {\n        signature = signature.split(\"{\")[0] + \"{ ... }\";\n      }\n\n      return signature;\n    }\n  }\n\n  // If no class signature found, return a placeholder\n  return \"class { ... }\";\n}\n\n/**\n * Extract a list of methods from a class\n *\n * @param {string[]} lines - Code lines\n * @returns {string} Formatted method list\n */\nfunction extractMethodList(lines) {\n  const methods = [];\n\n  // Regex to match method declarations\n  const methodRegex = /^\\s*(async\\s+)?(\\w+)\\s*\\([^)]*\\)/;\n\n  // Skip the first few lines to avoid matching the class declaration\n  const startFromLine = Math.min(5, lines.length);\n\n  for (let i = startFromLine; i < lines.length; i++) {\n    const match = lines[i].match(methodRegex);\n    if (match && !lines[i].trim().startsWith(\"//\")) {\n      methods.push(match[2]); // Push the method name\n    }\n  }\n\n  if (methods.length === 0) {\n    return \"\";\n  }\n\n  return `// Methods: ${methods.join(\", \")}`;\n}\n\n/**\n * Extract comment block from the beginning of code\n *\n * @param {string[]} lines - Code lines\n * @returns {string} Comment block or empty string\n */\nfunction extractCommentBlock(lines) {\n  let inComment = false;\n  let commentLines = [];\n\n  for (let i = 0; i < Math.min(20, lines.length); i++) {\n    const line = lines[i].trim();\n\n    // Check for JSDoc style comment start\n    if (line.startsWith(\"/**\")) {\n      inComment = true;\n      commentLines.push(line);\n      continue;\n    }\n\n    // Continue collecting comment lines\n    if (inComment) {\n      commentLines.push(line);\n      if (line.endsWith(\"*/\")) {\n        break;\n      }\n    }\n\n    // Check for single-line comments at the beginning\n    if (!inComment && commentLines.length === 0 && line.startsWith(\"//\")) {\n      commentLines.push(line);\n    } else if (!inComment && commentLines.length > 0 && line.startsWith(\"//\")) {\n      commentLines.push(line);\n    } else if (!inComment && commentLines.length > 0) {\n      // Stop if we've collected some comments and hit a non-comment line\n      break;\n    }\n  }\n\n  return commentLines.join(\"\\n\");\n}\n\n/**\n * Score code lines based on importance for summarization\n *\n * @param {string[]} lines - Code lines\n * @param {string[]} queryKeywords - Keywords to prioritize\n * @param {string} entityType - Type of entity\n * @returns {Array<{line: string, score: number, index: number}>} Scored lines\n */\nfunction scoreCodeLines(lines, queryKeywords, entityType) {\n  const scoredLines = [];\n\n  // Important patterns to look for in code\n  const importantPatterns = {\n    function: [\n      /\\breturn\\s+/, // Return statements\n      /\\bthrow\\s+/, // Error handling\n      /\\bif\\s*\\(/, // Conditionals\n      /\\bfor\\s*\\(/, // Loops\n      /\\bcatch\\s*\\(/, // Error catching\n      /\\bswitch\\s*\\(/, // Switch statements\n      /\\bconst\\s+\\w+\\s*=/, // Important variable declarations\n      /\\blet\\s+\\w+\\s*=/, // Variable declarations\n      /\\/\\/ [A-Z]/, // Comments that start with capital letters (likely important)\n    ],\n    class: [\n      /\\bconstructor\\s*\\(/, // Constructor\n      /\\bstatic\\s+/, // Static methods/properties\n      /\\bget\\s+\\w+\\s*\\(/, // Getters\n      /\\bset\\s+\\w+\\s*\\(/, // Setters\n      /\\bextends\\s+/, // Inheritance\n      /\\bimplements\\s+/, // Interface implementation\n      /\\breturn\\s+/, // Return statements\n    ],\n    file: [\n      /\\bexport\\s+(default\\s+)?function\\s+/, // Exported functions\n      /\\bexport\\s+(default\\s+)?class\\s+/, // Exported classes\n      /\\bexport\\s+(default\\s+)?const\\s+/, // Exported constants\n      /\\bmodule\\.exports\\s*=/, // CommonJS exports\n      /\\bimport\\s+/, // Imports\n      /\\brequire\\s*\\(/, // Requires\n    ],\n  };\n\n  // Common patterns across all entity types\n  const commonPatterns = [\n    /\\/\\/ TODO:/, // TODOs\n    /\\/\\/ FIXME:/, // FIXMEs\n    /\\/\\/ NOTE:/, // Notes\n    /\\/\\*\\*/, // JSDoc comments\n  ];\n\n  // Get patterns for this entity type\n  const patterns = [\n    ...(importantPatterns[entityType] || []),\n    ...commonPatterns,\n  ];\n\n  for (let i = 0; i < lines.length; i++) {\n    const line = lines[i].trim();\n    if (!line) continue; // Skip empty lines\n\n    let score = 0;\n\n    // Check for keyword matches\n    if (queryKeywords.length > 0) {\n      const tokens = tokenize(line, { includeIdentifiers: true });\n      const keywordMatches = queryKeywords.filter(\n        (keyword) =>\n          tokens.includes(keyword.toLowerCase()) ||\n          line.toLowerCase().includes(keyword.toLowerCase())\n      );\n\n      score += keywordMatches.length * 3; // High weight for query keywords\n    }\n\n    // Check for important patterns\n    for (const pattern of patterns) {\n      if (pattern.test(line)) {\n        score += 2;\n        break;\n      }\n    }\n\n    // Special score for first 5 and last 5 non-empty lines\n    if (i < 5) {\n      score += 1;\n    }\n\n    // Add a small score for lines with brackets (opening/closing) - structure indicators\n    if (line.includes(\"{\") || line.includes(\"}\")) {\n      score += 0.5;\n    }\n\n    // Push to scored lines\n    scoredLines.push({\n      line,\n      score,\n      index: i,\n    });\n  }\n\n  // Sort by score (highest first)\n  return scoredLines.sort((a, b) => b.score - a.score);\n}\n\n/**\n * Select important lines from scored lines, respecting the budget\n *\n * @param {Array<{line: string, score: number, index: number}>} scoredLines - Lines with scores\n * @param {number} budget - Character budget\n * @returns {string} Selected important lines\n */\nfunction selectImportantLines(scoredLines, budget) {\n  const selectedLines = [];\n  let usedBudget = 0;\n\n  // First, include all lines with a score above 3 (very important)\n  const highScoreLines = scoredLines.filter((item) => item.score >= 3);\n\n  for (const item of highScoreLines) {\n    if (usedBudget + item.line.length + 1 <= budget) {\n      // +1 for newline\n      selectedLines.push(item);\n      usedBudget += item.line.length + 1;\n    }\n  }\n\n  // Then add other lines if we have budget left\n  if (usedBudget < budget) {\n    const remainingLines = scoredLines\n      .filter((item) => item.score < 3)\n      .sort((a, b) => b.score - a.score); // Sort by score\n\n    for (const item of remainingLines) {\n      if (usedBudget + item.line.length + 1 <= budget) {\n        selectedLines.push(item);\n        usedBudget += item.line.length + 1;\n      }\n    }\n  }\n\n  // Sort by original index to maintain code order\n  selectedLines.sort((a, b) => a.index - b.index);\n\n  return selectedLines.map((item) => item.line).join(\"\\n\");\n}\n\n/**\n * Apply rule-based extractive summarization\n *\n * @param {string} text - The text to summarize\n * @param {number} maxLength - Maximum character length for the summary\n * @returns {string} Summarized text\n */\nfunction ruleBased(text, maxLength) {\n  // Split text into sentences\n  const sentences = splitIntoSentences(text);\n\n  // If we have very few sentences, handle specially\n  if (sentences.length <= 3) {\n    // For 1-3 sentences, return as much as fits within maxLength\n    return truncateToMaxLength(text, maxLength);\n  }\n\n  // Score sentences\n  const scoredSentences = sentences.map((sentence, index) => ({\n    text: sentence,\n    score: scoreSentence(sentence, index, sentences.length),\n    index,\n  }));\n\n  // Sort sentences by score (highest first)\n  scoredSentences.sort((a, b) => b.score - a.score);\n\n  // Select sentences to include in summary\n  const selectedSentences = [];\n  let currentLength = 0;\n\n  for (const scored of scoredSentences) {\n    // Check if adding this sentence would exceed maxLength\n    if (currentLength + scored.text.length + 1 <= maxLength) {\n      // +1 for space\n      selectedSentences.push(scored);\n      currentLength += scored.text.length + 1;\n    } else {\n      // If we can't add even the highest-scoring sentence, we need to truncate\n      if (selectedSentences.length === 0) {\n        return truncateToMaxLength(scored.text, maxLength);\n      }\n      // Otherwise, we've selected as many as we can\n      break;\n    }\n  }\n\n  // Sort selected sentences by original position to maintain coherence\n  selectedSentences.sort((a, b) => a.index - b.index);\n\n  // Join selected sentences\n  const summary = selectedSentences.map((s) => s.text).join(\" \");\n\n  // Final check to ensure we're within maxLength\n  return truncateToMaxLength(summary, maxLength);\n}\n\n/**\n * Split text into sentences using regex\n *\n * @param {string} text - Text to split into sentences\n * @returns {string[]} Array of sentences\n */\nfunction splitIntoSentences(text) {\n  // This regex handles common sentence endings (., !, ?)\n  // It tries to handle abbreviations, decimal numbers, etc.\n  const sentenceRegex = /[^.!?]*[.!?](?:\\s|$)/g;\n  const matches = text.match(sentenceRegex);\n\n  if (!matches) {\n    // If no matches (perhaps text doesn't end with punctuation),\n    // return the whole text as one sentence\n    return [text];\n  }\n\n  // Clean up sentences (trim whitespace)\n  return matches.map((s) => s.trim()).filter((s) => s.length > 0);\n}\n\n/**\n * Score a sentence based on heuristics\n *\n * @param {string} sentence - The sentence to score\n * @param {number} index - Index of sentence in original text\n * @param {number} totalSentences - Total number of sentences in text\n * @returns {number} Score for the sentence (higher is more important)\n */\nfunction scoreSentence(sentence, index, totalSentences) {\n  let score = 0;\n\n  // 1. Position score - first and last sentences are often important\n  if (index === 0) {\n    score += 3; // First sentence bonus\n  } else if (index === totalSentences - 1) {\n    score += 2; // Last sentence bonus\n  } else if (index === 1 || index === totalSentences - 2) {\n    score += 1; // Second and second-to-last sentence small bonus\n  }\n\n  // 2. Length score - penalize very short or very long sentences\n  const wordCount = sentence.split(/\\s+/).length;\n  if (wordCount >= 5 && wordCount <= 20) {\n    score += 1; // Ideal length\n  } else if (wordCount < 3 || wordCount > 30) {\n    score -= 1; // Too short or too long\n  }\n\n  // 3. Content score - check for indicators of important content\n  const importantPhrases = [\n    \"key\",\n    \"important\",\n    \"significant\",\n    \"critical\",\n    \"essential\",\n    \"main\",\n    \"primary\",\n    \"crucial\",\n    \"fundamental\",\n    \"vital\",\n    \"result\",\n    \"conclude\",\n    \"summary\",\n    \"therefore\",\n    \"thus\",\n    \"implement\",\n    \"function\",\n    \"method\",\n    \"class\",\n    \"object\",\n    \"return\",\n    \"export\",\n    \"import\",\n    \"require\",\n    \"define\",\n  ];\n\n  const lowerSentence = sentence.toLowerCase();\n\n  for (const phrase of importantPhrases) {\n    if (lowerSentence.includes(phrase)) {\n      score += 1;\n      break; // Only count once for important phrases\n    }\n  }\n\n  // 4. Code indication score - sentences with code patterns are often important\n  if (\n    lowerSentence.includes(\"function\") ||\n    lowerSentence.includes(\"class\") ||\n    lowerSentence.includes(\"=\") ||\n    lowerSentence.includes(\"return\") ||\n    sentence.includes(\"()\") ||\n    sentence.includes(\"{}\") ||\n    sentence.includes(\"[]\")\n  ) {\n    score += 2; // Code-related sentences are important in programming context\n  }\n\n  return score;\n}\n\n/**\n * Truncate text to ensure it doesn't exceed maxLength\n *\n * @param {string} text - Text to truncate\n * @param {number} maxLength - Maximum character length\n * @returns {string} Truncated text\n */\nfunction truncateToMaxLength(text, maxLength) {\n  if (text.length <= maxLength) {\n    return text;\n  }\n\n  // Try to cut at a sentence boundary\n  for (let i = maxLength - 1; i >= 0; i--) {\n    if (text[i] === \".\" || text[i] === \"!\" || text[i] === \"?\") {\n      return text.substring(0, i + 1);\n    }\n  }\n\n  // If no sentence boundary found, cut at a word boundary\n  for (let i = maxLength - 1; i >= 0; i--) {\n    if (text[i] === \" \") {\n      return text.substring(0, i) + \"...\";\n    }\n  }\n\n  // If all else fails, just truncate\n  return text.substring(0, maxLength - 3) + \"...\";\n}\n\n/**\n * Compresses a collection of context items to fit within token budget\n *\n * @param {Array<Object>} contextItems - Array of context items to compress\n * @param {Object} options - Compression options\n * @param {string} [options.detailLevel='medium'] - Detail level: 'high', 'medium', or 'low'\n * @param {number} [options.targetTokens=2000] - Target token count\n * @param {string[]} [options.queryKeywords=[]] - Optional query keywords for summarization\n * @returns {Promise<Array<Object>>} Compressed context items\n */\nexport async function compressContext(contextItems, options = {}) {\n  if (!contextItems || contextItems.length === 0) {\n    return [];\n  }\n\n  const detailLevel = options.detailLevel || \"medium\";\n  const targetTokens = options.targetTokens || 2000;\n  const queryKeywords = options.queryKeywords || [];\n\n  // Estimate average tokens per character for budget calculation\n  // This is a rough approximation (average English word is ~5 chars + 1 for space)\n  const tokensPerChar = 1 / 6;\n\n  // Convert token budget to character budget\n  const charBudget = Math.floor(targetTokens / tokensPerChar);\n\n  // Map context items to the format expected by manageTokenBudget\n  const scoredSnippets = contextItems.map((item) => ({\n    entity: {\n      entity_id: item.entity_id,\n      entity_type: item.type,\n      raw_content: item.content,\n      name: item.name,\n      file_path: item.path,\n    },\n    score: item.relevanceScore || 0.5,\n    content: item.content,\n  }));\n\n  // Apply detail level modifiers to budget\n  let modifiedBudget = charBudget;\n  switch (detailLevel) {\n    case \"high\":\n      // Increase budget by 30% for high detail\n      modifiedBudget = Math.floor(charBudget * 1.3);\n      break;\n    case \"low\":\n      // Decrease budget by 30% for low detail\n      modifiedBudget = Math.floor(charBudget * 0.7);\n      break;\n    default:\n      // Keep original budget for medium detail\n      break;\n  }\n\n  // Process snippets using manageTokenBudget\n  const processedSnippets = manageTokenBudget(\n    scoredSnippets,\n    modifiedBudget,\n    queryKeywords\n  );\n\n  // Transform back to the original format\n  return processedSnippets\n    .map((processed) => {\n      // Find the original item to copy properties from\n      const originalItem = contextItems.find(\n        (item) => item.entity_id === processed.entity_id\n      );\n      if (!originalItem) return null;\n\n      return {\n        ...originalItem,\n        content: processed.summarizedContent,\n        // Add compression metadata\n        compression: {\n          originalLength: originalItem.content.length,\n          compressedLength: processed.summarizedContent.length,\n          compressionRatio:\n            processed.summarizedContent.length / originalItem.content.length,\n          detailLevel,\n        },\n      };\n    })\n    .filter(Boolean); // Remove any nulls\n}\n", "/**\n * ConversationPurposeDetector.js\n *\n * Provides functionality to detect the purpose or intent of a conversation\n * by analyzing message content and patterns.\n */\n\nimport * as TextTokenizerLogic from \"./TextTokenizerLogic.js\";\nimport { executeQuery } from \"../db.js\";\nimport { v4 as uuidv4 } from \"uuid\";\n\n/**\n * @typedef {Object} Message\n * @property {string} content - The content of the message\n * @property {string} role - The role of the sender (e.g., 'user', 'assistant')\n * @property {Date} [timestamp] - Optional timestamp of the message\n * @property {string[]} [entity_ids] - Optional array of referenced entity IDs\n */\n\n/**\n * Purpose types with associated keywords and patterns\n */\nconst PURPOSE_TYPES = {\n  debugging: {\n    keywords: [\n      \"error\",\n      \"stacktrace\",\n      \"bug\",\n      \"fix\",\n      \"not working\",\n      \"exception\",\n      \"issue\",\n      \"failed\",\n      \"failing\",\n      \"crash\",\n      \"debug\",\n      \"broken\",\n      \"incorrect\",\n      \"problem\",\n      \"trouble\",\n      \"unexpected\",\n      \"diagnose\",\n      \"investigate\",\n    ],\n    patterns: [\n      /TypeError:/i,\n      /Error:/i,\n      /Exception:/i,\n      /failed with/i,\n      /doesn't work/i,\n      /not working/i,\n      /unexpected behavior/i,\n    ],\n    weight: 1.0,\n  },\n\n  feature_planning: {\n    keywords: [\n      \"requirement\",\n      \"design\",\n      \"new feature\",\n      \"implement\",\n      \"proposal\",\n      \"roadmap\",\n      \"spec\",\n      \"specification\",\n      \"plan\",\n      \"architecture\",\n      \"blueprint\",\n      \"feature\",\n      \"enhancement\",\n      \"improvement\",\n      \"add\",\n      \"create\",\n      \"develop\",\n      \"extend\",\n    ],\n    patterns: [\n      /could we add/i,\n      /we need to implement/i,\n      /design for/i,\n      /planning to/i,\n      /we should build/i,\n      /requirement is to/i,\n    ],\n    weight: 0.9,\n  },\n\n  code_review: {\n    keywords: [\n      \"PR\",\n      \"pull request\",\n      \"LGTM\",\n      \"suggestion\",\n      \"change request\",\n      \"review\",\n      \"approve\",\n      \"feedback\",\n      \"comment\",\n      \"revision\",\n      \"looks good\",\n      \"merge\",\n      \"style\",\n      \"convention\",\n      \"readability\",\n      \"clarity\",\n    ],\n    patterns: [\n      /pull request #\\d+/i,\n      /PR #\\d+/i,\n      /please review/i,\n      /looks good to me/i,\n      /suggested changes/i,\n      /can you review/i,\n    ],\n    weight: 0.85,\n  },\n\n  learning: {\n    keywords: [\n      \"learn\",\n      \"understand\",\n      \"explanation\",\n      \"tutorial\",\n      \"example\",\n      \"how does\",\n      \"what is\",\n      \"meaning\",\n      \"concept\",\n      \"definition\",\n      \"help me understand\",\n      \"documentation\",\n      \"guide\",\n      \"explain\",\n      \"clarify\",\n      \"teach\",\n    ],\n    patterns: [\n      /how does (it|this) work/i,\n      /what (is|does|are)/i,\n      /could you explain/i,\n      /I'm trying to understand/i,\n      /explain how/i,\n    ],\n    weight: 0.8,\n  },\n\n  code_generation: {\n    keywords: [\n      \"generate\",\n      \"create\",\n      \"build\",\n      \"write\",\n      \"implement\",\n      \"code for\",\n      \"function\",\n      \"class\",\n      \"method\",\n      \"module\",\n      \"script\",\n      \"algorithm\",\n      \"solution\",\n    ],\n    patterns: [\n      /can you (write|create|generate)/i,\n      /implement a/i,\n      /create a function/i,\n      /generate code for/i,\n      /need code to/i,\n    ],\n    weight: 0.9,\n  },\n\n  optimization: {\n    keywords: [\n      \"optimize\",\n      \"performance\",\n      \"efficiency\",\n      \"slow\",\n      \"faster\",\n      \"speed up\",\n      \"reduce\",\n      \"improve\",\n      \"bottleneck\",\n      \"memory\",\n      \"CPU\",\n      \"utilization\",\n      \"profiling\",\n      \"benchmark\",\n    ],\n    patterns: [\n      /too slow/i,\n      /needs to be faster/i,\n      /performance issue/i,\n      /optimize for/i,\n      /reduce (memory|time|usage)/i,\n    ],\n    weight: 0.85,\n  },\n\n  refactoring: {\n    keywords: [\n      \"refactor\",\n      \"restructure\",\n      \"rewrite\",\n      \"reorganize\",\n      \"clean up\",\n      \"improve\",\n      \"modernize\",\n      \"update\",\n      \"simplify\",\n      \"decouple\",\n      \"modularity\",\n      \"readability\",\n    ],\n    patterns: [\n      /need to refactor/i,\n      /code smells/i,\n      /technical debt/i,\n      /simplify the code/i,\n      /make it more maintainable/i,\n    ],\n    weight: 0.8,\n  },\n\n  general_query: {\n    keywords: [\n      \"question\",\n      \"ask\",\n      \"wondering\",\n      \"curious\",\n      \"thoughts\",\n      \"opinion\",\n      \"advice\",\n      \"suggestion\",\n      \"recommend\",\n      \"help\",\n      \"guidance\",\n    ],\n    patterns: [\n      /I have a question/i,\n      /can you help/i,\n      /what do you think/i,\n      /do you have any advice/i,\n    ],\n    weight: 0.7, // Lower weight as this is the default fallback\n  },\n};\n\n/**\n * Detects the primary purpose of a conversation by analyzing message content\n *\n * @param {Message[]} messages - Array of conversation messages to analyze\n * @returns {Promise<{purposeType: string, confidence: number}>} The detected purpose and confidence score\n */\nexport async function detectConversationPurpose(messages) {\n  try {\n    if (!messages || messages.length === 0) {\n      return { purposeType: \"general_query\", confidence: 0.5 };\n    }\n\n    // 1. Concatenate the content of messages, giving priority to user messages\n    let concatenatedContent = \"\";\n    const userMessages = messages.filter((msg) => msg.role === \"user\");\n\n    if (userMessages.length > 0) {\n      // If we have user messages, prioritize those\n      concatenatedContent = userMessages.map((msg) => msg.content).join(\" \");\n    } else {\n      // Otherwise use all messages\n      concatenatedContent = messages.map((msg) => msg.content).join(\" \");\n    }\n\n    // 2. Tokenize and extract keywords\n    const tokens = TextTokenizerLogic.tokenize(concatenatedContent);\n    const extractedKeywords = TextTokenizerLogic.extractKeywords(tokens, 20);\n\n    // 3. Score each purpose type\n    const purposeScores = {};\n\n    for (const [purposeType, purposeData] of Object.entries(PURPOSE_TYPES)) {\n      let score = 0;\n\n      // Score based on keyword matches\n      for (const keyword of purposeData.keywords) {\n        if (concatenatedContent.toLowerCase().includes(keyword.toLowerCase())) {\n          score += 1;\n        }\n\n        // Check for keyword in extracted keywords (stronger signal)\n        if (\n          extractedKeywords.some(\n            (k) =>\n              typeof k === \"string\" && k.toLowerCase() === keyword.toLowerCase()\n          )\n        ) {\n          score += 2;\n        }\n      }\n\n      // Score based on pattern matches\n      for (const pattern of purposeData.patterns) {\n        if (pattern.test(concatenatedContent)) {\n          score += 3; // Patterns are stronger signals\n        }\n      }\n\n      // Apply purpose-specific weight\n      score *= purposeData.weight;\n\n      // Store the score\n      purposeScores[purposeType] = score;\n    }\n\n    // 4. Find the purpose with the highest score\n    let highestScore = 0;\n    let detectedPurpose = \"general_query\"; // Default\n\n    for (const [purposeType, score] of Object.entries(purposeScores)) {\n      if (score > highestScore) {\n        highestScore = score;\n        detectedPurpose = purposeType;\n      }\n    }\n\n    // 5. Calculate confidence (normalize score)\n    // Find max possible score for the detected purpose type\n    const maxPossibleScore =\n      PURPOSE_TYPES[detectedPurpose].keywords.length * 3 + // Max keyword match score\n      PURPOSE_TYPES[detectedPurpose].patterns.length * 3; // Max pattern match score\n\n    // Normalize the confidence between 0 and 1\n    // Add a base confidence of 0.3 so it's never too low\n    let confidence =\n      0.3 +\n      0.7 *\n        (highestScore /\n          (maxPossibleScore * PURPOSE_TYPES[detectedPurpose].weight));\n\n    // Cap confidence at 1.0\n    confidence = Math.min(confidence, 1.0);\n\n    // If the highest score is very low, default to general_query with moderate confidence\n    if (highestScore < 3 && detectedPurpose !== \"general_query\") {\n      return { purposeType: \"general_query\", confidence: 0.6 };\n    }\n\n    return { purposeType: detectedPurpose, confidence };\n  } catch (error) {\n    console.error(\"Error detecting conversation purpose:\", error);\n    // Fallback to general query with low confidence\n    return { purposeType: \"general_query\", confidence: 0.5 };\n  }\n}\n\n/**\n * @typedef {Object} ConversationPurpose\n * @property {string} purpose_id - Unique identifier for the purpose record\n * @property {string} conversation_id - ID of the conversation\n * @property {string} purpose_type - Type of purpose (e.g., 'debugging', 'feature_planning')\n * @property {number} confidence - Confidence score for this purpose (0-1)\n * @property {string} start_timestamp - ISO timestamp when this purpose was detected\n * @property {string|null} end_timestamp - ISO timestamp when this purpose ended, or null if active\n */\n\n/**\n * Gets the currently active purpose for a conversation\n *\n * @param {string} conversationId - ID of the conversation\n * @returns {Promise<ConversationPurpose|null>} The active purpose or null if none\n */\nexport async function getActivePurpose(conversationId) {\n  try {\n    // Query for active purpose (where end_timestamp is NULL)\n    const query = `\n      SELECT * FROM conversation_purposes\n      WHERE conversation_id = ?\n        AND end_timestamp IS NULL\n      ORDER BY start_timestamp DESC\n      LIMIT 1\n    `;\n\n    const result = await executeQuery(query, [conversationId]);\n\n    // Check if result has a rows property and it's an array\n    const rows =\n      result && result.rows && Array.isArray(result.rows)\n        ? result.rows\n        : Array.isArray(result)\n        ? result\n        : [];\n\n    // If no valid results, return null\n    if (rows.length === 0) {\n      return null;\n    }\n\n    // Return the active purpose (the most recent one if multiple exist)\n    return rows[0];\n  } catch (error) {\n    console.error(\n      `Error getting active purpose for conversation ${conversationId}:`,\n      error\n    );\n    throw new Error(`Failed to get active purpose: ${error.message}`);\n  }\n}\n\n/**\n * Records a transition to a new conversation purpose\n *\n * @param {string} conversationId - ID of the conversation\n * @param {string} newPurposeType - The new detected purpose type\n * @param {number} confidence - Confidence score for this purpose detection (0-1)\n * @returns {Promise<string>} The ID of the newly created purpose record\n */\nexport async function trackPurposeTransition(\n  conversationId,\n  newPurposeType,\n  confidence\n) {\n  try {\n    // 1. Get current active purpose for the conversation\n    const activePurpose = await getActivePurpose(conversationId);\n\n    // 2. If there's an active purpose and it's different from the new one, close it\n    if (activePurpose && activePurpose.purpose_type !== newPurposeType) {\n      const currentTime = new Date().toISOString();\n\n      const updateQuery = `\n        UPDATE conversation_purposes\n        SET end_timestamp = ?\n        WHERE purpose_id = ?\n      `;\n\n      await executeQuery(updateQuery, [currentTime, activePurpose.purpose_id]);\n\n      console.log(\n        `Closed purpose ${activePurpose.purpose_type} for conversation ${conversationId}`\n      );\n    } else if (activePurpose && activePurpose.purpose_type === newPurposeType) {\n      // If the same purpose is already active, just return its ID\n      return activePurpose.purpose_id;\n    }\n\n    // 3. Generate a new purpose_id\n    const purpose_id = uuidv4();\n\n    // 4. Get current timestamp for start_timestamp\n    const start_timestamp = new Date().toISOString();\n\n    // 5. Insert the new purpose record\n    const insertQuery = `\n      INSERT INTO conversation_purposes (\n        purpose_id,\n        conversation_id,\n        purpose_type,\n        confidence,\n        start_timestamp\n      ) VALUES (?, ?, ?, ?, ?)\n    `;\n\n    const params = [\n      purpose_id,\n      conversationId,\n      newPurposeType,\n      confidence,\n      start_timestamp,\n    ];\n\n    await executeQuery(insertQuery, params);\n\n    console.log(\n      `Created new purpose record: ${newPurposeType} (${purpose_id}) for conversation ${conversationId}`\n    );\n\n    // 6. Return the purpose_id\n    return purpose_id;\n  } catch (error) {\n    console.error(\n      `Error tracking purpose transition for conversation ${conversationId}:`,\n      error\n    );\n    throw new Error(`Failed to track purpose transition: ${error.message}`);\n  }\n}\n\n/**\n * Gets the full history of purposes for a conversation\n *\n * @param {string} conversationId - ID of the conversation\n * @returns {Promise<ConversationPurpose[]>} Array of purpose records in chronological order\n */\nexport async function getPurposeHistory(conversationId) {\n  try {\n    // Query for all purposes for this conversation, ordered chronologically\n    const query = `\n      SELECT * FROM conversation_purposes\n      WHERE conversation_id = ?\n      ORDER BY start_timestamp ASC\n    `;\n\n    const result = await executeQuery(query, [conversationId]);\n\n    // Return the array of purpose objects (empty array if none found)\n    return result || [];\n  } catch (error) {\n    console.error(\n      `Error getting purpose history for conversation ${conversationId}:`,\n      error\n    );\n    throw new Error(`Failed to get purpose history: ${error.message}`);\n  }\n}\n\n/**\n * Purpose-specific prompt configurations\n */\nconst PURPOSE_PROMPTS = {\n  debugging: {\n    systemPrompt:\n      \"You are a debugging assistant. Focus on identifying errors and suggesting fixes for code issues. Analyze stack traces, error messages, and code snippets to help resolve problems efficiently.\",\n    modelBehavior:\n      \"Ask clarifying questions about error messages and code snippets. Provide step-by-step solutions. Be methodical in your approach. Look for common error patterns and suggest targeted debugging techniques.\",\n  },\n\n  feature_planning: {\n    systemPrompt:\n      \"You are a feature planning assistant. Help outline requirements, define scope, and create implementation tasks for new features. Consider architecture implications and integration points.\",\n    modelBehavior:\n      \"Break down complex features into manageable components. Discuss pros and cons of different design choices. Ask about constraints, requirements, and priorities. Suggest testing strategies and potential edge cases to consider.\",\n  },\n\n  code_review: {\n    systemPrompt:\n      \"You are a code review assistant. Help identify issues, suggest improvements, and ensure code quality. Focus on readability, performance, security, and maintainability.\",\n    modelBehavior:\n      \"Examine code thoroughly for bugs, edge cases, and potential improvements. Suggest more elegant or efficient approaches when appropriate. Reference best practices and design patterns. Be constructive and specific in feedback.\",\n  },\n\n  learning: {\n    systemPrompt:\n      \"You are a programming tutor. Focus on explaining concepts, providing clear examples, and building understanding. Adapt explanations to different knowledge levels.\",\n    modelBehavior:\n      \"Provide concise but thorough explanations. Use analogies and examples to illustrate complex concepts. Check understanding with questions. Encourage experimentation and hands-on learning. Break down complex topics into smaller chunks.\",\n  },\n\n  code_generation: {\n    systemPrompt:\n      \"You are a code generation assistant. Create high-quality, functional code that meets requirements while following best practices and project conventions.\",\n    modelBehavior:\n      \"Ask clarifying questions about requirements. Generate well-structured, well-documented, and tested code. Explain key design decisions. Suggest alternative implementations when appropriate. Follow idiomatic patterns for the language.\",\n  },\n\n  optimization: {\n    systemPrompt:\n      \"You are a performance optimization assistant. Help identify bottlenecks and suggest improvements to make code more efficient in terms of speed, memory usage, and resource utilization.\",\n    modelBehavior:\n      \"Ask for profiling data when available. Suggest specific optimization techniques. Explain tradeoffs between different approaches. Focus on high-impact changes first. Recommend measurement techniques to verify improvements.\",\n  },\n\n  refactoring: {\n    systemPrompt:\n      \"You are a code refactoring assistant. Help improve code structure, readability, and maintainability without changing functionality. Suggest cleaner, more modular designs.\",\n    modelBehavior:\n      \"Analyze code for code smells and improvement opportunities. Suggest refactoring in small, testable steps. Explain the benefits of each change. Follow established design principles and patterns. Consider readability and future maintenance.\",\n  },\n\n  general_query: {\n    systemPrompt:\n      \"You are a helpful programming assistant. Provide accurate and relevant information to help with coding tasks, questions, and challenges.\",\n    modelBehavior:\n      \"Answer questions clearly and concisely. Provide context-relevant code examples when appropriate. Ask clarifying questions if the request is ambiguous. Be factual and admit when you don't know something.\",\n  },\n};\n\n/**\n * Returns optimized prompts based on the detected conversation purpose\n *\n * @param {string} purposeType - The type of conversation purpose\n * @returns {{systemPrompt: string, modelBehavior: string}} Object containing prompts optimized for the purpose\n */\nexport function getPurposeSpecificPrompts(purposeType) {\n  // Look up the purpose type in our predefined prompts\n  if (PURPOSE_PROMPTS[purposeType]) {\n    return PURPOSE_PROMPTS[purposeType];\n  }\n\n  // Default to general_query for unknown purpose types\n  return PURPOSE_PROMPTS.general_query;\n}\n\n/**\n * Purpose-specific actionable request patterns\n */\nconst PURPOSE_ACTION_PATTERNS = {\n  debugging: {\n    keywords: [\n      \"fix\",\n      \"debug\",\n      \"solve\",\n      \"troubleshoot\",\n      \"resolve\",\n      \"diagnose\",\n      \"analyze\",\n      \"investigate\",\n      \"find the bug\",\n      \"identify the issue\",\n      \"fix this error\",\n      \"help me understand\",\n      \"what's wrong\",\n      \"why is this failing\",\n    ],\n    patterns: [\n      /how (do|can|should) I fix/i,\n      /what('s| is) causing/i,\n      /why (am I|is it) getting/i,\n      /can you (help me|assist|fix|debug|solve)/i,\n      /suggest a (fix|solution)/i,\n    ],\n  },\n\n  feature_planning: {\n    keywords: [\n      \"design\",\n      \"plan\",\n      \"create\",\n      \"implement\",\n      \"develop\",\n      \"architect\",\n      \"draft\",\n      \"outline\",\n      \"structure\",\n      \"requirements\",\n      \"specification\",\n      \"roadmap\",\n    ],\n    patterns: [\n      /how (do|can|should) I (design|implement|structure)/i,\n      /what('s| is) the best way to (design|implement)/i,\n      /help me (plan|design|create|outline)/i,\n      /can you (draft|create|help with)/i,\n      /suggest an? (architecture|approach|design)/i,\n    ],\n  },\n\n  code_review: {\n    keywords: [\n      \"review\",\n      \"evaluate\",\n      \"assess\",\n      \"improve\",\n      \"feedback\",\n      \"suggestion\",\n      \"better way\",\n      \"optimize\",\n      \"refactor\",\n      \"check\",\n    ],\n    patterns: [\n      /can you (review|look at|check)/i,\n      /what do you think (of|about)/i,\n      /how (can|could|would) (I|this|we) improve/i,\n      /is there a (better|cleaner|more efficient) way/i,\n      /please (review|evaluate|assess)/i,\n    ],\n  },\n\n  code_generation: {\n    keywords: [\n      \"generate\",\n      \"write\",\n      \"create\",\n      \"implement\",\n      \"code\",\n      \"script\",\n      \"function\",\n      \"class\",\n      \"method\",\n      \"program\",\n      \"example\",\n      \"show me how\",\n    ],\n    patterns: [\n      /can you (write|create|generate|implement|show me)/i,\n      /how (do|would) I (write|create|implement)/i,\n      /write a (function|class|method|program)/i,\n      /generate (code|a script|an example)/i,\n      /implement a (solution|feature|function)/i,\n    ],\n  },\n\n  learning: {\n    keywords: [\n      \"explain\",\n      \"teach\",\n      \"help me understand\",\n      \"clarify\",\n      \"what is\",\n      \"how does\",\n      \"why\",\n      \"concept\",\n      \"tutorial\",\n      \"example\",\n      \"guidance\",\n    ],\n    patterns: [\n      /can you (explain|clarify|teach me)/i,\n      /what (is|are|does)/i,\n      /how does (it|this|that) work/i,\n      /why (is|does|do)/i,\n      /I don't understand/i,\n      /help me understand/i,\n    ],\n  },\n\n  optimization: {\n    keywords: [\n      \"optimize\",\n      \"improve\",\n      \"speed up\",\n      \"performance\",\n      \"efficiency\",\n      \"faster\",\n      \"memory\",\n      \"resource\",\n      \"bottleneck\",\n      \"profile\",\n      \"benchmark\",\n    ],\n    patterns: [\n      /how (can|do) I (optimize|improve|speed up)/i,\n      /can you help (optimize|improve)/i,\n      /make (it|this) (faster|more efficient)/i,\n      /reduce (memory|CPU|resource) usage/i,\n      /find the (bottleneck|performance issue)/i,\n    ],\n  },\n\n  refactoring: {\n    keywords: [\n      \"refactor\",\n      \"restructure\",\n      \"reorganize\",\n      \"clean up\",\n      \"improve readability\",\n      \"simplify\",\n      \"modernize\",\n      \"update\",\n      \"better structure\",\n      \"clean code\",\n    ],\n    patterns: [\n      /how (can|do) I (refactor|restructure|improve)/i,\n      /can you (help|assist with) refactoring/i,\n      /make (it|this) (cleaner|more readable|more maintainable)/i,\n      /improve (the structure|readability|maintainability)/i,\n      /simplify this (code|implementation|approach)/i,\n    ],\n  },\n\n  general_query: {\n    keywords: [\n      \"how to\",\n      \"can you\",\n      \"please\",\n      \"show me\",\n      \"find\",\n      \"search\",\n      \"where is\",\n      \"display\",\n      \"tell me\",\n    ],\n    patterns: [\n      /can you (help|show|find|tell)/i,\n      /how (do|can|would) I/i,\n      /please (show|find|tell|help)/i,\n      /I need to/i,\n      /where (can|do) I/i,\n    ],\n  },\n};\n\n// General actionable request patterns that apply across all purpose types\nconst GENERAL_ACTION_PATTERNS = {\n  keywords: [\n    \"create\",\n    \"generate\",\n    \"build\",\n    \"write\",\n    \"implement\",\n    \"show\",\n    \"display\",\n    \"list\",\n    \"find\",\n    \"search\",\n    \"analyze\",\n    \"compare\",\n    \"calculate\",\n    \"run\",\n    \"execute\",\n    \"update\",\n    \"modify\",\n    \"change\",\n    \"add\",\n    \"remove\",\n    \"delete\",\n  ],\n  patterns: [\n    /can you/i,\n    /please/i,\n    /I need/i,\n    /could you/i,\n    /would you/i,\n    /show me/i,\n    /help me/i,\n    /let's/i,\n    /how (do|can|should) I/i,\n    /what (is|are) the/i,\n  ],\n  // Common question structures that often indicate actionable requests\n  questionPatterns: [\n    /\\?$/, // Ends with question mark\n    /^(what|how|where|when|who|why|can|could|would|should|is|are|do|does)/i, // Starts with question word\n  ],\n};\n\n/**\n * Determines if a message contains an actionable request based on its content and purpose\n *\n * @param {string} messageContent - The content of the message to analyze\n * @param {string} purposeType - The type of conversation purpose\n * @returns {boolean} True if the message contains an actionable request, false otherwise\n */\nexport function isActionableRequest(messageContent, purposeType) {\n  if (!messageContent || typeof messageContent !== \"string\") {\n    return false;\n  }\n\n  // Normalize message content\n  const content = messageContent.toLowerCase().trim();\n\n  // Very short messages are less likely to be actionable\n  if (content.length < 5) {\n    return false;\n  }\n\n  // Get purpose-specific patterns\n  const purposePatterns =\n    PURPOSE_ACTION_PATTERNS[purposeType] ||\n    PURPOSE_ACTION_PATTERNS.general_query;\n\n  // Check against purpose-specific keywords\n  for (const keyword of purposePatterns.keywords) {\n    if (content.includes(keyword.toLowerCase())) {\n      return true;\n    }\n  }\n\n  // Check against purpose-specific patterns\n  for (const pattern of purposePatterns.patterns) {\n    if (pattern.test(content)) {\n      return true;\n    }\n  }\n\n  // Check against general action keywords\n  for (const keyword of GENERAL_ACTION_PATTERNS.keywords) {\n    // Look for complete words by using word boundaries\n    const regex = new RegExp(`\\\\b${keyword}\\\\b`, \"i\");\n    if (regex.test(content)) {\n      return true;\n    }\n  }\n\n  // Check against general action patterns\n  for (const pattern of GENERAL_ACTION_PATTERNS.patterns) {\n    if (pattern.test(content)) {\n      // For general patterns, require a bit more confidence\n      // Either the message must be relatively long or it must contain a clear directive\n      if (\n        content.length > 15 ||\n        /^(please|can you|could you|would you|help me)/i.test(content)\n      ) {\n        return true;\n      }\n    }\n  }\n\n  // Check if it's a question (questions are often actionable)\n  for (const pattern of GENERAL_ACTION_PATTERNS.questionPatterns) {\n    if (pattern.test(content) && content.length > 10) {\n      return true;\n    }\n  }\n\n  // If none of the patterns matched, it's likely not an actionable request\n  return false;\n}\n\n/**\n * @typedef {Object} Pattern\n * @property {string} pattern_id - Unique identifier for the pattern\n * @property {string} name - Name of the pattern\n * @property {string} description - Description of the pattern\n * @property {string} pattern_type - Type/category of the pattern\n * @property {string} pattern_data - JSON string containing the pattern data\n * @property {boolean} is_global - Whether this is a global pattern\n * @property {number} utility_score - Score indicating the utility of the pattern\n * @property {number} confidence_score - Score indicating confidence in the pattern\n * @property {string} created_at - Timestamp when the pattern was created\n * @property {string} last_used - Timestamp when the pattern was last used\n * @property {number} use_count - Number of times the pattern has been used\n */\n\n/**\n * Purpose to pattern type mapping and relevant keywords\n */\nconst PURPOSE_PATTERN_MAPPING = {\n  debugging: {\n    patternTypes: [\n      \"debugging_common_error_fix\",\n      \"error_handling\",\n      \"bug_fix\",\n      \"troubleshooting\",\n      \"error_pattern\",\n    ],\n    keywords: [\n      \"debug\",\n      \"error\",\n      \"exception\",\n      \"fix\",\n      \"bug\",\n      \"issue\",\n      \"resolve\",\n      \"problem\",\n      \"crash\",\n      \"failure\",\n      \"unexpected\",\n    ],\n  },\n\n  feature_planning: {\n    patternTypes: [\n      \"feature_template\",\n      \"design_pattern\",\n      \"architecture\",\n      \"planning\",\n      \"requirements\",\n    ],\n    keywords: [\n      \"feature\",\n      \"design\",\n      \"plan\",\n      \"architecture\",\n      \"structure\",\n      \"requirement\",\n      \"specification\",\n      \"implementation\",\n      \"organize\",\n    ],\n  },\n\n  code_review: {\n    patternTypes: [\n      \"code_review\",\n      \"quality_check\",\n      \"best_practice\",\n      \"code_standard\",\n      \"linter_rule\",\n    ],\n    keywords: [\n      \"review\",\n      \"quality\",\n      \"standard\",\n      \"convention\",\n      \"best practice\",\n      \"style\",\n      \"formatting\",\n      \"consistency\",\n      \"readability\",\n      \"maintainability\",\n    ],\n  },\n\n  learning: {\n    patternTypes: [\n      \"tutorial\",\n      \"learning_example\",\n      \"concept_explanation\",\n      \"educational_pattern\",\n      \"learning_path\",\n    ],\n    keywords: [\n      \"learn\",\n      \"tutorial\",\n      \"example\",\n      \"explanation\",\n      \"concept\",\n      \"guide\",\n      \"understand\",\n      \"teach\",\n      \"educational\",\n      \"introduction\",\n    ],\n  },\n\n  code_generation: {\n    patternTypes: [\n      \"code_template\",\n      \"code_generation\",\n      \"boilerplate\",\n      \"snippet\",\n      \"example_implementation\",\n    ],\n    keywords: [\n      \"generate\",\n      \"create\",\n      \"template\",\n      \"boilerplate\",\n      \"skeleton\",\n      \"sample\",\n      \"example\",\n      \"implementation\",\n      \"snippet\",\n      \"scaffold\",\n    ],\n  },\n\n  optimization: {\n    patternTypes: [\n      \"performance_optimization\",\n      \"efficiency_pattern\",\n      \"resource_usage\",\n      \"optimization_technique\",\n      \"bottleneck_fix\",\n    ],\n    keywords: [\n      \"optimize\",\n      \"performance\",\n      \"efficiency\",\n      \"speed\",\n      \"memory\",\n      \"resource\",\n      \"bottleneck\",\n      \"fast\",\n      \"slow\",\n      \"improve\",\n    ],\n  },\n\n  refactoring: {\n    patternTypes: [\n      \"refactoring_pattern\",\n      \"code_cleanup\",\n      \"restructuring\",\n      \"code_improvement\",\n      \"modernization\",\n    ],\n    keywords: [\n      \"refactor\",\n      \"cleanup\",\n      \"improve\",\n      \"restructure\",\n      \"simplify\",\n      \"readability\",\n      \"maintainability\",\n      \"technical debt\",\n      \"modernize\",\n    ],\n  },\n\n  general_query: {\n    patternTypes: [\n      \"general_pattern\",\n      \"utility\",\n      \"common_solution\",\n      \"frequently_used\",\n      \"general_purpose\",\n    ],\n    keywords: [\n      \"common\",\n      \"general\",\n      \"utility\",\n      \"helper\",\n      \"frequently\",\n      \"standard\",\n      \"basic\",\n      \"typical\",\n      \"regular\",\n      \"normal\",\n    ],\n  },\n};\n\n/**\n * Retrieves patterns relevant to a specific conversation purpose\n *\n * @param {string} purposeType - The type of conversation purpose\n * @returns {Promise<Pattern[]>} Array of patterns relevant to the specified purpose\n */\nexport async function getPurposeCorrelatedPatterns(purposeType) {\n  try {\n    // 1. Get the pattern types and keywords relevant to this purpose\n    const purposeMapping =\n      PURPOSE_PATTERN_MAPPING[purposeType] ||\n      PURPOSE_PATTERN_MAPPING.general_query;\n    const { patternTypes, keywords } = purposeMapping;\n\n    // 2. Build pattern type condition for SQL\n    const patternTypeCondition = patternTypes\n      .map(() => \"pattern_type = ?\")\n      .join(\" OR \");\n\n    // 3. Build keyword LIKE conditions for description search\n    const keywordConditions = keywords\n      .map(() => \"description LIKE ?\")\n      .join(\" OR \");\n\n    // 4. Combine the conditions with an OR\n    const combinedCondition = `(${patternTypeCondition}) OR (${keywordConditions})`;\n\n    // 5. Prepare the complete query with ordering by priority\n    const query = `\n      SELECT * FROM project_patterns\n      WHERE ${combinedCondition}\n      ORDER BY \n        is_global DESC,\n        utility_score DESC,\n        confidence_score DESC,\n        use_count DESC\n      LIMIT 20\n    `;\n\n    // 6. Prepare the parameters array\n    const params = [\n      ...patternTypes,\n      ...keywords.map((keyword) => `%${keyword}%`),\n    ];\n\n    // 7. Execute the query\n    const patterns = await executeQuery(query, params);\n\n    // 8. Process and return the results\n    return patterns || [];\n  } catch (error) {\n    console.error(\n      `Error retrieving purpose correlated patterns for ${purposeType}:`,\n      error\n    );\n    throw new Error(\n      `Failed to get purpose correlated patterns: ${error.message}`\n    );\n  }\n}\n\n/**\n * Detects the initial purpose of a conversation based on the first query\n *\n * @param {string} conversationId - The ID of the conversation\n * @param {string} initialQuery - The initial query that started the conversation\n * @returns {Promise<{purposeType: string, confidence: number}>} The detected purpose and confidence\n */\nexport async function detectInitialPurpose(conversationId, initialQuery) {\n  try {\n    // Create a mock message from the initial query\n    const message = {\n      content: initialQuery,\n      role: \"user\",\n    };\n\n    // Use the existing detection logic\n    const result = await detectConversationPurpose([message]);\n\n    if (!result || !result.purposeType) {\n      // Default to general_query if no purpose is detected\n      result.purposeType = \"general_query\";\n      result.confidence = 0.5;\n    }\n\n    // Try to track this as the first purpose in the purpose history,\n    // but don't block initialization if this fails (could be a foreign key constraint)\n    try {\n      await trackPurposeTransition(\n        conversationId,\n        result.purposeType,\n        result.confidence\n      );\n\n      console.log(\n        `Initial purpose for conversation ${conversationId}: ${result.purposeType} (${result.confidence})`\n      );\n    } catch (trackingError) {\n      console.error(\"Error tracking purpose transition:\", trackingError);\n      console.log(\n        \"Continuing with initialization despite purpose tracking error\"\n      );\n    }\n\n    return result;\n  } catch (error) {\n    console.error(\"Error detecting initial purpose:\", error);\n\n    // Default to general_query in case of error\n    return {\n      purposeType: \"general_query\",\n      confidence: 0.5,\n    };\n  }\n}\n\n/**\n * Sets the active purpose for a conversation\n *\n * @param {string} conversationId - The ID of the conversation\n * @param {string} purposeType - The type of purpose to set\n * @param {number} confidence - Confidence score for the purpose (0-1)\n * @returns {Promise<void>}\n */\nexport async function setActivePurpose(\n  conversationId,\n  purposeType,\n  confidence\n) {\n  try {\n    if (!conversationId) {\n      throw new Error(\"Conversation ID is required\");\n    }\n\n    if (!purposeType) {\n      throw new Error(\"Purpose type is required\");\n    }\n\n    // Validate confidence score\n    confidence = Math.max(0, Math.min(1, confidence));\n\n    // First, close any existing active purpose\n    const query1 = `\n      UPDATE conversation_purposes\n      SET end_timestamp = ?\n      WHERE conversation_id = ? AND end_timestamp IS NULL\n    `;\n    await executeQuery(query1, [new Date().toISOString(), conversationId]);\n\n    // Then create the new purpose record\n    const purposeId = uuidv4();\n    const startTimestamp = new Date().toISOString();\n\n    const query2 = `\n      INSERT INTO conversation_purposes (\n        purpose_id,\n        conversation_id,\n        purpose_type,\n        confidence,\n        start_timestamp,\n        end_timestamp\n      ) VALUES (?, ?, ?, ?, ?, NULL)\n    `;\n\n    await executeQuery(query2, [\n      purposeId,\n      conversationId,\n      purposeType,\n      confidence,\n      startTimestamp,\n    ]);\n\n    console.log(\n      `Set active purpose for conversation ${conversationId} to ${purposeType} (${confidence})`\n    );\n  } catch (error) {\n    console.error(\"Error setting active purpose:\", error);\n    throw new Error(\"Failed to set active purpose: \" + error.message);\n  }\n}\n", "/**\n * IntentPredictorLogic.js\n *\n * Provides functions for predicting user intent from queries and conversation history.\n */\n\nimport * as TextTokenizerLogic from \"./TextTokenizerLogic.js\";\nimport { executeQuery } from \"../db.js\";\nimport { v4 as uuidv4 } from \"uuid\";\nimport * as TimelineManagerLogic from \"./TimelineManagerLogic.js\";\n\n/**\n * @typedef {Object} Message\n * @property {string} content - The content of the message\n * @property {string} role - The role of the message sender (user or assistant)\n */\n\n/**\n * @typedef {Object} TimelineEvent\n * @property {string} event_id - Unique identifier for the event\n * @property {string} event_type - Type of event\n * @property {number} timestamp - Timestamp when the event occurred\n * @property {Object} data - Event data parsed from JSON\n * @property {string[]} associated_entity_ids - IDs of entities associated with this event\n * @property {string|null} conversation_id - Optional conversation ID this event belongs to\n * @property {string} created_at - Timestamp when the event was created in the database\n */\n\n/**\n * @typedef {Object} CodeChangeInfo\n * @property {string} path - Path to the file being edited\n * @property {string} [content] - Optional content of the file\n */\n\n/**\n * @typedef {Object} FocusArea\n * @property {string} focus_id - Unique identifier for the focus area\n * @property {string} focus_type - Type of focus area ('file', 'directory', 'task_type')\n * @property {string} identifier - Primary identifier for the focus area (e.g., file path)\n * @property {string} description - Human-readable description of the focus area\n * @property {string} related_entity_ids - JSON string of related entity IDs\n * @property {string} keywords - JSON string of keywords related to this focus area\n * @property {number} last_activated_at - Timestamp when this focus area was last active\n * @property {boolean} is_active - Whether this focus area is currently active\n */\n\n/**\n * @typedef {Object} IntentInfo\n * @property {string} intent - The inferred intent type\n * @property {number} [confidence] - Confidence score for the intent (0-1)\n * @property {string[]} [keywords] - Array of extracted keywords\n * @property {FocusArea} [focusArea] - The currently active focus area, if available\n */\n\n/**\n * @typedef {Object} IntentUpdateResult\n * @property {IntentInfo} [newIntent] - The newly inferred intent, if available\n * @property {boolean} [focusUpdated] - Whether the focus area was updated\n * @property {FocusArea} [currentFocus] - The current focus area after update\n */\n\n/**\n * Infers the user's intent from a query and conversation history\n *\n * @param {string} query - The user's query\n * @param {Message[]} [conversationHistory=[]] - The recent conversation history\n * @returns {Object} Object containing intent and keywords\n * @returns {string} .intent - The inferred intent\n * @returns {string[]} .keywords - Array of extracted keywords\n */\nexport function inferIntentFromQuery(query, conversationHistory = []) {\n  // Define possible intents\n  const intents = {\n    GENERAL_QUERY: \"general_query\",\n    CODE_SEARCH: \"code_search\",\n    EXPLANATION_REQUEST: \"explanation_request\",\n    DEBUGGING_ASSIST: \"debugging_assist\",\n    REFACTORING_SUGGESTION: \"refactoring_suggestion\",\n    IMPLEMENTATION_REQUEST: \"implementation_request\",\n    DOCUMENTATION_REQUEST: \"documentation_request\",\n  };\n\n  // Initialize scores for each intent\n  const intentScores = {\n    [intents.GENERAL_QUERY]: 0.1, // Base score\n    [intents.CODE_SEARCH]: 0,\n    [intents.EXPLANATION_REQUEST]: 0,\n    [intents.DEBUGGING_ASSIST]: 0,\n    [intents.REFACTORING_SUGGESTION]: 0,\n    [intents.IMPLEMENTATION_REQUEST]: 0,\n    [intents.DOCUMENTATION_REQUEST]: 0,\n  };\n\n  // Normalize the query\n  const normalizedQuery = query.toLowerCase();\n\n  // Extract keywords using TextTokenizerLogic\n  const tokens = TextTokenizerLogic.tokenize(query);\n  const keywords = TextTokenizerLogic.extractKeywords(tokens);\n\n  // Check for question marks (indicates question/explanation request)\n  if (normalizedQuery.includes(\"?\")) {\n    intentScores[intents.EXPLANATION_REQUEST] += 0.3;\n  }\n\n  // Check for code snippets (code blocks, function names, variable declarations)\n  const codePatterns = [\n    /```[\\s\\S]*?```/, // Code blocks\n    /function\\s+\\w+\\s*\\(.*?\\)/, // Function declarations\n    /const|let|var\\s+\\w+\\s*=/, // Variable declarations\n    /class\\s+\\w+/, // Class declarations\n    /import\\s+.*?from/, // Import statements\n  ];\n\n  for (const pattern of codePatterns) {\n    if (pattern.test(query)) {\n      intentScores[intents.CODE_SEARCH] += 0.2;\n      intentScores[intents.DEBUGGING_ASSIST] += 0.2;\n      break;\n    }\n  }\n\n  // Check for specific keywords\n  const keywordPatterns = [\n    // Search related\n    {\n      patterns: [\"find\", \"search\", \"where is\", \"locate\", \"look for\"],\n      intent: intents.CODE_SEARCH,\n      score: 0.6,\n    },\n    // Explanation related\n    {\n      patterns: [\n        \"explain\",\n        \"how does\",\n        \"what is\",\n        \"why\",\n        \"how to\",\n        \"tell me about\",\n      ],\n      intent: intents.EXPLANATION_REQUEST,\n      score: 0.6,\n    },\n    // Debugging related\n    {\n      patterns: [\n        \"error\",\n        \"bug\",\n        \"issue\",\n        \"problem\",\n        \"fix\",\n        \"debug\",\n        \"not working\",\n        \"exception\",\n        \"fail\",\n      ],\n      intent: intents.DEBUGGING_ASSIST,\n      score: 0.7,\n    },\n    // Refactoring related\n    {\n      patterns: [\n        \"refactor\",\n        \"improve\",\n        \"optimize\",\n        \"clean\",\n        \"better way\",\n        \"restructure\",\n        \"revise\",\n      ],\n      intent: intents.REFACTORING_SUGGESTION,\n      score: 0.65,\n    },\n    // Implementation related\n    {\n      patterns: [\n        \"implement\",\n        \"create\",\n        \"make\",\n        \"build\",\n        \"develop\",\n        \"code\",\n        \"add\",\n        \"new feature\",\n      ],\n      intent: intents.IMPLEMENTATION_REQUEST,\n      score: 0.6,\n    },\n    // Documentation related\n    {\n      patterns: [\n        \"document\",\n        \"comment\",\n        \"describe\",\n        \"explain code\",\n        \"documentation\",\n      ],\n      intent: intents.DOCUMENTATION_REQUEST,\n      score: 0.55,\n    },\n  ];\n\n  for (const { patterns, intent, score } of keywordPatterns) {\n    for (const pattern of patterns) {\n      if (normalizedQuery.includes(pattern)) {\n        intentScores[intent] += score;\n        break; // Only add the score once per pattern group\n      }\n    }\n  }\n\n  // Analyze conversation history for context\n  if (conversationHistory && conversationHistory.length > 0) {\n    // Get last few messages, focusing on user messages\n    const recentMessages = conversationHistory\n      .slice(-3) // Last 3 messages\n      .filter((msg) => msg.content);\n\n    for (const message of recentMessages) {\n      const normalizedContent = message.content.toLowerCase();\n\n      // If previous messages contained errors or debug terms, boost debugging intent\n      if (\n        /error|bug|issue|problem|fix|debug|not working|exception|fail/.test(\n          normalizedContent\n        )\n      ) {\n        intentScores[intents.DEBUGGING_ASSIST] += 0.2;\n      }\n\n      // If previous messages discussed code structure, boost refactoring intent\n      if (\n        /refactor|improve|optimize|clean|better|restructure|architecture/.test(\n          normalizedContent\n        )\n      ) {\n        intentScores[intents.REFACTORING_SUGGESTION] += 0.2;\n      }\n\n      // If previous messages were about explaining, boost explanation intent\n      if (\n        /explain|how does|what is|why|how to|understand/.test(normalizedContent)\n      ) {\n        intentScores[intents.EXPLANATION_REQUEST] += 0.15;\n      }\n    }\n  }\n\n  // Determine the winning intent\n  let maxScore = 0;\n  let inferredIntent = intents.GENERAL_QUERY; // Default\n\n  for (const [intent, score] of Object.entries(intentScores)) {\n    if (score > maxScore) {\n      maxScore = score;\n      inferredIntent = intent;\n    }\n  }\n\n  return {\n    intent: inferredIntent,\n    keywords,\n  };\n}\n\n/**\n * Predicts the current focus area based on recent activity and code edits\n *\n * @param {TimelineEvent[]} recentActivity - Recent events from the timeline\n * @param {CodeChangeInfo[]} currentCodeEdits - Information about currently edited files\n * @returns {Promise<FocusArea|null>} The predicted focus area or null if no clear focus\n */\nexport async function predictFocusArea(\n  recentActivity = [],\n  currentCodeEdits = []\n) {\n  try {\n    // Track file/path frequencies to determine most common focus areas\n    const pathFrequency = new Map();\n    const entityFrequency = new Map();\n    const activityTypes = new Map();\n    let keywordsSet = new Set();\n\n    // Process recent activity from timeline events\n    for (const event of recentActivity) {\n      // Count event types\n      activityTypes.set(\n        event.event_type,\n        (activityTypes.get(event.event_type) || 0) + 1\n      );\n\n      // Track file paths from event data\n      if (event.data && event.data.path) {\n        const path = event.data.path;\n        pathFrequency.set(path, (pathFrequency.get(path) || 0) + 1);\n\n        // Add depth to different path segments (directories, etc)\n        const segments = path.split(\"/\");\n        for (let i = 1; i < segments.length; i++) {\n          const dirPath = segments.slice(0, i).join(\"/\");\n          if (dirPath) {\n            pathFrequency.set(dirPath, (pathFrequency.get(dirPath) || 0) + 0.3);\n          }\n        }\n      }\n\n      // Track related entities\n      if (\n        event.associated_entity_ids &&\n        event.associated_entity_ids.length > 0\n      ) {\n        for (const entityId of event.associated_entity_ids) {\n          entityFrequency.set(\n            entityId,\n            (entityFrequency.get(entityId) || 0) + 1\n          );\n        }\n      }\n\n      // Extract keywords from event data\n      if (event.data && typeof event.data === \"object\") {\n        // Extract keywords from any descriptive fields\n        const textFields = [\n          event.data.description,\n          event.data.message,\n          event.data.content,\n          event.data.query,\n        ].filter(Boolean);\n\n        for (const text of textFields) {\n          if (text && typeof text === \"string\") {\n            const tokens = TextTokenizerLogic.tokenize(text);\n            const extractedKeywords =\n              TextTokenizerLogic.extractKeywords(tokens);\n            extractedKeywords.forEach((keyword) => keywordsSet.add(keyword));\n          }\n        }\n      }\n    }\n\n    // Process current code edits (these should get more weight as they represent current focus)\n    for (const edit of currentCodeEdits) {\n      const path = edit.path;\n      // Give higher weight to current edits\n      pathFrequency.set(path, (pathFrequency.get(path) || 0) + 3);\n\n      // Add depth to different path segments (directories, etc)\n      const segments = path.split(\"/\");\n      for (let i = 1; i < segments.length; i++) {\n        const dirPath = segments.slice(0, i).join(\"/\");\n        if (dirPath) {\n          pathFrequency.set(dirPath, (pathFrequency.get(dirPath) || 0) + 0.5);\n        }\n      }\n\n      // Extract keywords from content if available\n      if (edit.content) {\n        const tokens = TextTokenizerLogic.tokenize(edit.content);\n        const extractedKeywords = TextTokenizerLogic.extractKeywords(tokens);\n        extractedKeywords.forEach((keyword) => keywordsSet.add(keyword));\n      }\n    }\n\n    // Find the most frequent paths and entities\n    let primaryFocusPath = \"\";\n    let maxFrequency = 0;\n    let focusType = \"file\";\n\n    for (const [path, frequency] of pathFrequency.entries()) {\n      if (frequency > maxFrequency) {\n        maxFrequency = frequency;\n        primaryFocusPath = path;\n\n        // Determine if it's a file or directory\n        focusType =\n          path.includes(\".\") && !path.endsWith(\"/\") ? \"file\" : \"directory\";\n      }\n    }\n\n    // If we couldn't determine a clear focus from paths, try to determine from activity types\n    if (!primaryFocusPath && activityTypes.size > 0) {\n      let primaryActivityType = \"\";\n      maxFrequency = 0;\n\n      for (const [type, frequency] of activityTypes.entries()) {\n        if (frequency > maxFrequency) {\n          maxFrequency = frequency;\n          primaryActivityType = type;\n        }\n      }\n\n      if (primaryActivityType) {\n        primaryFocusPath = `activity:${primaryActivityType}`;\n        focusType = \"task_type\";\n      }\n    }\n\n    // If we still have no clear focus, return null\n    if (!primaryFocusPath) {\n      return null;\n    }\n\n    // Create a human-readable description\n    let description = \"\";\n    if (focusType === \"file\") {\n      description = `Working on file ${primaryFocusPath}`;\n    } else if (focusType === \"directory\") {\n      description = `Working in directory ${primaryFocusPath}`;\n    } else {\n      description = `${primaryFocusPath.replace(\"activity:\", \"\")} activity`;\n    }\n\n    // Collect related entity IDs (most frequent ones)\n    const relatedEntityIds = Array.from(entityFrequency.entries())\n      .sort((a, b) => b[1] - a[1])\n      .slice(0, 10)\n      .map(([entityId]) => entityId);\n\n    // Collect keywords (convert Set to Array)\n    const keywords = Array.from(keywordsSet).slice(0, 20);\n\n    // Create the focus area object\n    const focusArea = {\n      focus_id: uuidv4(),\n      focus_type: focusType,\n      identifier: primaryFocusPath,\n      description,\n      related_entity_ids: JSON.stringify(relatedEntityIds),\n      keywords: JSON.stringify(keywords),\n      last_activated_at: Date.now(),\n      is_active: true,\n    };\n\n    // Call updateFocusAreaInDb to persist the focus area\n    // Note: This function will be implemented in another task\n    try {\n      await updateFocusAreaInDb(focusArea);\n    } catch (error) {\n      // Log the error but don't fail - we still want to return the computed focus area\n      console.error(\"Error updating focus area in database:\", error);\n    }\n\n    return focusArea;\n  } catch (error) {\n    console.error(\"Error predicting focus area:\", error);\n    return null;\n  }\n}\n\n/**\n * Updates or creates a focus area in the database\n *\n * @param {FocusArea} focus - The focus area to update or create\n * @returns {Promise<void>}\n */\nexport async function updateFocusAreaInDb(focus) {\n  try {\n    // Ensure that related_entity_ids and keywords are JSON strings\n    const relatedEntityIds =\n      typeof focus.related_entity_ids === \"string\"\n        ? focus.related_entity_ids\n        : JSON.stringify(focus.related_entity_ids || []);\n\n    const keywords =\n      typeof focus.keywords === \"string\"\n        ? focus.keywords\n        : JSON.stringify(focus.keywords || []);\n\n    // Ensure last_activated_at is set to current time if not provided\n    const lastActivated = focus.last_activated_at || Date.now();\n\n    // Begin transaction - execute a series of queries that should complete together\n    await executeQuery(\"BEGIN TRANSACTION\");\n\n    try {\n      // Step 1: Set all existing focus areas to inactive\n      await executeQuery(\n        \"UPDATE focus_areas SET is_active = FALSE WHERE is_active = TRUE\"\n      );\n\n      // Step 2: Check if the focus area already exists\n      const existingFocus = await executeQuery(\n        \"SELECT focus_id FROM focus_areas WHERE identifier = ?\",\n        [focus.identifier]\n      );\n\n      if (existingFocus && existingFocus.length > 0) {\n        // Update existing focus area\n        await executeQuery(\n          `UPDATE focus_areas SET \n            focus_type = ?,\n            description = ?,\n            related_entity_ids = ?,\n            keywords = ?,\n            last_activated_at = ?,\n            is_active = TRUE\n          WHERE focus_id = ?`,\n          [\n            focus.focus_type,\n            focus.description,\n            relatedEntityIds,\n            keywords,\n            lastActivated,\n            existingFocus[0].focus_id,\n          ]\n        );\n      } else {\n        // Insert new focus area\n        await executeQuery(\n          `INSERT INTO focus_areas (\n            focus_id,\n            focus_type,\n            identifier,\n            description,\n            related_entity_ids,\n            keywords,\n            last_activated_at,\n            is_active\n          ) VALUES (?, ?, ?, ?, ?, ?, ?, TRUE)`,\n          [\n            focus.focus_id,\n            focus.focus_type,\n            focus.identifier,\n            focus.description,\n            relatedEntityIds,\n            keywords,\n            lastActivated,\n          ]\n        );\n      }\n\n      // Commit the transaction\n      await executeQuery(\"COMMIT\");\n    } catch (error) {\n      // If any query fails, roll back the transaction\n      await executeQuery(\"ROLLBACK\");\n      throw error;\n    }\n  } catch (error) {\n    console.error(\"Error updating focus area in database:\", error);\n    throw error;\n  }\n}\n\n/**\n * Retrieves and analyzes the current intent for a conversation\n *\n * @param {string} conversationId - The ID of the conversation to analyze\n * @returns {Promise<IntentInfo|null>} The intent information or null if no clear intent\n */\nexport async function getIntent(conversationId) {\n  try {\n    // 1. Retrieve the most recent messages for the given conversationId\n    const recentMessages = await executeQuery(\n      `SELECT content, role, timestamp \n       FROM conversation_history \n       WHERE conversation_id = ? \n       ORDER BY timestamp DESC \n       LIMIT 5`,\n      [conversationId]\n    );\n\n    if (!recentMessages || recentMessages.length === 0) {\n      return null; // No messages found for this conversation\n    }\n\n    // Convert to the Message format expected by inferIntentFromQuery\n    const messages = recentMessages.map((msg) => ({\n      content: msg.content,\n      role: msg.role,\n    }));\n\n    // Get the most recent user message\n    const lastUserMessage = messages.find((msg) => msg.role === \"user\");\n\n    if (!lastUserMessage) {\n      return null; // No user messages found\n    }\n\n    // 2. Analyze the messages using inferIntentFromQuery\n    const { intent, keywords } = inferIntentFromQuery(\n      lastUserMessage.content,\n      messages\n    );\n\n    // 3. Get the currently active focus area\n    const activeFocusAreas = await executeQuery(\n      \"SELECT * FROM focus_areas WHERE is_active = TRUE LIMIT 1\"\n    );\n\n    let focusArea = null;\n    if (activeFocusAreas && activeFocusAreas.length > 0) {\n      const rawFocusArea = activeFocusAreas[0];\n\n      // Parse JSON fields\n      focusArea = {\n        ...rawFocusArea,\n        related_entity_ids: JSON.parse(rawFocusArea.related_entity_ids || \"[]\"),\n        keywords: JSON.parse(rawFocusArea.keywords || \"[]\"),\n      };\n    }\n\n    // 4. Calculate a confidence score based on the clarity of intent\n    // This is simplified - a real implementation might use more sophisticated scoring\n    let confidence = 0.5; // Default medium confidence\n\n    // Increase confidence if we have both clear intent and matching focus area\n    if (intent !== \"general_query\" && focusArea) {\n      confidence = 0.7;\n\n      // Check if any keywords match the focus area keywords\n      if (focusArea.keywords && keywords) {\n        const matchingKeywords = keywords.filter((k) =>\n          focusArea.keywords.includes(k)\n        );\n\n        if (matchingKeywords.length > 0) {\n          confidence += Math.min(0.3, matchingKeywords.length * 0.05);\n        }\n      }\n    }\n\n    // 5. Combine the information into an IntentInfo object\n    const intentInfo = {\n      intent,\n      confidence,\n      keywords,\n      focusArea,\n    };\n\n    return intentInfo;\n  } catch (error) {\n    console.error(\"Error getting intent for conversation:\", error);\n    return null;\n  }\n}\n\n/**\n * Updates the intent and focus area based on new activity signals\n *\n * @param {Object} params - Parameters containing activity signals\n * @param {string} params.conversationId - ID of the conversation to update\n * @param {string} [params.newMessage] - New message content, if any\n * @param {boolean} [params.isUser=false] - Whether the new message is from the user\n * @param {string} [params.activeFile] - Currently active file path, if any\n * @param {CodeChangeInfo[]} [params.codeChanges] - Information about code changes\n * @returns {Promise<IntentUpdateResult>} Result indicating intent and focus updates\n */\nexport async function updateIntent(params) {\n  try {\n    const {\n      conversationId,\n      newMessage,\n      isUser = false,\n      activeFile,\n      codeChanges = [],\n    } = params;\n\n    let newIntent = null;\n    let focusUpdated = false;\n    let currentFocus = null;\n\n    // 1. If new message is present and from user, determine textual intent\n    if (newMessage && isUser) {\n      // Get recent conversation history\n      const recentMessages = await executeQuery(\n        `SELECT content, role, timestamp \n         FROM conversation_history \n         WHERE conversation_id = ? \n         ORDER BY timestamp DESC \n         LIMIT 5`,\n        [conversationId]\n      );\n\n      // Convert to Message format and add the new message\n      const messages = recentMessages.map((msg) => ({\n        content: msg.content,\n        role: msg.role,\n      }));\n\n      // Add the new message to the history\n      messages.unshift({\n        content: newMessage,\n        role: \"user\",\n      });\n\n      // Infer intent from the new message\n      const { intent, keywords } = inferIntentFromQuery(newMessage, messages);\n\n      // Get the current focus area\n      const activeFocusAreas = await executeQuery(\n        \"SELECT * FROM focus_areas WHERE is_active = TRUE LIMIT 1\"\n      );\n\n      let focusArea = null;\n      if (activeFocusAreas && activeFocusAreas.length > 0) {\n        const rawFocusArea = activeFocusAreas[0];\n\n        // Parse JSON fields\n        focusArea = {\n          ...rawFocusArea,\n          related_entity_ids: JSON.parse(\n            rawFocusArea.related_entity_ids || \"[]\"\n          ),\n          keywords: JSON.parse(rawFocusArea.keywords || \"[]\"),\n        };\n      }\n\n      // Calculate confidence\n      let confidence = 0.5; // Default medium confidence\n\n      if (intent !== \"general_query\" && focusArea) {\n        confidence = 0.7;\n\n        // Check if any keywords match the focus area keywords\n        if (focusArea.keywords && keywords) {\n          const matchingKeywords = keywords.filter((k) =>\n            focusArea.keywords.includes(k)\n          );\n\n          if (matchingKeywords.length > 0) {\n            confidence += Math.min(0.3, matchingKeywords.length * 0.05);\n          }\n        }\n      }\n\n      // Create IntentInfo object\n      newIntent = {\n        intent,\n        confidence,\n        keywords,\n        focusArea,\n      };\n    }\n\n    // 2. Determine if project-level focus has shifted based on code activity\n    // First, gather relevant activity information\n    const codeActivity = [];\n\n    // Add active file as a code activity if provided\n    if (activeFile) {\n      codeActivity.push({\n        path: activeFile,\n      });\n    }\n\n    // Add code changes\n    if (codeChanges && codeChanges.length > 0) {\n      codeActivity.push(...codeChanges);\n    }\n\n    // Get recent timeline events\n    const recentEvents = await TimelineManagerLogic.getEvents({\n      limit: 20,\n      types: [\"code_change\", \"file_open\", \"cursor_move\", \"navigation\"],\n    });\n\n    // If we have any code activity or recent events, check for focus shift\n    if (codeActivity.length > 0 || recentEvents.length > 0) {\n      // Predict focus area based on activity\n      const newFocusArea = await predictFocusArea(recentEvents, codeActivity);\n\n      if (newFocusArea) {\n        // Focus was updated by predictFocusArea\n        focusUpdated = true;\n        currentFocus = newFocusArea;\n      } else {\n        // No focus update, get current focus\n        const activeFocusAreas = await executeQuery(\n          \"SELECT * FROM focus_areas WHERE is_active = TRUE LIMIT 1\"\n        );\n\n        if (activeFocusAreas && activeFocusAreas.length > 0) {\n          const rawFocusArea = activeFocusAreas[0];\n\n          // Parse JSON fields\n          currentFocus = {\n            ...rawFocusArea,\n            related_entity_ids: JSON.parse(\n              rawFocusArea.related_entity_ids || \"[]\"\n            ),\n            keywords: JSON.parse(rawFocusArea.keywords || \"[]\"),\n          };\n        }\n      }\n    } else {\n      // No code activity, just get current focus\n      const activeFocusAreas = await executeQuery(\n        \"SELECT * FROM focus_areas WHERE is_active = TRUE LIMIT 1\"\n      );\n\n      if (activeFocusAreas && activeFocusAreas.length > 0) {\n        const rawFocusArea = activeFocusAreas[0];\n\n        // Parse JSON fields\n        currentFocus = {\n          ...rawFocusArea,\n          related_entity_ids: JSON.parse(\n            rawFocusArea.related_entity_ids || \"[]\"\n          ),\n          keywords: JSON.parse(rawFocusArea.keywords || \"[]\"),\n        };\n      }\n    }\n\n    // If we have a new intent but no focus area in it, add the current focus\n    if (newIntent && !newIntent.focusArea && currentFocus) {\n      newIntent.focusArea = currentFocus;\n    }\n\n    // Return the IntentUpdateResult\n    return {\n      newIntent,\n      focusUpdated,\n      currentFocus,\n    };\n  } catch (error) {\n    console.error(\"Error updating intent:\", error);\n    // Return minimal information in case of error\n    return {\n      focusUpdated: false,\n    };\n  }\n}\n", "/**\n * TimelineManagerLogic.js\n *\n * Provides functions for managing and recording timeline events.\n */\n\nimport { v4 as uuidv4 } from \"uuid\";\nimport { executeQuery } from \"../db.js\";\n\n/**\n * @typedef {Object} TimelineEvent\n * @property {string} event_id - Unique identifier for the event\n * @property {string} event_type - Type of event\n * @property {number} timestamp - Timestamp when the event occurred\n * @property {Object} data - Event data parsed from JSON\n * @property {string[]} associated_entity_ids - IDs of entities associated with this event\n * @property {string|null} conversation_id - Optional conversation ID this event belongs to\n * @property {string} created_at - Timestamp when the event was created in the database\n */\n\n/**\n * @typedef {Object} Snapshot\n * @property {string} snapshot_id - Unique identifier for the snapshot\n * @property {string|null} name - Optional name of the snapshot\n * @property {string|null} description - Optional description of the snapshot\n * @property {Object} snapshot_data - Parsed snapshot data from JSON\n * @property {string|null} timeline_event_id - Optional ID of the associated timeline event\n * @property {string} created_at - Timestamp when the snapshot was created in the database\n */\n\n/**\n * Records an event in the timeline\n *\n * @param {string} type - Type of event\n * @param {object} data - Event data object\n * @param {string[]} [associatedEntityIds=[]] - IDs of entities associated with this event\n * @param {string} [conversationId] - Optional conversation ID this event belongs to\n * @returns {Promise<string>} Generated event ID\n */\nexport async function recordEvent(\n  type,\n  data,\n  associatedEntityIds = [],\n  conversationId = null\n) {\n  try {\n    // Generate a unique event ID\n    const eventId = uuidv4();\n\n    // Convert data and associatedEntityIds to JSON strings\n    const dataJson = JSON.stringify(data);\n    const entityIdsJson = JSON.stringify(associatedEntityIds);\n\n    // Get current timestamp\n    const timestamp = Date.now();\n\n    // Construct the SQL query\n    const query = `\n      INSERT INTO timeline_events (\n        event_id, \n        event_type, \n        timestamp, \n        data, \n        associated_entity_ids,\n        conversation_id\n      ) VALUES (?, ?, ?, ?, ?, ?)\n    `;\n\n    // Execute the query with parameters\n    await executeQuery(query, [\n      eventId,\n      type,\n      timestamp,\n      dataJson,\n      entityIdsJson,\n      conversationId,\n    ]);\n\n    return eventId;\n  } catch (error) {\n    console.error(`Error recording timeline event (${type}):`, error);\n    throw error;\n  }\n}\n\n/**\n * Creates a snapshot of the active context data\n *\n * @param {object} activeContextData - Data to be snapshotted (active entity IDs, focus, etc.)\n * @param {string} [name] - Optional name for the snapshot\n * @param {string} [description] - Optional description of the snapshot\n * @param {string} [timeline_event_id] - Optional ID of an associated timeline event\n * @returns {Promise<string>} Generated snapshot ID\n */\nexport async function createSnapshot(\n  activeContextData,\n  name = null,\n  description = null,\n  timeline_event_id = null\n) {\n  try {\n    // Generate a unique snapshot ID\n    const snapshot_id = uuidv4();\n\n    // Convert activeContextData to a JSON string\n    const snapshot_data = JSON.stringify(activeContextData);\n\n    // Construct the SQL query\n    const query = `\n      INSERT INTO context_snapshots (\n        snapshot_id,\n        name,\n        description,\n        snapshot_data,\n        timeline_event_id\n      ) VALUES (?, ?, ?, ?, ?)\n    `;\n\n    // Execute the query with parameters\n    await executeQuery(query, [\n      snapshot_id,\n      name,\n      description,\n      snapshot_data,\n      timeline_event_id,\n    ]);\n\n    return snapshot_id;\n  } catch (error) {\n    console.error(\"Error creating context snapshot:\", error);\n    throw error;\n  }\n}\n\n/**\n * Manages the creation of implicit checkpoints based on activity thresholds\n * This function checks for substantial activity and creates automatic snapshots when appropriate\n *\n * @returns {Promise<void>}\n */\nexport async function manageImplicitCheckpoints() {\n  try {\n    // Define activity thresholds\n    const MIN_EVENTS_FOR_CHECKPOINT = 10;\n    const MIN_MINUTES_SINCE_LAST_CHECKPOINT = 15;\n    const SIGNIFICANT_EVENT_TYPES = [\n      \"code_change\",\n      \"conversation_end\",\n      \"focus_change\",\n    ];\n\n    // Get the timestamp of the last implicit checkpoint\n    const lastCheckpointQuery = `\n      SELECT cs.snapshot_id, te.timestamp\n      FROM context_snapshots cs\n      LEFT JOIN timeline_events te ON cs.timeline_event_id = te.event_id\n      WHERE (cs.name LIKE 'Implicit Checkpoint%' OR te.event_type = 'implicit_checkpoint_creation')\n      ORDER BY te.timestamp DESC\n      LIMIT 1\n    `;\n\n    const lastCheckpoint = await executeQuery(lastCheckpointQuery);\n    const lastCheckpointTime =\n      lastCheckpoint.rows && lastCheckpoint.rows.length > 0\n        ? lastCheckpoint.rows[0].timestamp\n        : 0;\n\n    // Calculate time threshold\n    const timeThreshold =\n      Date.now() - MIN_MINUTES_SINCE_LAST_CHECKPOINT * 60 * 1000;\n\n    // Check if enough time has passed since last checkpoint\n    if (lastCheckpointTime > timeThreshold) {\n      // Not enough time has passed\n      return;\n    }\n\n    // Count events since last checkpoint\n    const countEventsQuery = `\n      SELECT COUNT(*) as event_count\n      FROM timeline_events\n      WHERE timestamp > ?\n    `;\n\n    const eventCountResult = await executeQuery(countEventsQuery, [\n      lastCheckpointTime,\n    ]);\n    const eventCount =\n      eventCountResult.rows && eventCountResult.rows.length > 0\n        ? eventCountResult.rows[0].event_count || 0\n        : 0;\n\n    // Count significant events\n    const significantEventsQuery = `\n      SELECT COUNT(*) as significant_count\n      FROM timeline_events\n      WHERE timestamp > ? AND event_type IN (${SIGNIFICANT_EVENT_TYPES.map(\n        () => \"?\"\n      ).join(\",\")})\n    `;\n\n    const significantCountResult = await executeQuery(significantEventsQuery, [\n      lastCheckpointTime,\n      ...SIGNIFICANT_EVENT_TYPES,\n    ]);\n    const significantCount =\n      significantCountResult.rows && significantCountResult.rows.length > 0\n        ? significantCountResult.rows[0].significant_count || 0\n        : 0;\n\n    // Determine if we should create a checkpoint\n    const shouldCreateCheckpoint =\n      eventCount >= MIN_EVENTS_FOR_CHECKPOINT || significantCount > 0;\n\n    if (shouldCreateCheckpoint) {\n      // Get active context data\n      // In a real implementation, this would come from ActiveContextManager\n      // Since that's not available, we'll create mock data to demonstrate the function\n      const activeContextData = {\n        activeEntities: [], // This would be populated with actual entity IDs\n        activeFocus: null, // This would be the current focus area\n        timestamp: Date.now(),\n      };\n\n      // Try to get actual context data if available\n      try {\n        // This is a placeholder - in real implementation, we would check if\n        // ActiveContextManager is available and use it to get context data\n        const ActiveContextManager = global.ActiveContextManager;\n        if (\n          ActiveContextManager &&\n          typeof ActiveContextManager.getActiveContextAsEntities === \"function\"\n        ) {\n          const contextData =\n            await ActiveContextManager.getActiveContextAsEntities();\n          if (contextData) {\n            activeContextData.activeEntities = contextData.entities || [];\n            activeContextData.activeFocus = contextData.focus || null;\n          }\n        }\n      } catch (error) {\n        console.warn(\n          \"Could not retrieve data from ActiveContextManager:\",\n          error.message\n        );\n        // Continue with mock data\n      }\n\n      // Generate checkpoint name and description\n      const timestamp = new Date().toISOString();\n      const checkpointName = `Implicit Checkpoint [${timestamp}]`;\n      let description = \"Automatically created checkpoint due to \";\n\n      if (eventCount >= MIN_EVENTS_FOR_CHECKPOINT) {\n        description += `high activity (${eventCount} events)`;\n      } else if (significantCount > 0) {\n        description += `significant changes (${significantCount} significant events)`;\n      }\n\n      // Record the checkpoint creation event\n      const eventId = await recordEvent(\"implicit_checkpoint_creation\", {\n        reason: description,\n        eventCount,\n        significantCount,\n      });\n\n      // Create the snapshot\n      await createSnapshot(\n        activeContextData,\n        checkpointName,\n        description,\n        eventId\n      );\n    }\n  } catch (error) {\n    console.error(\"Error managing implicit checkpoints:\", error);\n    // Don't throw - this function should not crash the application\n  }\n}\n\n/**\n * Retrieves timeline events based on specified filters\n *\n * @param {Object} options - Query options\n * @param {string[]} [options.types] - Filter events by these event types\n * @param {number} [options.limit] - Maximum number of events to return\n * @param {string} [options.conversationId] - Filter events by this conversation ID\n * @param {boolean} [options.includeMilestones=true] - Whether to include milestone events\n * @param {string} [options.excludeConversationId] - Exclude events with this conversation ID\n * @returns {Promise<TimelineEvent[]>} Array of timeline events with parsed JSON fields\n */\nexport async function getEvents(options = {}) {\n  try {\n    const {\n      types,\n      limit,\n      conversationId,\n      includeMilestones = true,\n      excludeConversationId,\n    } = options;\n\n    // Build the base query\n    let query = \"SELECT * FROM timeline_events WHERE 1=1\";\n    const params = [];\n\n    // Apply filters based on options\n    if (types && types.length > 0) {\n      query += ` AND event_type IN (${types.map(() => \"?\").join(\",\")})`;\n      params.push(...types);\n    }\n\n    if (conversationId) {\n      query += \" AND conversation_id = ?\";\n      params.push(conversationId);\n    }\n\n    if (excludeConversationId) {\n      query += \" AND (conversation_id != ? OR conversation_id IS NULL)\";\n      params.push(excludeConversationId);\n    }\n\n    // Handle milestone events filtering\n    // Assuming milestone events have specific types like 'milestone_created' or are linked to snapshots\n    if (!includeMilestones) {\n      // Define the event types that are considered milestones\n      const milestoneEventTypes = [\n        \"milestone_created\",\n        \"implicit_checkpoint_creation\",\n        \"checkpoint_created\",\n      ];\n      query += ` AND event_type NOT IN (${milestoneEventTypes\n        .map(() => \"?\")\n        .join(\",\")})`;\n      params.push(...milestoneEventTypes);\n\n      // Additionally exclude events that have an associated snapshot\n      query += ` AND NOT EXISTS (\n        SELECT 1 FROM context_snapshots \n        WHERE context_snapshots.timeline_event_id = timeline_events.event_id\n      )`;\n    }\n\n    // Add ordering\n    query += \" ORDER BY timestamp DESC\";\n\n    // Apply limit if specified\n    if (limit && Number.isInteger(limit) && limit > 0) {\n      query += \" LIMIT ?\";\n      params.push(limit);\n    }\n\n    // Execute the query\n    const events = await executeQuery(query, params);\n\n    // Check if events has a rows property and it's an array\n    const rows =\n      events && events.rows && Array.isArray(events.rows)\n        ? events.rows\n        : Array.isArray(events)\n        ? events\n        : [];\n\n    // If no valid results, return empty array\n    if (rows.length === 0) {\n      console.warn(\"No valid timeline events found\");\n      return [];\n    }\n\n    // Parse JSON fields in each event\n    return rows.map((event) => ({\n      ...event,\n      data: JSON.parse(event.data || \"{}\"),\n      associated_entity_ids: JSON.parse(event.associated_entity_ids || \"[]\"),\n    }));\n  } catch (error) {\n    console.error(\"Error retrieving timeline events:\", error);\n    throw error;\n  }\n}\n\n/**\n * Retrieves context snapshots (milestones) based on specified filters\n *\n * @param {Object} options - Query options\n * @param {string[]} [options.types] - Filter snapshots by type-related keywords in name or description\n * @param {number} [options.limit] - Maximum number of snapshots to return\n * @returns {Promise<Snapshot[]>} Array of context snapshots with parsed snapshot_data field\n */\nexport async function getMilestones(options = {}) {\n  try {\n    const { types, limit } = options;\n\n    // Start building the query\n    let query = `\n      SELECT cs.*, te.event_type\n      FROM context_snapshots cs\n      LEFT JOIN timeline_events te ON cs.timeline_event_id = te.event_id\n      WHERE 1=1\n    `;\n\n    const params = [];\n\n    // Apply type filtering based on name, description or associated event type\n    if (types && types.length > 0) {\n      const typeConditions = [];\n\n      for (const type of types) {\n        // Create pattern for LIKE queries\n        const pattern = `%${type}%`;\n\n        // Add conditions for name, description and associated event type\n        typeConditions.push(\"cs.name LIKE ?\");\n        params.push(pattern);\n\n        typeConditions.push(\"cs.description LIKE ?\");\n        params.push(pattern);\n\n        typeConditions.push(\"te.event_type LIKE ?\");\n        params.push(pattern);\n\n        // Also search in event data if linked to a timeline event\n        typeConditions.push(`\n          EXISTS (\n            SELECT 1 FROM timeline_events\n            WHERE timeline_events.event_id = cs.timeline_event_id\n            AND timeline_events.data LIKE ?\n          )\n        `);\n        params.push(`%\"category\":\"${type}\"%`);\n      }\n\n      if (typeConditions.length > 0) {\n        query += ` AND (${typeConditions.join(\" OR \")})`;\n      }\n    }\n\n    // Add ordering by timestamp (assuming cs.timestamp exists)\n    query += \" ORDER BY timestamp DESC\";\n\n    // Apply limit if specified\n    if (limit && Number.isInteger(limit) && limit > 0) {\n      query += \" LIMIT ?\";\n      params.push(limit);\n    }\n\n    // Execute the query\n    const snapshots = await executeQuery(query, params);\n\n    // Check if snapshots has a rows property and it's an array\n    const rows =\n      snapshots && snapshots.rows && Array.isArray(snapshots.rows)\n        ? snapshots.rows\n        : Array.isArray(snapshots)\n        ? snapshots\n        : [];\n\n    // If no valid results, return empty array\n    if (rows.length === 0) {\n      console.warn(\"No valid snapshots found\");\n      return [];\n    }\n\n    // Parse snapshot_data from JSON for each result\n    return rows.map((snapshot) => ({\n      ...snapshot,\n      snapshot_data: JSON.parse(snapshot.snapshot_data || \"{}\"),\n    }));\n  } catch (error) {\n    console.error(\"Error retrieving milestones:\", error);\n    throw error;\n  }\n}\n\n/**\n * Gets recent events for a specific conversation\n *\n * @param {string} conversationId - The conversation ID\n * @param {number} [limit=10] - Maximum number of events to return\n * @param {string[]} [eventTypes] - Optional array of event types to filter by\n * @returns {Promise<TimelineEvent[]>} Array of timeline events\n */\nexport async function getRecentEventsForConversation(\n  conversationId,\n  limit = 10,\n  eventTypes = null\n) {\n  try {\n    if (!conversationId) {\n      throw new Error(\"Conversation ID is required\");\n    }\n\n    // Build the query\n    let query = `\n      SELECT \n        event_id,\n        event_type,\n        timestamp,\n        data,\n        associated_entity_ids,\n        conversation_id\n      FROM \n        timeline_events\n      WHERE \n        conversation_id = ?\n    `;\n\n    const params = [conversationId];\n\n    // Add event type filter if provided\n    if (eventTypes && Array.isArray(eventTypes) && eventTypes.length > 0) {\n      const placeholders = eventTypes.map(() => \"?\").join(\",\");\n      query += ` AND event_type IN (${placeholders})`;\n      params.push(...eventTypes);\n    }\n\n    // Add order and limit\n    query += `\n      ORDER BY \n        timestamp DESC\n      LIMIT ?\n    `;\n    params.push(limit);\n\n    // Execute the query\n    const results = await executeQuery(query, params);\n\n    // Check if results has a rows property and it's an array\n    const rows =\n      results && results.rows && Array.isArray(results.rows)\n        ? results.rows\n        : Array.isArray(results)\n        ? results\n        : [];\n\n    // If no valid results, return empty array\n    if (rows.length === 0) {\n      console.warn(\"No recent events found for conversation:\", conversationId);\n      return [];\n    }\n\n    // Parse the JSON fields\n    return rows.map((event) => ({\n      ...event,\n      data: JSON.parse(event.data || \"{}\"),\n      associated_entity_ids: JSON.parse(event.associated_entity_ids || \"[]\"),\n    }));\n  } catch (error) {\n    console.error(\n      `Error getting recent events for conversation ${conversationId}:`,\n      error\n    );\n    return [];\n  }\n}\n", "/**\n * SmartSearchServiceLogic.js\n *\n * Provides advanced search capabilities for code entities using a combination\n * of Full-Text Search (FTS) and keyword-based matching.\n */\n\nimport { executeQuery } from \"../db.js\";\nimport { tokenize, extractKeywords, stem } from \"./TextTokenizerLogic.js\";\n\n/**\n * @typedef {Object} SearchOptions\n * @property {string[]} [entityTypes] - Types of entities to search (e.g., 'file', 'function', 'class')\n * @property {string[]} [filePaths] - File paths to limit the search to\n * @property {Object} [dateRange] - Date range to filter by last modified date\n * @property {Date} [dateRange.start] - Start date of range\n * @property {Date} [dateRange.end] - End date of range\n * @property {string} [sortBy] - Field to sort results by\n * @property {number} [limit] - Maximum number of results to return\n * @property {number} [minRelevance] - Minimum relevance score for results\n * @property {string} [strategy] - Search strategy to use ('fts', 'keywords', 'combined')\n * @property {string} [booleanOperator] - Boolean operator for keyword combination\n * @property {boolean} [useExactMatch] - Whether to use exact phrase matching\n * @property {boolean} [useProximity] - Whether to use proximity search\n * @property {number} [proximityDistance] - Distance for proximity search\n */\n\n/**\n * @typedef {Object} CodeEntity\n * @property {string} entity_id - Unique identifier for the code entity\n * @property {string} file_path - Path to the file containing the entity\n * @property {string} entity_type - Type of code entity (e.g., 'file', 'function', 'class')\n * @property {string} name - Name of the code entity\n * @property {string} [parent_entity_id] - ID of the parent entity (if any)\n * @property {string} [content_hash] - Hash of the entity content\n * @property {string} [raw_content] - Raw content of the entity\n * @property {number} [start_line] - Start line of the entity within the file\n * @property {number} [end_line] - End line of the entity within the file\n * @property {string} [language] - Programming language of the entity\n * @property {string} [created_at] - Creation timestamp\n * @property {string} [last_modified_at] - Last modification timestamp\n */\n\n/**\n * @typedef {Object} SearchResult\n * @property {CodeEntity} entity - The found code entity\n * @property {number} relevanceScore - Relevance score for the search result\n */\n\n/**\n * Searches code entities by keywords using Full-Text Search and/or entity_keywords table\n *\n * @param {string[]} keywords - Keywords to search for\n * @param {SearchOptions} [options={}] - Search options including:\n *   - entityTypes: Types of entities to search\n *   - filePaths: File paths with glob pattern support\n *   - dateRange: Date range to filter by\n *   - sortBy: Field to sort by\n *   - limit: Max results\n *   - minRelevance: Minimum relevance score\n *   - strategy: Search strategy ('fts', 'keywords', 'combined')\n *   - booleanOperator: 'AND' or 'OR' for keyword combination\n *   - useExactMatch: Whether to use exact phrase matching\n *   - useProximity: Whether to use proximity search\n *   - proximityDistance: Distance for proximity search\n * @returns {Promise<SearchResult[]>} Array of search results\n */\nexport async function searchByKeywords(keywords, options = {}) {\n  try {\n    // Validate and normalize input\n    if (!keywords || !Array.isArray(keywords) || keywords.length === 0) {\n      throw new Error(\"Keywords array is required and cannot be empty\");\n    }\n\n    // Handle single string input with boolean operators\n    if (\n      keywords.length === 1 &&\n      /\\s+(AND|OR|NOT|NEAR\\/\\d+)\\s+/i.test(keywords[0])\n    ) {\n      // Keep as is - will be processed by searchUsingFTS\n    } else {\n      // Process and clean keywords\n      keywords = keywords.map((kw) => kw.trim()).filter((kw) => kw.length > 0);\n    }\n\n    // Set default options\n    options = {\n      strategy: \"combined\", // Default to combined search\n      booleanOperator: \"OR\", // Default to OR for broader matches\n      limit: 100, // Default result limit\n      ...options,\n    };\n\n    // Prepare results array\n    let searchResults = [];\n\n    // If strategy is 'fts' or 'combined', perform FTS search\n    if (options.strategy === \"fts\" || options.strategy === \"combined\") {\n      const ftsResults = await searchUsingFTS(keywords, options);\n      searchResults = [...ftsResults];\n    }\n\n    // If strategy is 'keywords' or 'combined', or if FTS returned no results, perform keyword-based search\n    if (\n      options.strategy === \"keywords\" ||\n      options.strategy === \"combined\" ||\n      (options.strategy === \"fts\" && searchResults.length === 0)\n    ) {\n      const keywordResults = await searchUsingKeywords(keywords, options);\n\n      if (options.strategy === \"combined\" && searchResults.length > 0) {\n        // Merge and deduplicate results\n        searchResults = mergeSearchResults(searchResults, keywordResults);\n      } else {\n        searchResults = keywordResults;\n      }\n    }\n\n    // Apply minimum relevance filter if specified\n    if (options.minRelevance) {\n      searchResults = searchResults.filter(\n        (result) => result.relevanceScore >= options.minRelevance\n      );\n    }\n\n    // Apply result limit if not already applied in search functions\n    if (options.limit && searchResults.length > options.limit) {\n      searchResults = searchResults.slice(0, options.limit);\n    }\n\n    // Return the search results\n    return searchResults;\n  } catch (error) {\n    console.error(\"Error in searchByKeywords:\", error);\n    throw error;\n  }\n}\n\n/**\n * Searches code entities using Full-Text Search\n *\n * @param {string[]} keywords - Keywords to search for\n * @param {SearchOptions} options - Search options\n * @returns {Promise<SearchResult[]>} Search results\n */\nasync function searchUsingFTS(keywords, options) {\n  try {\n    // Process keywords for FTS5 query\n    const processedKeywords = keywords.map((keyword) => {\n      // Apply stemming to match the behavior used when indexing content\n      const stemmed = stem(keyword.toLowerCase());\n\n      // Sanitize special characters and escape quotes for FTS\n      // Note: SQLite FTS5 has special handling for \" and other special characters\n      const sanitized = stemmed.replace(\n        /[\\\\\"\\(\\)\\[\\]\\{\\}\\^\\$\\+\\*\\?\\.]/g,\n        (char) => `\\\\${char}`\n      );\n\n      return sanitized;\n    });\n\n    // Determine boolean operator based on options or use default\n    // Default to OR for broader results, use AND for more specific matching\n    const booleanOperator =\n      options.booleanOperator?.toUpperCase() === \"AND\" ? \"AND\" : \"OR\";\n\n    // Construct the FTS query\n    let ftsQuery;\n\n    if (options.useExactMatch) {\n      // For exact phrase matching, wrap the entire phrase in quotes\n      ftsQuery = `\"${processedKeywords.join(\" \")}\"`;\n    } else if (options.useProximity && processedKeywords.length > 1) {\n      // For proximity search, use NEAR operator with optional distance\n      const distance = options.proximityDistance || 10;\n      ftsQuery = `${processedKeywords.join(` NEAR/${distance} `)}`;\n    } else {\n      // Standard boolean search\n      ftsQuery = processedKeywords.join(` ${booleanOperator} `);\n    }\n\n    // Check if the user provided explicit boolean syntax like \"library AND file OR module\"\n    // If so, respect their input instead of our processing\n    if (\n      keywords.length === 1 &&\n      /\\s+(AND|OR|NOT|NEAR\\/\\d+)\\s+/i.test(keywords[0])\n    ) {\n      ftsQuery = keywords[0];\n    }\n\n    // Start building the SQL query\n    let sql = `\n      SELECT\n        e.*,\n        fts.rank as relevance_score\n      FROM\n        code_entities_fts fts\n      JOIN\n        code_entities e ON fts.rowid = e.rowid\n      WHERE\n        fts.code_entities_fts MATCH ?\n    `;\n\n    // Array to hold query parameters\n    const queryParams = [ftsQuery];\n\n    // Apply filters from options\n    if (options.entityTypes && options.entityTypes.length > 0) {\n      const placeholders = options.entityTypes.map(() => \"?\").join(\", \");\n      sql += ` AND e.entity_type IN (${placeholders})`;\n      queryParams.push(...options.entityTypes);\n    }\n\n    // Apply file path filters with proper wildcard handling\n    if (options.filePaths && options.filePaths.length > 0) {\n      sql += \" AND (\";\n\n      const filePathConditions = [];\n\n      for (const pathPattern of options.filePaths) {\n        // Handle glob patterns by converting to SQL LIKE patterns\n        let sqlPattern = pathPattern\n          .replace(/\\*/g, \"%\") // Convert * to %\n          .replace(/\\?/g, \"_\"); // Convert ? to _\n\n        // Handle **/ pattern (recursive directory matching)\n        sqlPattern = sqlPattern.replace(/%\\/%/g, \"%\");\n\n        filePathConditions.push(\"e.file_path LIKE ?\");\n        queryParams.push(sqlPattern);\n      }\n\n      sql += filePathConditions.join(\" OR \");\n      sql += \")\";\n    }\n\n    // Apply date range filter\n    if (options.dateRange) {\n      if (options.dateRange.start) {\n        sql += \" AND e.last_modified_at >= ?\";\n        queryParams.push(options.dateRange.start.toISOString());\n      }\n\n      if (options.dateRange.end) {\n        sql += \" AND e.last_modified_at <= ?\";\n        queryParams.push(options.dateRange.end.toISOString());\n      }\n    }\n\n    // Apply custom ranking if available, otherwise use default FTS rank\n    if (options.customRanking) {\n      sql += ` ORDER BY ${options.customRanking}`;\n    } else {\n      // Enhance default ranking with optional boosts\n      sql += `\n        ORDER BY \n          relevance_score * \n          CASE \n            WHEN e.entity_type = 'file' THEN 1.2\n            WHEN e.entity_type = 'class' THEN 1.1\n            WHEN e.entity_type = 'function' THEN 1.0\n            ELSE 0.9\n          END DESC\n      `;\n    }\n\n    // Apply limit with reasonable default\n    const limit = options.limit && options.limit > 0 ? options.limit : 100;\n    sql += \" LIMIT ?\";\n    queryParams.push(limit);\n\n    // Execute the query\n    const results = await executeQuery(sql, queryParams);\n\n    // Map results to SearchResult objects\n    return mapToSearchResults(results);\n  } catch (error) {\n    console.error(\"Error in searchUsingFTS:\", error);\n    throw error;\n  }\n}\n\n/**\n * Searches code entities using the entity_keywords table\n *\n * @param {string[]} keywords - Keywords to search for\n * @param {SearchOptions} options - Search options\n * @returns {Promise<SearchResult[]>} Search results\n */\nasync function searchUsingKeywords(keywords, options) {\n  try {\n    // Handle single string input with boolean operators by splitting into individual terms\n    let processedKeywords;\n    if (keywords.length === 1 && /\\s+(AND|OR|NOT)\\s+/i.test(keywords[0])) {\n      // Split the complex query string into individual terms, ignoring operators\n      processedKeywords = keywords[0]\n        .split(/\\s+(?:AND|OR|NOT)\\s+/i)\n        .map((term) => term.trim())\n        .filter((term) => term.length > 0);\n    } else {\n      processedKeywords = keywords;\n    }\n\n    // Stem the keywords for more effective matching with the entity_keywords table\n    const stemmedKeywords = processedKeywords.map((keyword) =>\n      stem(keyword.toLowerCase())\n    );\n\n    // Use prepared statement with placeholders for security\n    let sql = `\n      SELECT \n        e.*,\n        SUM(ek.weight * (1.0 + (0.1 * count_matches))) as relevance_score\n      FROM (\n        SELECT \n          entity_id, \n          COUNT(DISTINCT keyword) as count_matches,\n          MAX(weight) as weight\n        FROM \n          entity_keywords\n        WHERE \n          keyword IN (${stemmedKeywords.map(() => \"?\").join(\",\")})\n        GROUP BY \n          entity_id\n      ) as ek\n      JOIN \n        code_entities e ON ek.entity_id = e.entity_id\n    `;\n\n    // Array to hold query parameters\n    const queryParams = [...stemmedKeywords];\n\n    // Apply filters using our updated filter function\n    sql = applyFilters(sql, options, queryParams);\n\n    // Apply ranking with type-based boosts similar to searchUsingFTS\n    if (options.sortBy) {\n      sql += ` ORDER BY e.${options.sortBy}`;\n    } else {\n      // Provide entity-type-based boosting along with the keyword match score\n      sql += `\n        ORDER BY \n          relevance_score * \n          CASE \n            WHEN e.entity_type = 'file' THEN 1.2\n            WHEN e.entity_type = 'class' THEN 1.1\n            WHEN e.entity_type = 'function' THEN 1.0\n            ELSE 0.9\n          END DESC\n      `;\n    }\n\n    // Apply limit with reasonable default\n    const limit = options.limit && options.limit > 0 ? options.limit : 100;\n    sql += \" LIMIT ?\";\n    queryParams.push(limit);\n\n    // Execute the query\n    const results = await executeQuery(sql, queryParams);\n\n    // Map results to SearchResult objects\n    return mapToSearchResults(results);\n  } catch (error) {\n    console.error(\"Error in searchUsingKeywords:\", error);\n    throw error;\n  }\n}\n\n/**\n * Apply filters from search options to SQL query\n * Note: This function is mainly used by searchUsingKeywords.\n * The searchUsingFTS function now applies filters directly for better query construction.\n *\n * @param {string} sql - SQL query to enhance\n * @param {SearchOptions} options - Search options\n * @param {Array} queryParams - Query parameters array to append to\n * @returns {string} Enhanced SQL query with filters\n */\nfunction applyFilters(sql, options, queryParams) {\n  // The provided SQL should already have a WHERE clause, so we'll use AND\n\n  // Apply entity type filters\n  if (options.entityTypes && options.entityTypes.length > 0) {\n    const placeholders = options.entityTypes.map(() => \"?\").join(\", \");\n    sql += ` AND e.entity_type IN (${placeholders})`;\n    queryParams.push(...options.entityTypes);\n  }\n\n  // Apply file path filters with proper glob pattern support\n  if (options.filePaths && options.filePaths.length > 0) {\n    sql += \" AND (\";\n\n    const filePathConditions = [];\n\n    for (const pathPattern of options.filePaths) {\n      // Handle glob patterns by converting to SQL LIKE patterns\n      let sqlPattern = pathPattern\n        .replace(/\\*/g, \"%\") // Convert * to %\n        .replace(/\\?/g, \"_\"); // Convert ? to _\n\n      // Handle **/ pattern (recursive directory matching)\n      sqlPattern = sqlPattern.replace(/%\\/%/g, \"%\");\n\n      filePathConditions.push(\"e.file_path LIKE ?\");\n      queryParams.push(sqlPattern);\n    }\n\n    sql += filePathConditions.join(\" OR \");\n    sql += \")\";\n  }\n\n  // Apply date range filter\n  if (options.dateRange) {\n    if (options.dateRange.start) {\n      sql += \" AND e.last_modified_at >= ?\";\n      queryParams.push(options.dateRange.start.toISOString());\n    }\n\n    if (options.dateRange.end) {\n      sql += \" AND e.last_modified_at <= ?\";\n      queryParams.push(options.dateRange.end.toISOString());\n    }\n  }\n\n  return sql;\n}\n\n/**\n * Map database results to SearchResult objects\n *\n * @param {Array} results - Database query results\n * @returns {Array<SearchResult>} Mapped search results\n */\nfunction mapToSearchResults(results) {\n  // Check if results has a rows property and it's an array\n  const rows =\n    results && results.rows && Array.isArray(results.rows)\n      ? results.rows\n      : Array.isArray(results)\n      ? results\n      : [];\n\n  // If no valid results, return empty array\n  if (rows.length === 0) {\n    console.warn(\"No valid search results found to map\");\n    return [];\n  }\n\n  return rows.map((row) => ({\n    entity: {\n      entity_id: row.entity_id,\n      file_path: row.file_path,\n      entity_type: row.entity_type,\n      name: row.name,\n      parent_entity_id: row.parent_entity_id,\n      content_hash: row.content_hash,\n      raw_content: row.raw_content,\n      start_line: row.start_line,\n      end_line: row.end_line,\n      language: row.language,\n      created_at: row.created_at,\n      last_modified_at: row.last_modified_at,\n    },\n    relevanceScore: row.relevance_score,\n  }));\n}\n\n/**\n * Merge and deduplicate search results from multiple sources\n *\n * @param {Array<SearchResult>} resultsA - First set of search results\n * @param {Array<SearchResult>} resultsB - Second set of search results\n * @returns {Array<SearchResult>} Merged and deduplicated results\n */\nfunction mergeSearchResults(resultsA, resultsB) {\n  // Create a map to deduplicate by entity_id\n  const entityMap = new Map();\n\n  // Process the first result set (higher priority)\n  for (const result of resultsA) {\n    entityMap.set(result.entity.entity_id, result);\n  }\n\n  // Process the second result set, only adding entities not already present\n  // or combining scores if the entity already exists\n  for (const result of resultsB) {\n    const entityId = result.entity.entity_id;\n\n    if (entityMap.has(entityId)) {\n      // Entity already exists, update relevance score\n      // Using a weighted average here, favoring FTS results\n      const existingResult = entityMap.get(entityId);\n      const combinedScore =\n        existingResult.relevanceScore * 0.7 + result.relevanceScore * 0.3;\n\n      entityMap.set(entityId, {\n        ...existingResult,\n        relevanceScore: combinedScore,\n      });\n    } else {\n      // New entity, add to results\n      entityMap.set(entityId, result);\n    }\n  }\n\n  // Convert map back to array and sort by relevance score\n  return Array.from(entityMap.values()).sort(\n    (a, b) => b.relevanceScore - a.relevanceScore\n  );\n}\n\n/**\n * Calculate a custom relevance score for an entity based on non-vector factors\n *\n * @param {CodeEntity} entity - The code entity to score\n * @param {string[]} queryKeywords - Keywords from the search query\n * @param {string[]} [focusKeywords=[]] - Keywords representing the current focus area\n * @returns {number} A relevance score between 0 and 1\n */\nexport function nonVectorRelevanceScore(\n  entity,\n  queryKeywords,\n  focusKeywords = []\n) {\n  // Ensure we have valid inputs\n  if (!entity || !queryKeywords || queryKeywords.length === 0) {\n    return 0;\n  }\n\n  // Initialize base score\n  let score = 0.5;\n\n  // Prepare keywords by stemming\n  const stemmedQueryKeywords = queryKeywords.map((kw) =>\n    stem(kw.toLowerCase())\n  );\n  const stemmedFocusKeywords = focusKeywords.map((kw) =>\n    stem(kw.toLowerCase())\n  );\n\n  // 1. Keyword Matching Score\n  const keywordMatchScore = calculateKeywordMatchScore(\n    entity,\n    stemmedQueryKeywords\n  );\n\n  // 2. Focus Area Boost\n  const focusBoost = calculateFocusAreaBoost(entity, stemmedFocusKeywords);\n\n  // 3. Recency Factor\n  const recencyFactor = calculateRecencyFactor(entity);\n\n  // 4. Importance Score Factor\n  const importanceFactor =\n    entity.importance_score !== undefined ? entity.importance_score : 0.5;\n\n  // 5. Type-Based Weighting\n  const typeWeight = calculateTypeWeight(entity);\n\n  // 6. Hierarchical Proximity (simplified first pass)\n  const hierarchyBoost = 1.0; // Default value for now, can be enhanced later\n\n  // Combine all factors with appropriate weights\n  score =\n    (keywordMatchScore * 0.35 + // 35% weight for keyword matching\n      focusBoost * 0.2 + // 20% weight for focus area boost\n      recencyFactor * 0.15 + // 15% weight for recency\n      importanceFactor * 0.2 + // 20% weight for importance\n      typeWeight * 0.1) * // 10% weight for entity type\n    hierarchyBoost; // Apply hierarchy boost as a multiplier\n\n  // Ensure score is between 0 and 1\n  return Math.max(0, Math.min(1, score));\n}\n\n/**\n * Calculate keyword matching score based on entity content and query keywords\n *\n * @param {CodeEntity} entity - The code entity\n * @param {string[]} stemmedQueryKeywords - Stemmed query keywords\n * @returns {number} Keyword match score between 0 and 1\n */\nfunction calculateKeywordMatchScore(entity, stemmedQueryKeywords) {\n  // Extract meaningful tokens from entity name and content\n  const nameTokens = tokenize(entity.name || \"\").map((token) =>\n    stem(token.toLowerCase())\n  );\n\n  // Use summary if available, otherwise use raw_content\n  const contentText = entity.summary || entity.raw_content || \"\";\n  const contentTokens = tokenize(contentText).map((token) =>\n    stem(token.toLowerCase())\n  );\n\n  // Combine unique tokens\n  const entityTokens = Array.from(new Set([...nameTokens, ...contentTokens]));\n\n  if (entityTokens.length === 0) return 0;\n\n  // Calculate matches\n  let nameMatches = 0;\n  let contentMatches = 0;\n\n  for (const queryKw of stemmedQueryKeywords) {\n    // Check for matches in name (higher importance)\n    if (nameTokens.includes(queryKw)) {\n      nameMatches++;\n    }\n    // Check for matches in content\n    else if (contentTokens.includes(queryKw)) {\n      contentMatches++;\n    }\n  }\n\n  // Calculate Jaccard index for overall similarity\n  const matchingTokens = stemmedQueryKeywords.filter((kw) =>\n    entityTokens.includes(kw)\n  ).length;\n\n  const jaccardIndex =\n    matchingTokens /\n    (entityTokens.length + stemmedQueryKeywords.length - matchingTokens);\n\n  // Calculate final keyword score with boosted name matches\n  const nameMatchScore = (nameMatches / stemmedQueryKeywords.length) * 1.5; // 50% boost for name matches\n  const contentMatchScore = contentMatches / stemmedQueryKeywords.length;\n  const overallMatchScore = jaccardIndex * 0.5; // Base similarity\n\n  return Math.min(1.0, nameMatchScore + contentMatchScore + overallMatchScore);\n}\n\n/**\n * Calculate focus area boost based on overlap with focus keywords\n *\n * @param {CodeEntity} entity - The code entity\n * @param {string[]} stemmedFocusKeywords - Stemmed focus area keywords\n * @returns {number} Focus area boost between 0 and 1\n */\nfunction calculateFocusAreaBoost(entity, stemmedFocusKeywords) {\n  if (!stemmedFocusKeywords || stemmedFocusKeywords.length === 0) {\n    return 0;\n  }\n\n  // Extract tokens from entity\n  const entityText = [\n    entity.name || \"\",\n    entity.summary || \"\",\n    entity.raw_content || \"\",\n  ].join(\" \");\n\n  const entityTokens = tokenize(entityText).map((token) =>\n    stem(token.toLowerCase())\n  );\n\n  // Count matching focus keywords\n  const matchingFocusKeywords = stemmedFocusKeywords.filter((kw) =>\n    entityTokens.includes(kw)\n  ).length;\n\n  // Calculate focus boost based on proportion of matching focus keywords\n  return matchingFocusKeywords / stemmedFocusKeywords.length;\n}\n\n/**\n * Calculate recency factor based on entity's last modified or accessed date\n *\n * @param {CodeEntity} entity - The code entity\n * @returns {number} Recency factor between 0 and 1\n */\nfunction calculateRecencyFactor(entity) {\n  // Use last_modified_at or last_accessed_at, whichever is more recent\n  const lastModified = entity.last_modified_at\n    ? new Date(entity.last_modified_at)\n    : null;\n  const lastAccessed = entity.last_accessed_at\n    ? new Date(entity.last_accessed_at)\n    : null;\n\n  if (!lastModified && !lastAccessed) {\n    return 0.5; // Default value if no dates available\n  }\n\n  // Use the most recent date\n  const mostRecentDate = !lastAccessed\n    ? lastModified\n    : !lastModified\n    ? lastAccessed\n    : lastAccessed > lastModified\n    ? lastAccessed\n    : lastModified;\n\n  const now = new Date();\n  const ageInDays = (now - mostRecentDate) / (1000 * 60 * 60 * 24);\n\n  // Exponential decay function: score = e^(-ageInDays/30)\n  // This gives a score of ~1.0 for today, ~0.37 for 30 days ago, ~0.14 for 60 days ago\n  return Math.exp(-ageInDays / 30);\n}\n\n/**\n * Calculate type-based weight for different entity types\n *\n * @param {CodeEntity} entity - The code entity\n * @returns {number} Type weight between 0 and 1\n */\nfunction calculateTypeWeight(entity) {\n  // Define weights for different entity types\n  const typeWeights = {\n    function: 0.9,\n    class: 0.9,\n    method: 0.85,\n    file: 0.8,\n    variable: 0.75,\n    comment: 0.5,\n    default: 0.7, // Default weight for unknown types\n  };\n\n  const entityType = (entity.entity_type || \"\").toLowerCase();\n  return typeWeights[entityType] || typeWeights.default;\n}\n\n/**\n * Retrieves code entities by their entity IDs\n *\n * @param {string[]} entityIds - Array of entity IDs to retrieve\n * @returns {Promise<CodeEntity[]>} Array of code entities\n */\nexport async function searchByEntityIds(entityIds) {\n  try {\n    // Validate input\n    if (!entityIds || !Array.isArray(entityIds) || entityIds.length === 0) {\n      throw new Error(\"Entity IDs array is required and cannot be empty\");\n    }\n\n    // Create placeholders for the IN clause\n    const placeholders = entityIds.map(() => \"?\").join(\", \");\n\n    // Build and execute the query\n    const sql = `\n      SELECT * FROM code_entities\n      WHERE entity_id IN (${placeholders})\n    `;\n\n    const results = await executeQuery(sql, entityIds);\n\n    // Return the raw entity objects\n    return results;\n  } catch (error) {\n    console.error(\"Error in searchByEntityIds:\", error);\n    throw error;\n  }\n}\n", "/**\n * ActiveContextManager.js\n *\n * Manages the \"Active Context\" in-memory for the current conversation.\n * Provides functions to get, set, and manipulate the entities and focus\n * that are currently active in the developer's context.\n */\n\nimport { executeQuery } from \"../db.js\";\nimport * as ContextPrioritizerLogic from \"./ContextPrioritizerLogic.js\";\nimport * as ContextCompressorLogic from \"./ContextCompressorLogic.js\";\n\n/**\n * @typedef {Object} FocusArea\n * @property {string} focus_id - Unique identifier for the focus area\n * @property {string} focus_type - Type of focus area ('file', 'directory', 'task_type')\n * @property {string} identifier - Primary identifier for the focus area (e.g., file path)\n * @property {string} description - Human-readable description of the focus area\n * @property {string[]} related_entity_ids - Array of related entity IDs\n * @property {string[]} keywords - Array of keywords related to this focus area\n * @property {number} last_activated_at - Timestamp when this focus area was last active\n * @property {boolean} is_active - Whether this focus area is currently active\n */\n\n/**\n * @typedef {Object} CodeEntity\n * @property {string} id - Unique identifier for the code entity\n * @property {string} path - File path of the code entity\n * @property {string} type - Type of code entity ('file', 'function', 'class', etc.)\n * @property {string} name - Name of the code entity\n * @property {string} content - Content of the code entity\n * @property {string} symbol_path - Full symbol path of the entity\n * @property {number} version - Version number of the entity\n * @property {string} parent_id - ID of the parent entity, if any\n * @property {string} created_at - Timestamp when entity was created\n * @property {string} updated_at - Timestamp when entity was last updated\n */\n\n/**\n * @typedef {Object} Snippet\n * @property {string} entity_id - ID of the entity\n * @property {string} summarizedContent - Compressed/summarized content\n * @property {number} originalScore - Original relevance score\n * @property {string} type - Type of snippet\n */\n\n// Module-scoped state variables\n/**\n * Set of entity IDs currently in active context\n * @type {Set<string>}\n */\nconst activeEntityIds = new Set();\n\n/**\n * Current focus area\n * @type {FocusArea|null}\n */\nlet activeFocus = null;\n\n/**\n * History of context changes for short-term memory\n * @type {Array<{timestamp: number, added?: string[], removed?: string[]}>}\n */\nconst contextHistory = [];\n\n/**\n * Returns the current active focus area\n *\n * @returns {FocusArea|null} The current focus area or null if no focus is set\n */\nexport function getActiveFocus() {\n  return activeFocus;\n}\n\n/**\n * Sets the active focus area and optionally adds related entity IDs to active context\n *\n * @param {FocusArea} focus - The focus area to set as active\n */\nexport function setActiveFocus(focus) {\n  activeFocus = focus;\n\n  // If focus has related entity IDs, add them to active context\n  if (focus && Array.isArray(focus.related_entity_ids)) {\n    updateActiveContext(focus.related_entity_ids, []);\n  }\n}\n\n/**\n * Updates the active context by adding and removing entity IDs\n *\n * @param {string[]} addEntityIds - Array of entity IDs to add to active context\n * @param {string[]} removeEntityIds - Array of entity IDs to remove from active context\n */\nexport function updateActiveContext(addEntityIds = [], removeEntityIds = []) {\n  const changeRecord = {\n    timestamp: Date.now(),\n  };\n\n  // Add entities to active context\n  if (addEntityIds.length > 0) {\n    addEntityIds.forEach((id) => activeEntityIds.add(id));\n    changeRecord.added = [...addEntityIds];\n  }\n\n  // Remove entities from active context\n  if (removeEntityIds.length > 0) {\n    removeEntityIds.forEach((id) => activeEntityIds.delete(id));\n    changeRecord.removed = [...removeEntityIds];\n  }\n\n  // Record this change in history if anything changed\n  if (addEntityIds.length > 0 || removeEntityIds.length > 0) {\n    contextHistory.push(changeRecord);\n\n    // Limit history size (keep last 50 changes)\n    if (contextHistory.length > 50) {\n      contextHistory.shift();\n    }\n  }\n}\n\n/**\n * Returns all entity IDs in the active context\n *\n * @returns {string[]} Array of active entity IDs\n */\nexport function getActiveContextEntityIds() {\n  return [...activeEntityIds];\n}\n\n/**\n * Clears the active context by resetting all state variables\n */\nexport function clearActiveContext() {\n  activeEntityIds.clear();\n  activeFocus = null;\n\n  // Record this change in history\n  contextHistory.push({\n    timestamp: Date.now(),\n    event: \"clear_context\",\n  });\n}\n\n/**\n * Returns the active context history\n * Used for debugging and analytics purposes\n *\n * @returns {Array} The context history array\n */\nexport function getContextHistory() {\n  return [...contextHistory];\n}\n\n/**\n * Retrieves full entity details for all active context items from the database\n *\n * @returns {Promise<CodeEntity[]>} Array of code entity objects\n */\nexport async function getActiveContextAsEntities() {\n  // Get current active entity IDs\n  const entityIds = getActiveContextEntityIds();\n\n  // If no active entities, return empty array\n  if (entityIds.length === 0) {\n    return [];\n  }\n\n  try {\n    // Construct placeholders for SQL query\n    const placeholders = entityIds.map(() => \"?\").join(\",\");\n\n    // Construct and execute SQL query\n    const query = `SELECT * FROM code_entities WHERE id IN (${placeholders})`;\n    const entities = await executeQuery(query, entityIds);\n\n    return entities;\n  } catch (error) {\n    console.error(\"Error retrieving active context entities:\", error);\n    // Return empty array in case of error\n    return [];\n  }\n}\n\n/**\n * Retrieves prioritized and compressed snippets of the active context\n *\n * @param {any} prioritizerLogic - Logic module for prioritizing context items\n * @param {any} compressorLogic - Logic module for compressing content\n * @param {any} db - Database access module\n * @param {number} tokenBudget - Maximum number of tokens to include\n * @param {string[]} [queryKeywords] - Optional keywords to prioritize content\n * @returns {Promise<Snippet[]>} Array of prioritized and compressed context snippets\n */\nexport async function getActiveContextAsSnippets(\n  prioritizerLogic = ContextPrioritizerLogic,\n  compressorLogic = ContextCompressorLogic,\n  db = { executeQuery },\n  tokenBudget = 2000,\n  queryKeywords = []\n) {\n  try {\n    // 1. Get active entities\n    const activeEntities = await getActiveContextAsEntities();\n\n    // 2. If no active entities, return empty array\n    if (!activeEntities || activeEntities.length === 0) {\n      return [];\n    }\n\n    // 3. Convert entities to ContextSnippet format for prioritization\n    const contextSnippets = activeEntities.map((entity) => {\n      // Get recency information from context history\n      const recencyFactor = _calculateRecencyFactor(entity.id);\n\n      return {\n        entity_id: entity.id,\n        content: entity.content,\n        type: entity.type,\n        path: entity.path,\n        name: entity.name,\n        baseRelevance: 0.5 + recencyFactor, // Base score plus recency boost\n        metadata: {\n          symbolPath: entity.symbol_path,\n          parentId: entity.parent_id,\n          version: entity.version,\n        },\n      };\n    });\n\n    // 4. Get current focus for prioritization\n    const currentFocus = getActiveFocus();\n\n    // 5. Prioritize the context snippets\n    const prioritizedSnippets = await prioritizerLogic.prioritizeContexts(\n      contextSnippets,\n      queryKeywords,\n      currentFocus,\n      Math.max(50, activeEntities.length * 2) // Higher limit to prioritize from\n    );\n\n    // 6. Compress the prioritized snippets to fit within token budget\n    const compressedSnippets = await compressorLogic.manageTokenBudget(\n      prioritizedSnippets,\n      tokenBudget,\n      queryKeywords\n    );\n\n    // 7. Map the compressed snippets to the expected Snippet format\n    return compressedSnippets.map((snippet) => ({\n      entity_id: snippet.entity_id,\n      summarizedContent: snippet.processedContent || snippet.content,\n      originalScore: snippet.relevanceScore || snippet.baseRelevance,\n      type: snippet.type,\n    }));\n  } catch (error) {\n    console.error(\"Error generating context snippets:\", error);\n    return [];\n  }\n}\n\n/**\n * Calculate a recency factor for an entity based on context history\n *\n * @private\n * @param {string} entityId - The entity ID to check\n * @returns {number} A recency factor between 0 and 0.5\n */\nfunction _calculateRecencyFactor(entityId) {\n  // Start from the most recent history entries\n  for (let i = contextHistory.length - 1; i >= 0; i--) {\n    const record = contextHistory[i];\n\n    // If this entity was recently added, give it a boost\n    if (record.added && record.added.includes(entityId)) {\n      // Calculate how recent this addition was (0 = newest, 1 = oldest)\n      const recencyIndex =\n        (contextHistory.length - 1 - i) / contextHistory.length;\n      // Convert to a score boost between 0.1 and 0.5 (newer = higher boost)\n      return 0.5 - recencyIndex * 0.4;\n    }\n  }\n\n  // Default recency factor if not found in history\n  return 0.1;\n}\n\n/**\n * Returns a complete snapshot of the current active context state\n *\n * @returns {Promise<Object>} Object containing the current active context state\n */\nexport async function getActiveContextState() {\n  try {\n    // Get current active entities\n    const entities = await getActiveContextAsEntities();\n\n    // Get current focus\n    const focus = getActiveFocus();\n\n    // Get recent context history (last 10 changes)\n    const recentHistory = contextHistory.slice(-10);\n\n    // Create and return the context state\n    return {\n      activeEntityIds: [...activeEntityIds],\n      activeFocus: focus,\n      entities,\n      recentChanges: recentHistory,\n      timestamp: Date.now(),\n    };\n  } catch (error) {\n    console.error(\"Error getting active context state:\", error);\n    // Return basic state in case of error\n    return {\n      activeEntityIds: [...activeEntityIds],\n      activeFocus: activeFocus,\n      entities: [],\n      recentChanges: [],\n      timestamp: Date.now(),\n      error: error.message,\n    };\n  }\n}\n\n/**\n * Updates the focus based on code changes\n *\n * @param {Array<{entityId: string, changeType: string, content: string}>} codeChanges - Array of code change objects\n * @returns {Promise<{updatedFocus: FocusArea|null, addedEntities: string[], removedEntities: string[]}>}\n */\nexport async function updateFocusWithCodeChanges(codeChanges) {\n  try {\n    if (\n      !codeChanges ||\n      !Array.isArray(codeChanges) ||\n      codeChanges.length === 0\n    ) {\n      return {\n        updatedFocus: activeFocus,\n        addedEntities: [],\n        removedEntities: [],\n      };\n    }\n\n    // Track entities to add and remove\n    const entitiesToAdd = new Set();\n    const entitiesToRemove = new Set();\n\n    // Process each code change\n    for (const change of codeChanges) {\n      const { entityId, changeType } = change;\n\n      if (changeType === \"delete\") {\n        // If entity is deleted, remove it from active context\n        entitiesToRemove.add(entityId);\n      } else {\n        // For additions or modifications, add to active context\n        entitiesToAdd.add(entityId);\n      }\n    }\n\n    // Handle focus changes based on the most significant code change\n    let updatedFocus = activeFocus;\n\n    // If there are significant changes, potentially update the focus\n    if (codeChanges.length > 0) {\n      // Use the first changed file as a potential new focus\n      // A more sophisticated implementation would analyze the changes\n      // to determine the most important one\n      const primaryChange = codeChanges[0];\n\n      if (primaryChange.changeType !== \"delete\") {\n        // Query for more info about this entity\n        const query = `SELECT * FROM code_entities WHERE id = ?`;\n        const entityResults = await executeQuery(query, [\n          primaryChange.entityId,\n        ]);\n\n        if (entityResults.length > 0) {\n          const entity = entityResults[0];\n\n          // Determine whether to update focus\n          const shouldUpdateFocus =\n            // If no current focus\n            !activeFocus ||\n            // Or current focus is less specific than this entity\n            (entity.type === \"function\" && activeFocus.focus_type === \"file\") ||\n            // Or significant changes to the current focus\n            (activeFocus.related_entity_ids &&\n              activeFocus.related_entity_ids.includes(primaryChange.entityId) &&\n              primaryChange.changeType === \"modify\");\n\n          if (shouldUpdateFocus) {\n            // Create a new focus area based on the changed entity\n            updatedFocus = {\n              focus_id: entity.id,\n              focus_type: entity.type,\n              identifier: entity.path || entity.name,\n              description: `Focus on ${entity.type}: ${entity.name}`,\n              related_entity_ids: [entity.id],\n              keywords: [], // Would be filled with keywords from the entity\n              last_activated_at: Date.now(),\n              is_active: true,\n            };\n\n            // Update the active focus\n            setActiveFocus(updatedFocus);\n          }\n        }\n      }\n    }\n\n    // Update active context with added and removed entities\n    const addedEntities = [...entitiesToAdd];\n    const removedEntities = [...entitiesToRemove];\n\n    // Don't add entities that are being removed\n    const filteredAdded = addedEntities.filter(\n      (id) => !entitiesToRemove.has(id)\n    );\n\n    // Update the active context\n    updateActiveContext(filteredAdded, removedEntities);\n\n    return {\n      updatedFocus,\n      addedEntities: filteredAdded,\n      removedEntities,\n    };\n  } catch (error) {\n    console.error(\"Error updating focus with code changes:\", error);\n    return {\n      updatedFocus: activeFocus,\n      addedEntities: [],\n      removedEntities: [],\n      error: error.message,\n    };\n  }\n}\n", "/**\n * ContextPrioritizerLogic.js\n *\n * Logic for prioritizing and scoring context snippets based on\n * query relevance, focus area alignment, recency, and relationships.\n */\n\nimport { nonVectorRelevanceScore } from \"./SmartSearchServiceLogic.js\";\nimport { getRelatedEntities } from \"./RelationshipContextManagerLogic.js\";\nimport { executeQuery } from \"../db.js\";\nimport { CONTEXT_DECAY_RATE } from \"../config.js\";\n\n/**\n * @typedef {Object} CodeEntity\n * @property {string} entity_id - Unique identifier for the code entity\n * @property {string} file_path - Path to the file containing the entity\n * @property {string} entity_type - Type of code entity (e.g., 'file', 'function', 'class')\n * @property {string} name - Name of the code entity\n * @property {string} [parent_entity_id] - ID of the parent entity (if any)\n * @property {string} [content_hash] - Hash of the entity content\n * @property {string} [raw_content] - Raw content of the entity\n * @property {number} [start_line] - Start line of the entity within the file\n * @property {number} [end_line] - End line of the entity within the file\n * @property {string} [language] - Programming language of the entity\n * @property {Date} [created_at] - Creation timestamp\n * @property {Date} [last_modified_at] - Last modification timestamp\n * @property {Date} [last_accessed_at] - Last access timestamp\n * @property {number} [importance_score] - Predefined importance score\n */\n\n/**\n * @typedef {Object} ContextSnippet\n * @property {CodeEntity} entity - The code entity\n * @property {number} baseRelevance - Base relevance score from initial search\n */\n\n/**\n * @typedef {Object} FocusArea\n * @property {string} focus_id - Unique identifier for the focus area\n * @property {string} focus_type - Type of focus (e.g., 'file', 'function', 'task')\n * @property {string} identifier - Human-readable identifier\n * @property {string} description - Description of the focus area\n * @property {string[]} related_entity_ids - IDs of entities related to this focus\n * @property {string[]} keywords - Keywords associated with this focus area\n */\n\n/**\n * @typedef {Object} RecencyInfo\n * @property {Date} lastAccessedThreshold - Threshold for considering entities as recently accessed\n * @property {Date} [lastModifiedThreshold] - Threshold for considering entities as recently modified\n * @property {number} [recencyBoostFactor] - Factor to boost score for recent entities (default: 1.25)\n */\n\n/**\n * Score a context snippet based on multiple relevance factors\n *\n * @param {ContextSnippet} snippet - The context snippet to score\n * @param {string[]} queryKeywords - Keywords from the current query\n * @param {FocusArea} currentFocus - Current focus area\n * @param {RecencyInfo} recencyData - Information about recency thresholds\n * @returns {number} Final relevance score\n */\nexport async function scoreContextSnippet(\n  snippet,\n  queryKeywords,\n  currentFocus,\n  recencyData\n) {\n  try {\n    // Define weights for different scoring factors\n    const weights = {\n      baseRelevance: 0.2, // 20% weight for initial relevance\n      queryRelevance: 0.25, // 25% weight for query matching\n      focusAlignment: 0.25, // 25% weight for focus area alignment (reduced from 30%)\n      recency: 0.15, // 15% weight for recency\n      entityType: 0.05, // 5% weight for entity type priority\n      relationshipProximity: 0.1, // 10% weight for relationship proximity (increased from 5%)\n    };\n\n    // 1. Query Relevance\n    const queryRelevanceScore = nonVectorRelevanceScore(\n      snippet.entity,\n      queryKeywords,\n      currentFocus.keywords\n    );\n\n    // 2. Focus Alignment\n    const focusAlignmentScore = calculateFocusAlignmentScore(\n      snippet.entity,\n      currentFocus\n    );\n\n    // 3. Recency\n    const recencyScore = calculateRecencyScore(snippet.entity, recencyData);\n\n    // 4. Entity Type Priority\n    const entityTypePriorityScore = calculateEntityTypePriorityScore(\n      snippet.entity,\n      currentFocus.focus_type\n    );\n\n    // 5. Relationship Proximity (async)\n    const relationshipProximityScore =\n      await calculateRelationshipProximityScore(\n        snippet.entity,\n        currentFocus.related_entity_ids\n      );\n\n    // Combine all factors into a weighted score\n    const finalScore =\n      snippet.baseRelevance * weights.baseRelevance +\n      queryRelevanceScore * weights.queryRelevance +\n      focusAlignmentScore * weights.focusAlignment +\n      recencyScore * weights.recency +\n      entityTypePriorityScore * weights.entityType +\n      relationshipProximityScore * weights.relationshipProximity;\n\n    // Ensure score is between 0 and 1\n    return Math.max(0, Math.min(1, finalScore));\n  } catch (error) {\n    console.error(\"Error scoring context snippet:\", error);\n    // Fall back to base relevance in case of error\n    return snippet.baseRelevance;\n  }\n}\n\n/**\n * Calculate focus alignment score based on the relationship between\n * the entity and the current focus area\n *\n * @param {CodeEntity} entity - The code entity\n * @param {FocusArea} focus - Current focus area\n * @returns {number} Focus alignment score between 0 and 1\n */\nfunction calculateFocusAlignmentScore(entity, focus) {\n  // Highest score if the entity is directly in the focus area's related entities\n  if (focus.related_entity_ids.includes(entity.entity_id)) {\n    return 1.0;\n  }\n\n  // Check parent relationship - high score if parent is in focus\n  if (\n    entity.parent_entity_id &&\n    focus.related_entity_ids.includes(entity.parent_entity_id)\n  ) {\n    return 0.9;\n  }\n\n  // Check if the entity is from the same file as the focus\n  const focusEntityPaths = focus.related_entity_ids.map((id) => {\n    // This is a simplified approach - in practice, you would look up the entity\n    // path from the database or another data structure\n    return id.split(\":\")[0]; // Assuming ID format includes file path\n  });\n\n  if (\n    entity.file_path &&\n    focusEntityPaths.some((path) => entity.file_path.startsWith(path))\n  ) {\n    return 0.7;\n  }\n\n  // Check keyword overlap\n  if (focus.keywords && focus.keywords.length > 0) {\n    const entityText = [entity.name || \"\", entity.raw_content || \"\"]\n      .join(\" \")\n      .toLowerCase();\n\n    const matchingKeywords = focus.keywords.filter((keyword) =>\n      entityText.includes(keyword.toLowerCase())\n    );\n\n    if (matchingKeywords.length > 0) {\n      return 0.5 * (matchingKeywords.length / focus.keywords.length);\n    }\n  }\n\n  // Minimal focus alignment\n  return 0.1;\n}\n\n/**\n * Calculate recency score based on when the entity was last accessed or modified\n *\n * @param {CodeEntity} entity - The code entity\n * @param {RecencyInfo} recencyData - Information about recency thresholds\n * @returns {number} Recency score between 0 and 1\n */\nfunction calculateRecencyScore(entity, recencyData) {\n  const {\n    lastAccessedThreshold,\n    lastModifiedThreshold,\n    recencyBoostFactor = 1.25,\n  } = recencyData;\n  let recencyScore = 0.5; // Default medium score\n\n  // Check if the entity has been accessed recently\n  if (entity.last_accessed_at) {\n    const lastAccessed = new Date(entity.last_accessed_at);\n    if (lastAccessed >= lastAccessedThreshold) {\n      recencyScore = 0.8; // High score for recently accessed entities\n    }\n  }\n\n  // Check if the entity has been modified recently (this takes precedence)\n  if (entity.last_modified_at && lastModifiedThreshold) {\n    const lastModified = new Date(entity.last_modified_at);\n    if (lastModified >= lastModifiedThreshold) {\n      recencyScore = 1.0; // Maximum score for recently modified entities\n    }\n  }\n\n  // Apply recency decay based on time since last access/modification\n  if (entity.last_accessed_at || entity.last_modified_at) {\n    const lastTimepoint = entity.last_modified_at || entity.last_accessed_at;\n    const lastTime = new Date(lastTimepoint);\n    const now = new Date();\n    const daysSince = (now - lastTime) / (1000 * 60 * 60 * 24);\n\n    // Exponential decay: score = baseScore * e^(-daysSince/60)\n    // This gives a decay to ~37% of original value after 60 days\n    const decayFactor = Math.exp(-daysSince / 60);\n    recencyScore *= decayFactor;\n  }\n\n  return recencyScore;\n}\n\n/**\n * Calculate entity type priority score based on entity type and current focus\n *\n * @param {CodeEntity} entity - The code entity\n * @param {string} focusType - Type of the current focus\n * @returns {number} Entity type priority score between 0 and 1\n */\nfunction calculateEntityTypePriorityScore(entity, focusType) {\n  // Base priorities for different entity types\n  const typePriorities = {\n    function: 0.9,\n    class: 0.9,\n    method: 0.85,\n    file: 0.8,\n    variable: 0.7,\n    comment: 0.5,\n    default: 0.6,\n  };\n\n  // Get base priority for this entity type\n  const entityType = (entity.entity_type || \"\").toLowerCase();\n  let typePriority = typePriorities[entityType] || typePriorities.default;\n\n  // Boost priority if the entity type matches the focus type\n  if (entityType === focusType.toLowerCase()) {\n    typePriority = Math.min(1.0, typePriority * 1.2);\n  }\n\n  // Additional context-based adjustments could be added here\n  // For example, if working on a bug fix, error handling code might get a boost\n\n  return typePriority;\n}\n\n/**\n * Calculate relationship proximity score based on relationship to focus entities\n *\n * @param {CodeEntity} entity - The code entity\n * @param {string[]} focusEntityIds - IDs of entities in the current focus\n * @returns {number} Relationship proximity score between 0 and 1\n */\nasync function calculateRelationshipProximityScore(entity, focusEntityIds) {\n  // Return default score if no entity ID or no focus entities\n  if (!entity.entity_id || !focusEntityIds || focusEntityIds.length === 0) {\n    return 0.5; // Default score if there are no focus entities\n  }\n\n  try {\n    // Import necessary modules\n    const { getRelationships, findCodePaths } = await import(\n      \"./RelationshipContextManagerLogic.js\"\n    );\n    const LRUCache = (await import(\"../utils/lru-cache.js\")).default;\n\n    // Create or get the relationship cache (static cache shared across function calls)\n    if (!calculateRelationshipProximityScore.cache) {\n      calculateRelationshipProximityScore.cache = new LRUCache(100); // Cache up to 100 relationship lookups\n    }\n    const cache = calculateRelationshipProximityScore.cache;\n\n    // Check if we have this relationship calculation cached\n    const cacheKey = `${entity.entity_id}:${focusEntityIds.join(\",\")}`;\n    const cachedScore = cache.get(cacheKey);\n    if (cachedScore !== null) {\n      return cachedScore;\n    }\n\n    // Define weights for different relationship types\n    const relationshipTypeWeights = {\n      calls: 1.0, // Direct function calls are very relevant\n      extends: 0.9, // Class inheritance is highly relevant\n      implements: 0.9, // Interface implementation is highly relevant\n      imports: 0.8, // Import relationship is fairly relevant\n      references: 0.7, // References relationship is somewhat relevant\n      depends_on: 0.7, // Dependencies are somewhat relevant\n      contains: 0.6, // Containment is moderately relevant\n      references_variable: 0.5, // Variable references are less relevant\n      default: 0.5, // Default weight for other types\n    };\n\n    // Collect metrics to calculate the final score\n    let totalScore = 0;\n    let relationshipCount = 0;\n    let hasDirectRelationship = false;\n    let hasSecondDegreeRelationship = false;\n\n    // Check for direct (1st-degree) relationships\n    const firstDegreeRelationships = await getRelationships(\n      entity.entity_id,\n      \"both\", // Get both incoming and outgoing relationships\n      [] // All relationship types\n    );\n\n    if (firstDegreeRelationships.length === 0) {\n      // Store in cache and return slightly below default if no relationships exist\n      const score = 0.4;\n      cache.put(cacheKey, score);\n      return score;\n    }\n\n    // Process direct relationships with focus entities\n    const directRelationshipsWithFocus = firstDegreeRelationships.filter(\n      (rel) => {\n        const otherEntityId =\n          rel.source_entity_id === entity.entity_id\n            ? rel.target_entity_id\n            : rel.source_entity_id;\n        return focusEntityIds.includes(otherEntityId);\n      }\n    );\n\n    if (directRelationshipsWithFocus.length > 0) {\n      hasDirectRelationship = true;\n\n      // Calculate score based on relationship types and weights\n      for (const rel of directRelationshipsWithFocus) {\n        const relType = rel.relationship_type;\n        const weight =\n          relationshipTypeWeights[relType] || relationshipTypeWeights.default;\n\n        // Direction matters - outgoing relationships (entity calls/uses focus) are slightly more relevant\n        const directionMultiplier =\n          rel.source_entity_id === entity.entity_id ? 1.0 : 0.9;\n\n        totalScore += weight * directionMultiplier;\n        relationshipCount++;\n      }\n    }\n\n    // Check for 2nd-degree relationships (only if we don't have strong direct relationships)\n    // This is more expensive, so we limit it\n    if (\n      !hasDirectRelationship ||\n      (hasDirectRelationship && directRelationshipsWithFocus.length < 2)\n    ) {\n      // Get all entities related to our entity (1st degree connections)\n      const connectedEntityIds = firstDegreeRelationships.map((rel) =>\n        rel.source_entity_id === entity.entity_id\n          ? rel.target_entity_id\n          : rel.source_entity_id\n      );\n\n      // Check if any focus entity is connected to any of our 1st degree connections\n      // We're limiting this to 5 entities to avoid expensive queries\n      const focusEntitiesToCheck = focusEntityIds.slice(0, 5);\n      const secondDegreeConnectionPromises = [];\n\n      // For each focus entity, check if it has connections to any of our 1st degree connections\n      for (const focusEntityId of focusEntitiesToCheck) {\n        // Skip focus entities that already have direct connections\n        if (\n          directRelationshipsWithFocus.some(\n            (rel) =>\n              rel.source_entity_id === focusEntityId ||\n              rel.target_entity_id === focusEntityId\n          )\n        ) {\n          continue;\n        }\n\n        const promise = getRelationships(focusEntityId, \"both\", []).then(\n          (focusRelationships) => {\n            const secondDegreeConnections = focusRelationships.filter((rel) => {\n              const otherEntityId =\n                rel.source_entity_id === focusEntityId\n                  ? rel.target_entity_id\n                  : rel.source_entity_id;\n              return connectedEntityIds.includes(otherEntityId);\n            });\n\n            if (secondDegreeConnections.length > 0) {\n              hasSecondDegreeRelationship = true;\n\n              // Second-degree relationships are less valuable, so we apply a discount\n              for (const rel of secondDegreeConnections) {\n                const relType = rel.relationship_type;\n                const weight =\n                  relationshipTypeWeights[relType] ||\n                  relationshipTypeWeights.default;\n\n                // Second-degree connections are worth less\n                totalScore += weight * 0.5;\n                relationshipCount++;\n              }\n            }\n          }\n        );\n\n        secondDegreeConnectionPromises.push(promise);\n      }\n\n      // Wait for all second-degree connection checks to complete\n      await Promise.all(secondDegreeConnectionPromises);\n    }\n\n    // For exceptional cases, try to find paths between the entity and important focus entities\n    // This is expensive, so we only do it for a limited number of focus entities and when we don't have many direct relationships\n    if (\n      (!hasDirectRelationship && !hasSecondDegreeRelationship) ||\n      relationshipCount < 2\n    ) {\n      // Only consider the first 2 focus entities for this expensive operation\n      const importantFocusEntities = focusEntityIds.slice(0, 2);\n\n      for (const focusEntityId of importantFocusEntities) {\n        // Try to find paths up to 3 hops away (this can be expensive)\n        try {\n          // Look for important relationship types\n          for (const relType of [\"calls\", \"extends\", \"implements\", \"imports\"]) {\n            const paths = await findCodePaths(\n              entity.entity_id,\n              focusEntityId,\n              relType\n            );\n\n            if (paths.length > 0) {\n              hasSecondDegreeRelationship = true;\n\n              // For each path, calculate a score based on path length\n              for (const path of paths) {\n                const pathLength = path.length;\n                if (pathLength <= 4) {\n                  // Only consider relatively short paths\n                  const pathScore =\n                    relationshipTypeWeights[relType] * (1 / pathLength);\n                  totalScore += pathScore;\n                  relationshipCount++;\n                }\n              }\n\n              // If we found paths, no need to check other relationship types\n              break;\n            }\n          }\n        } catch (error) {\n          console.error(\n            `Error finding code paths for entity ${entity.entity_id}:`,\n            error\n          );\n          // Continue processing other focus entities\n        }\n      }\n    }\n\n    // Calculate final score\n    let finalScore;\n\n    if (relationshipCount === 0) {\n      // No relationships found, return below default\n      finalScore = 0.45;\n    } else {\n      // Normalize the score\n      let normalizedScore = totalScore / relationshipCount;\n\n      // Apply bonuses for direct and indirect relationships\n      if (hasDirectRelationship) {\n        normalizedScore *= 1.2; // 20% boost for direct relationships\n      }\n      if (hasSecondDegreeRelationship) {\n        normalizedScore *= 1.1; // 10% boost for second-degree relationships\n      }\n\n      // Ensure score is between 0 and 1\n      finalScore = Math.min(1.0, normalizedScore);\n    }\n\n    // Cache the result\n    cache.put(cacheKey, finalScore);\n\n    return finalScore;\n  } catch (error) {\n    console.error(\"Error calculating relationship proximity:\", error);\n    return 0.5; // Default score in case of error\n  }\n}\n\n/**\n * Prioritize context snippets based on relevance to query and current focus\n *\n * @param {ContextSnippet[]} contexts - Array of context snippets to prioritize\n * @param {string[]} queryKeywords - Keywords from the current query\n * @param {FocusArea} currentFocus - Current focus area\n * @param {number} maxResults - Maximum number of results to return\n * @returns {Promise<ContextSnippet[]>} Prioritized context snippets\n */\nexport async function prioritizeContexts(\n  contexts,\n  queryKeywords,\n  currentFocus,\n  maxResults\n) {\n  // Create recencyData with default thresholds\n  const recencyData = {\n    lastAccessedThreshold: new Date(Date.now() - 24 * 60 * 60 * 1000), // 24 hours ago\n    lastModifiedThreshold: new Date(Date.now() - 7 * 24 * 60 * 60 * 1000), // 7 days ago\n    recencyBoostFactor: 1.25,\n  };\n\n  // Score each context snippet\n  const scoredContexts = [];\n\n  for (const snippet of contexts) {\n    try {\n      // Score the snippet\n      const finalScore = await scoreContextSnippet(\n        snippet,\n        queryKeywords,\n        currentFocus,\n        recencyData\n      );\n\n      // Add the score to the snippet object\n      scoredContexts.push({\n        ...snippet,\n        finalScore,\n      });\n    } catch (error) {\n      console.error(`Error scoring context snippet: ${error.message}`);\n      // Include the snippet with its base relevance as fallback\n      scoredContexts.push({\n        ...snippet,\n        finalScore: snippet.baseRelevance || 0,\n      });\n    }\n  }\n\n  // Sort contexts by finalScore in descending order\n  scoredContexts.sort((a, b) => b.finalScore - a.finalScore);\n\n  // Return top maxResults\n  return scoredContexts.slice(0, maxResults);\n}\n\n/**\n * Apply decay to importance scores of all entities that haven't been\n * accessed recently to reflect diminishing relevance over time\n *\n * @returns {Promise<void>}\n */\nexport async function applyDecayToAll() {\n  try {\n    // Define the minimum threshold to prevent scores from becoming too small\n    const MIN_IMPORTANCE_THRESHOLD = 0.1;\n\n    // Define the access threshold (entities not accessed in the last 30 days)\n    const accessThreshold = new Date(Date.now() - 30 * 24 * 60 * 60 * 1000);\n\n    // Construct and execute the SQL query to apply decay\n    const query = `\n      UPDATE code_entities \n      SET importance_score = importance_score * ? \n      WHERE last_accessed_at < ? \n      AND importance_score > ?\n    `;\n\n    const params = [\n      CONTEXT_DECAY_RATE,\n      accessThreshold.toISOString(),\n      MIN_IMPORTANCE_THRESHOLD,\n    ];\n\n    // Execute the query\n    const result = await executeQuery(query, params);\n\n    console.log(`Applied decay to ${result.changes || 0} entities`);\n  } catch (error) {\n    console.error(\"Error applying decay to importance scores:\", error);\n    throw error;\n  }\n}\n", "/**\n * GlobalPatternRepository.js\n *\n * Manages global patterns that are available across all sessions.\n * Provides functionality to store, retrieve, and manage patterns in the global repository.\n */\n\nimport { executeQuery } from \"../db.js\";\nimport { v4 as uuidv4 } from \"uuid\";\n\n/**\n * @typedef {Object} PatternDefinition\n * @property {string} pattern_type - Type of the pattern\n * @property {string} [name] - Human-readable name for the pattern\n * @property {string} [description] - Description of what the pattern represents\n * @property {string} representation - Textual or structured representation of the pattern\n * @property {string|Object} [detection_rules] - Rules used to detect this pattern\n * @property {string} [language] - Programming language this pattern applies to (e.g., 'javascript', 'python', or 'any' for language-agnostic patterns)\n */\n\n/**\n * @typedef {Object} Pattern\n * @property {string} pattern_id - Unique identifier for the pattern\n * @property {string} pattern_type - Type of the pattern\n * @property {string} name - Human-readable name for the pattern\n * @property {string} description - Description of what the pattern represents\n * @property {string} representation - Textual or structured representation of the pattern\n * @property {string} detection_rules - JSON string of rules used to detect this pattern\n * @property {string} language - Programming language this pattern applies to (e.g., 'javascript', 'python', or 'any' for language-agnostic patterns)\n * @property {number} frequency - How often this pattern has been observed\n * @property {number} utility_score - How useful this pattern is rated\n * @property {number} confidence_score - Confidence in this pattern's correctness\n * @property {number} reinforcement_count - How many times this pattern has been reinforced\n * @property {boolean} is_global - Whether this pattern is global across sessions\n * @property {string} created_at - When this pattern was created\n * @property {string} updated_at - When this pattern was last updated\n */\n\n/**\n * Stores a pattern in the global pattern repository\n *\n * @param {PatternDefinition} patternDefinition - Definition of the pattern to store\n * @param {number} [confidenceScore=0.5] - Confidence score for this pattern (0-1)\n * @returns {Promise<string>} The ID of the newly stored global pattern\n */\nexport async function storeGlobalPattern(\n  patternDefinition,\n  confidenceScore = 0.5\n) {\n  try {\n    // 1. Generate a unique ID for the pattern\n    const pattern_id = uuidv4();\n\n    // 2. Extract and prepare pattern data with defaults\n    const {\n      pattern_type,\n      name = `Global_Pattern_${pattern_id.substring(0, 8)}`,\n      description = \"Globally recognized pattern\",\n      representation,\n      detection_rules = \"{}\",\n      language = \"any\",\n    } = patternDefinition;\n\n    // 3. Ensure representation and detection_rules are in string format for storage\n    const representationStr =\n      typeof representation === \"object\"\n        ? JSON.stringify(representation)\n        : representation;\n\n    const detectionRulesStr =\n      typeof detection_rules === \"object\"\n        ? JSON.stringify(detection_rules)\n        : detection_rules;\n\n    // 4. Set default scores and counters for a global pattern\n    const frequency = 0; // New global patterns start with zero frequency\n    const utility_score = 0.5; // Medium utility by default\n    const reinforcement_count = 1; // Initial reinforcement\n    const is_global = true; // Mark as global pattern\n    const created_at = new Date().toISOString();\n    const updated_at = created_at;\n\n    // 5. Insert the pattern into the database\n    const query = `\n      INSERT INTO project_patterns (\n        pattern_id, \n        pattern_type, \n        name, \n        description, \n        representation, \n        detection_rules,\n        language,\n        frequency,\n        utility_score,\n        confidence_score,\n        reinforcement_count,\n        is_global,\n        created_at,\n        updated_at\n      ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n    `;\n\n    const params = [\n      pattern_id,\n      pattern_type,\n      name,\n      description,\n      representationStr,\n      detectionRulesStr,\n      language,\n      frequency,\n      utility_score,\n      confidenceScore,\n      reinforcement_count,\n      is_global,\n      created_at,\n      updated_at,\n    ];\n\n    await executeQuery(query, params);\n\n    console.log(\n      `Added new global pattern \"${name}\" (${pattern_id}) to repository`\n    );\n\n    // 6. Return the generated pattern ID\n    return pattern_id;\n  } catch (error) {\n    console.error(\"Error adding global pattern to repository:\", error);\n    throw new Error(`Failed to add global pattern: ${error.message}`);\n  }\n}\n\n/**\n * Retrieves global patterns from the repository with optional filtering\n *\n * @param {Object} filterOptions - Options to filter the global patterns\n * @param {string} [filterOptions.type] - Filter by pattern type\n * @param {number} [filterOptions.minConfidence] - Filter by minimum confidence score\n * @param {number} [filterOptions.limit] - Maximum number of patterns to return\n * @param {string} [filterOptions.language] - Filter by programming language\n * @returns {Promise<Pattern[]>} Array of global patterns matching the filters\n */\nexport async function retrieveGlobalPatterns(filterOptions = {}) {\n  try {\n    const { type, minConfidence, limit, language } = filterOptions;\n\n    // Build the query\n    let query = \"SELECT * FROM project_patterns WHERE is_global = TRUE\";\n    const params = [];\n\n    // Apply additional filters if provided\n    if (type) {\n      query += \" AND pattern_type = ?\";\n      params.push(type);\n    }\n\n    if (minConfidence !== undefined && !isNaN(minConfidence)) {\n      query += \" AND confidence_score >= ?\";\n      params.push(minConfidence);\n    }\n\n    // Apply language filter if provided\n    if (language) {\n      query += \" AND (language = ? OR language = ? OR language IS NULL)\";\n      params.push(language, \"any\"); // Include language-specific, universal, and legacy NULL patterns\n    }\n\n    // Order by confidence score and utility score\n    query += \" ORDER BY confidence_score DESC, utility_score DESC\";\n\n    // Apply limit if provided\n    if (limit !== undefined && !isNaN(limit) && limit > 0) {\n      query += \" LIMIT ?\";\n      params.push(limit);\n    }\n\n    // Execute the query\n    const patterns = await executeQuery(query, params);\n\n    // Check if patterns has a rows property and it's an array\n    const rows =\n      patterns && patterns.rows && Array.isArray(patterns.rows)\n        ? patterns.rows\n        : Array.isArray(patterns)\n        ? patterns\n        : [];\n\n    // If no valid results, return empty array\n    if (rows.length === 0) {\n      console.warn(\"No valid global patterns found\");\n      return [];\n    }\n\n    // Parse detection_rules JSON for each pattern\n    return rows.map((pattern) => ({\n      ...pattern,\n      detection_rules: JSON.parse(pattern.detection_rules || \"{}\"),\n      is_global: Boolean(pattern.is_global), // Ensure is_global is a boolean\n    }));\n  } catch (error) {\n    console.error(\"Error retrieving global patterns:\", error);\n    throw new Error(`Failed to retrieve global patterns: ${error.message}`);\n  }\n}\n\n/**\n * Promotes an existing pattern to global status\n *\n * @param {string} patternId - ID of the pattern to promote to global\n * @param {number} [newConfidence] - New confidence score to assign (optional)\n * @returns {Promise<boolean>} True if the pattern was successfully promoted, false otherwise\n */\nexport async function promotePatternToGlobal(patternId, newConfidence) {\n  try {\n    // Build the update query\n    let query = \"UPDATE project_patterns SET is_global = TRUE\";\n    const params = [];\n\n    // Update confidence score if provided\n    if (newConfidence !== undefined && !isNaN(newConfidence)) {\n      query += \", confidence_score = ?\";\n      params.push(newConfidence);\n    }\n\n    // Update the timestamp\n    const updated_at = new Date().toISOString();\n    query += \", updated_at = ?\";\n    params.push(updated_at);\n\n    // Add WHERE clause\n    query += \" WHERE pattern_id = ?\";\n    params.push(patternId);\n\n    // Execute the query\n    const result = await executeQuery(query, params);\n\n    // Check if a row was affected\n    const success = result.affectedRows > 0;\n\n    if (success) {\n      console.log(\n        `Pattern ${patternId} successfully promoted to global status`\n      );\n\n      if (newConfidence !== undefined) {\n        console.log(`Updated confidence score to ${newConfidence}`);\n      }\n    } else {\n      console.warn(`No pattern with ID ${patternId} found to promote`);\n    }\n\n    return success;\n  } catch (error) {\n    console.error(`Error promoting pattern ${patternId} to global:`, error);\n    throw new Error(`Failed to promote pattern: ${error.message}`);\n  }\n}\n\n/**\n * Records a pattern observation and updates its metrics\n *\n * @param {string} patternId - ID of the pattern to reinforce\n * @param {'usage'|'confirmation'|'rejection'} observationType - Type of the observation\n * @param {any} [contextData] - Additional context data for the observation\n * @returns {Promise<void>}\n */\nexport async function reinforcePattern(\n  patternId,\n  observationType,\n  contextData = {}\n) {\n  try {\n    // Create observation ID\n    const observation_id = uuidv4();\n    const timestamp = new Date().toISOString();\n\n    // Convert contextData to JSON string\n    const observation_data = JSON.stringify(contextData || {});\n\n    // Define adjustment values\n    const confidenceAdjustments = {\n      usage: 0.03, // Small increase for usage\n      confirmation: 0.05, // Moderate increase for explicit confirmation\n      rejection: -0.08, // Larger decrease for rejection\n    };\n\n    const utilityAdjustments = {\n      usage: 0.04, // Moderate increase for usage (indicates utility)\n      confirmation: 0.03, // Small increase for confirmation\n      rejection: -0.02, // Small decrease for rejection\n    };\n\n    // Begin transaction\n    await executeQuery(\"BEGIN TRANSACTION\");\n\n    try {\n      // 1. Insert observation record\n      const insertObservationQuery = `\n        INSERT INTO pattern_observations (\n          observation_id,\n          pattern_id,\n          observation_type,\n          observation_data,\n          timestamp\n        ) VALUES (?, ?, ?, ?, ?)\n      `;\n\n      await executeQuery(insertObservationQuery, [\n        observation_id,\n        patternId,\n        observationType,\n        observation_data,\n        timestamp,\n      ]);\n\n      // 2. Get current pattern data\n      const getPatternQuery =\n        \"SELECT confidence_score, utility_score, reinforcement_count FROM project_patterns WHERE pattern_id = ?\";\n      const patternResult = await executeQuery(getPatternQuery, [patternId]);\n\n      if (patternResult.length === 0) {\n        throw new Error(`Pattern with ID ${patternId} not found`);\n      }\n\n      const pattern = patternResult[0];\n\n      // 3. Calculate new scores\n      let newConfidenceScore =\n        pattern.confidence_score +\n        (confidenceAdjustments[observationType] || 0);\n      let newUtilityScore =\n        pattern.utility_score + (utilityAdjustments[observationType] || 0);\n\n      // Ensure scores stay within bounds\n      newConfidenceScore = Math.max(0, Math.min(1, newConfidenceScore));\n      newUtilityScore = Math.max(0, Math.min(1, newUtilityScore));\n\n      // 4. Update pattern metrics\n      const updatePatternQuery = `\n        UPDATE project_patterns SET\n          reinforcement_count = reinforcement_count + 1,\n          confidence_score = ?,\n          utility_score = ?,\n          updated_at = ?\n      `;\n\n      // Add last_detected_at update if observation is a usage\n      const updateLastDetected =\n        observationType === \"usage\" ? \", last_detected_at = ?\" : \"\";\n      const updatePatternParams = [\n        newConfidenceScore,\n        newUtilityScore,\n        timestamp,\n      ];\n\n      // Add timestamp parameter if updating last_detected_at\n      if (observationType === \"usage\") {\n        updatePatternParams.push(timestamp);\n      }\n\n      // Complete the query with WHERE clause\n      const finalUpdateQuery =\n        updatePatternQuery + updateLastDetected + \" WHERE pattern_id = ?\";\n      updatePatternParams.push(patternId);\n\n      await executeQuery(finalUpdateQuery, updatePatternParams);\n\n      // Commit transaction\n      await executeQuery(\"COMMIT\");\n\n      console.log(\n        `Pattern ${patternId} reinforced with '${observationType}' observation`\n      );\n    } catch (error) {\n      // Rollback transaction in case of error\n      await executeQuery(\"ROLLBACK\");\n      throw error;\n    }\n  } catch (error) {\n    console.error(`Error reinforcing pattern ${patternId}:`, error);\n    throw new Error(`Failed to reinforce pattern: ${error.message}`);\n  }\n}\n\n/**\n * Calculate similarity between two patterns\n *\n * @param {Pattern} pattern1 - First pattern to compare\n * @param {Pattern} pattern2 - Second pattern to compare\n * @returns {number} Similarity score between 0 and 1\n */\nexport function calculatePatternSimilarity(pattern1, pattern2) {\n  // Initialize similarity scores for different components\n  let representationSimilarity = 0;\n  let rulesSimilarity = 0;\n  let typeSimilarity = 0;\n  let languageSimilarity = 1.0; // Default to full match for language\n\n  // 1. Base type similarity on pattern_type match\n  typeSimilarity = pattern1.pattern_type === pattern2.pattern_type ? 1.0 : 0.3;\n\n  // 2. Check language similarity\n  if (pattern1.language && pattern2.language) {\n    // If both patterns have specific languages defined\n    if (pattern1.language === \"any\" || pattern2.language === \"any\") {\n      // If either is language-agnostic, still a good match but slightly penalized\n      languageSimilarity = 0.9;\n    } else if (pattern1.language !== pattern2.language) {\n      // Different specific languages - significant penalty\n      languageSimilarity = 0.2; // Significantly different patterns\n    }\n  } else if (pattern1.language || pattern2.language) {\n    // One has a language, the other doesn't (might be a legacy pattern or NULL)\n    // This is a reasonable match but less confident\n    languageSimilarity = 0.7;\n  }\n\n  // 3. Compare representations using Jaccard similarity\n  representationSimilarity = calculateJaccardSimilarity(\n    extractTokensFromField(pattern1.representation),\n    extractTokensFromField(pattern2.representation)\n  );\n\n  // 4. Compare detection_rules using Jaccard similarity\n  rulesSimilarity = calculateJaccardSimilarity(\n    extractTokensFromField(pattern1.detection_rules),\n    extractTokensFromField(pattern2.detection_rules)\n  );\n\n  // 5. Combine the similarities with weights\n  // Representation is the most important, followed by rules, then language and type\n  const combinedSimilarity =\n    representationSimilarity * 0.5 +\n    rulesSimilarity * 0.3 +\n    languageSimilarity * 0.15 +\n    typeSimilarity * 0.05;\n\n  // Ensure the result is within [0,1]\n  return Math.max(0, Math.min(1, combinedSimilarity));\n}\n\n/**\n * Extract tokens from a pattern field which could be a JSON string or regular text\n *\n * @param {string} field - The field to extract tokens from\n * @returns {string[]} Array of normalized tokens\n */\nfunction extractTokensFromField(field) {\n  if (!field) return [];\n\n  let content = field;\n\n  // If the field is a JSON string, try to parse it to get its content\n  if (\n    typeof field === \"string\" &&\n    (field.startsWith(\"{\") || field.startsWith(\"[\"))\n  ) {\n    try {\n      const parsed = JSON.parse(field);\n      // Convert the parsed object back to a string for tokenization\n      content = JSON.stringify(parsed, null, 0).toLowerCase();\n    } catch (e) {\n      // If parsing fails, use the original string\n      content = field.toLowerCase();\n    }\n  } else if (typeof field === \"object\") {\n    // If it's already an object, stringify it\n    content = JSON.stringify(field, null, 0).toLowerCase();\n  } else {\n    // For plain strings, just use as is\n    content = String(field).toLowerCase();\n  }\n\n  // Simple tokenization: split by non-alphanumeric chars and filter empty tokens\n  // In a real implementation, we would use TextTokenizerLogic.tokenize and stem\n  return content\n    .split(/[^a-z0-9_]+/)\n    .filter((token) => token.length > 1)\n    .map((token) => token.trim());\n}\n\n/**\n * Calculate Jaccard similarity index between two sets of tokens\n *\n * @param {string[]} tokens1 - First set of tokens\n * @param {string[]} tokens2 - Second set of tokens\n * @returns {number} Jaccard similarity index (0-1)\n */\nfunction calculateJaccardSimilarity(tokens1, tokens2) {\n  if (!tokens1.length && !tokens2.length) return 1.0; // Both empty means identical\n  if (!tokens1.length || !tokens2.length) return 0.0; // One empty means no similarity\n\n  // Create sets from the token arrays to eliminate duplicates\n  const set1 = new Set(tokens1);\n  const set2 = new Set(tokens2);\n\n  // Calculate intersection size\n  let intersectionSize = 0;\n  for (const token of set1) {\n    if (set2.has(token)) {\n      intersectionSize++;\n    }\n  }\n\n  // Calculate union size\n  const unionSize = set1.size + set2.size - intersectionSize;\n\n  // Jaccard similarity = size of intersection / size of union\n  return intersectionSize / unionSize;\n}\n\n/**\n * Consolidates session patterns by promoting or merging them with global patterns\n *\n * @param {Object} options - Options for consolidation\n * @param {number} [options.minReinforcementCount=3] - Minimum reinforcement count for promotion\n * @param {number} [options.minConfidence=0.6] - Minimum confidence score for promotion\n * @param {number} [options.similarityThreshold=0.8] - Threshold for pattern similarity to consider merging\n * @returns {Promise<{promoted: number, merged: number}>} Count of promoted and merged patterns\n */\nexport async function consolidateSessionPatterns(options = {}) {\n  try {\n    // Set default options\n    const {\n      minReinforcementCount = 3,\n      minConfidence = 0.6,\n      similarityThreshold = 0.8,\n    } = options;\n\n    console.log(\n      `Starting pattern consolidation process (minReinforcementCount=${minReinforcementCount}, minConfidence=${minConfidence})`\n    );\n\n    // Track counts\n    let promotedCount = 0;\n    let mergedCount = 0;\n\n    // 1. Find non-global patterns that meet the criteria\n    const query = `\n      SELECT * FROM project_patterns \n      WHERE is_global = FALSE \n      AND reinforcement_count >= ? \n      AND confidence_score >= ?\n    `;\n\n    const sessionPatterns = await executeQuery(query, [\n      minReinforcementCount,\n      minConfidence,\n    ]);\n\n    console.log(\n      `Found ${sessionPatterns.length} session patterns that qualify for promotion or merging`\n    );\n\n    if (sessionPatterns.length === 0) {\n      return { promoted: 0, merged: 0 };\n    }\n\n    // 2. Get existing global patterns for potential merging\n    const globalPatterns = await retrieveGlobalPatterns();\n\n    // 3. Process each qualifying session pattern\n    for (const sessionPattern of sessionPatterns) {\n      const patternId = sessionPattern.pattern_id;\n\n      // Try to find a similar global pattern for merging\n      let shouldPromote = true;\n      let similarGlobalPattern = null;\n\n      for (const globalPattern of globalPatterns) {\n        const similarity = calculatePatternSimilarity(\n          sessionPattern,\n          globalPattern\n        );\n\n        if (similarity >= similarityThreshold) {\n          shouldPromote = false;\n          similarGlobalPattern = globalPattern;\n          break;\n        }\n      }\n\n      if (shouldPromote) {\n        // Promote the pattern to global status\n        console.log(`Promoting session pattern ${patternId} to global status`);\n        const promoted = await promotePatternToGlobal(\n          patternId,\n          sessionPattern.confidence_score\n        );\n\n        if (promoted) {\n          promotedCount++;\n          console.log(`Successfully promoted pattern ${patternId}`);\n        }\n      } else if (similarGlobalPattern) {\n        // Placeholder for merging logic\n        console.log(\n          `Merge attempt for pattern ${patternId} with ${similarGlobalPattern.pattern_id}`\n        );\n\n        // In a real implementation, the merging would:\n        // 1. Update the global pattern with some attributes from the session pattern\n        // 2. Maybe increase confidence/utility/reinforcement_count of the global pattern\n        // 3. Delete the session pattern or mark it as merged\n\n        // For now, just log and count\n        mergedCount++;\n      }\n    }\n\n    console.log(\n      `Pattern consolidation complete. Promoted: ${promotedCount}, Merged: ${mergedCount}`\n    );\n\n    return {\n      promoted: promotedCount,\n      merged: mergedCount,\n    };\n  } catch (error) {\n    console.error(\"Error consolidating session patterns:\", error);\n    throw new Error(`Failed to consolidate session patterns: ${error.message}`);\n  }\n}\n\n/**\n * Schedules a periodic background process for pattern consolidation\n *\n * @param {number} [intervalMinutes=60] - The interval in minutes to run the consolidation\n * @returns {number} The interval ID that can be used to clear the interval if needed\n */\nexport function scheduleConsolidation(intervalMinutes = 60) {\n  // Convert intervalMinutes to milliseconds\n  const intervalMs = intervalMinutes * 60 * 1000;\n\n  console.log(\n    `Scheduling pattern consolidation to run every ${intervalMinutes} minutes`\n  );\n\n  // Set up the interval\n  const intervalId = setInterval(async () => {\n    console.log(\n      `Running scheduled pattern consolidation (interval: ${intervalMinutes} minutes)`\n    );\n\n    try {\n      // Call consolidateSessionPatterns with sensible defaults\n      const result = await consolidateSessionPatterns({\n        minReinforcementCount: 5,\n        minConfidence: 0.7,\n      });\n\n      console.log(\n        `Pattern consolidation completed: ${result.promoted} patterns promoted, ${result.merged} patterns merged`\n      );\n    } catch (error) {\n      console.error(`Error during scheduled pattern consolidation:`, error);\n    }\n  }, intervalMs);\n\n  return intervalId;\n}\n\n/**\n * Retrieves usage statistics for a specific pattern\n *\n * @param {string} patternId - The ID of the pattern to get statistics for\n * @returns {Promise<{usageCount: number, successRate: number, avgConfidence: number}>} Usage statistics\n */\nexport async function getPatternUsageStats(patternId) {\n  try {\n    // Get observation counts from pattern_observations table\n    const observationsQuery = `\n      SELECT \n        COUNT(*) as total_observations,\n        SUM(CASE WHEN observation_type IN ('usage', 'confirmation') THEN 1 ELSE 0 END) as successful_uses,\n        SUM(CASE WHEN observation_type = 'rejection' THEN 1 ELSE 0 END) as failed_uses\n      FROM pattern_observations\n      WHERE pattern_id = ?\n    `;\n\n    const observationsResult = await executeQuery(observationsQuery, [\n      patternId,\n    ]);\n\n    if (!observationsResult || observationsResult.length === 0) {\n      return {\n        usageCount: 0,\n        successRate: 0,\n        avgConfidence: 0,\n      };\n    }\n\n    const stats = observationsResult[0];\n    const usageCount = stats.total_observations || 0;\n\n    // Calculate success rate: (successful uses) / (successful uses + failed uses)\n    // Avoid division by zero if there are no success/failure observations\n    const successPlusFailed =\n      (stats.successful_uses || 0) + (stats.failed_uses || 0);\n    const successRate =\n      successPlusFailed > 0\n        ? (stats.successful_uses || 0) / successPlusFailed\n        : 0;\n\n    // Get current confidence score from project_patterns table\n    const patternQuery = `\n      SELECT confidence_score\n      FROM project_patterns\n      WHERE pattern_id = ?\n    `;\n\n    const patternResult = await executeQuery(patternQuery, [patternId]);\n\n    // If pattern not found, return zero confidence\n    const avgConfidence =\n      patternResult && patternResult.length > 0\n        ? patternResult[0].confidence_score\n        : 0;\n\n    return {\n      usageCount,\n      successRate,\n      avgConfidence,\n    };\n  } catch (error) {\n    console.error(`Error getting pattern usage stats for ${patternId}:`, error);\n    throw new Error(`Failed to get pattern usage statistics: ${error.message}`);\n  }\n}\n", "/**\n * Schema definitions for MCP tool inputs and outputs\n * Using Zod for schema validation\n */\n\nimport { z } from \"zod\";\nimport { DEFAULT_TOKEN_BUDGET } from \"../config.js\";\n\n/**\n * Schema for initialize_conversation_context tool input\n * Note: projectId field is explicitly removed as the server operates\n * with a database instance dedicated to a single project\n */\nexport const initializeConversationContextInputSchema = {\n  // No projectId field as per the blueprint\n  initialQuery: z.string().optional(),\n  focusHint: z\n    .object({\n      type: z.string(),\n      identifier: z.string(),\n    })\n    .optional(),\n  includeArchitecture: z.boolean().optional().default(true),\n  includeRecentConversations: z.boolean().optional().default(true),\n  maxCodeContextItems: z.number().optional().default(5),\n  maxRecentChanges: z.number().optional().default(5),\n  contextDepth: z\n    .enum([\"minimal\", \"standard\", \"comprehensive\"])\n    .optional()\n    .default(\"standard\"),\n  tokenBudget: z.number().optional().default(DEFAULT_TOKEN_BUDGET),\n};\n\n/**\n * Schema for initialize_conversation_context tool output\n * Includes comprehensive context object\n */\nexport const initializeConversationContextOutputSchema = {\n  conversationId: z.string(),\n  initialContextSummary: z.string(),\n  predictedIntent: z.string().optional(),\n  comprehensiveContext: z\n    .object({\n      codeContext: z.array(z.any()).optional(),\n      architectureContext: z\n        .object({\n          summary: z.string(),\n          sources: z.array(\n            z.object({\n              name: z.string(),\n              path: z.string(),\n            })\n          ),\n        })\n        .nullable(),\n      recentConversations: z\n        .array(\n          z.object({\n            timestamp: z.number(),\n            summary: z.string(),\n            purpose: z.string(),\n          })\n        )\n        .optional(),\n      activeWorkflows: z\n        .array(\n          z.object({\n            name: z.string(),\n            description: z.string(),\n            timestamp: z.number(),\n          })\n        )\n        .optional(),\n      projectStructure: z.any().nullable(),\n      recentChanges: z\n        .array(\n          z.object({\n            timestamp: z.number(),\n            files: z.array(z.string()),\n            summary: z.string(),\n          })\n        )\n        .optional(),\n      globalPatterns: z\n        .array(\n          z.object({\n            name: z.string(),\n            type: z.string(),\n            description: z.string(),\n            confidence: z.number(),\n          })\n        )\n        .optional(),\n    })\n    .optional(),\n};\n\n/**\n * Schema for update_conversation_context tool input\n * Note: projectId field is explicitly removed as the server operates\n * with a database instance dedicated to a single project\n */\nexport const updateConversationContextInputSchema = {\n  // No projectId field as per the blueprint\n  conversationId: z.string(),\n  newMessages: z\n    .array(\n      z.object({\n        role: z.enum([\"user\", \"assistant\", \"system\"]),\n        content: z.string(),\n      })\n    )\n    .optional()\n    .default([]),\n  codeChanges: z\n    .array(\n      z.object({\n        filePath: z.string(),\n        newContent: z.string(),\n        languageHint: z.string().optional(),\n      })\n    )\n    .optional()\n    .default([]),\n  preserveContextOnTopicShift: z.boolean().optional().default(true),\n  contextIntegrationLevel: z\n    .enum([\"minimal\", \"balanced\", \"aggressive\"])\n    .optional()\n    .default(\"balanced\"),\n  trackIntentTransitions: z.boolean().optional().default(true),\n  tokenBudget: z.number().optional().default(DEFAULT_TOKEN_BUDGET),\n};\n\n/**\n * Schema for update_conversation_context tool output\n * Includes continuity tracking, context synthesis, and intent transition detection\n */\nexport const updateConversationContextOutputSchema = {\n  status: z.enum([\"success\", \"partial\", \"failure\"]),\n  updatedFocus: z\n    .object({\n      type: z.string(),\n      identifier: z.string(),\n    })\n    .optional(),\n  contextContinuity: z.object({\n    preserved: z.boolean(),\n    topicShift: z.boolean(),\n    intentTransition: z.boolean(),\n  }),\n  contextSynthesis: z\n    .object({\n      summary: z.string(),\n      topPriorities: z.array(z.string()).optional(),\n    })\n    .optional(),\n  intentTransition: z\n    .object({\n      from: z.string().nullable(),\n      to: z.string().nullable(),\n      confidence: z.number(),\n    })\n    .optional(),\n};\n\n/**\n * Schema for retrieve_relevant_context tool input\n * Note: projectId field is explicitly removed as the server operates\n * with a database instance dedicated to a single project\n */\nexport const retrieveRelevantContextInputSchema = {\n  // No projectId field as per the blueprint\n  conversationId: z.string(),\n  query: z.string(),\n  tokenBudget: z.number().optional().default(DEFAULT_TOKEN_BUDGET),\n  constraints: z\n    .object({\n      entityTypes: z.array(z.string()).optional(),\n      filePaths: z.array(z.string()).optional(),\n      includeConversation: z.boolean().optional().default(true),\n      crossTopicSearch: z.boolean().optional().default(false),\n      focusOverride: z\n        .object({ type: z.string(), identifier: z.string() })\n        .optional(),\n    })\n    .optional()\n    .default({}),\n  contextFilters: z\n    .object({\n      minRelevanceScore: z.number().optional().default(0.3),\n      excludeTypes: z.array(z.string()).optional(),\n      preferredLanguages: z.array(z.string()).optional(),\n      timeframe: z\n        .object({\n          from: z.number().optional(),\n          to: z.number().optional(),\n        })\n        .optional(),\n    })\n    .optional()\n    .default({}),\n  weightingStrategy: z\n    .enum([\"relevance\", \"recency\", \"hierarchy\", \"balanced\"])\n    .optional()\n    .default(\"balanced\"),\n  balanceStrategy: z\n    .enum([\"proportional\", \"equal_representation\", \"priority_based\"])\n    .optional()\n    .default(\"proportional\"),\n  contextBalance: z\n    .union([\n      z.enum([\"auto\", \"code_heavy\", \"balanced\", \"documentation_focused\"]),\n      z.object({\n        code: z.number().optional(),\n        conversation: z.number().optional(),\n        documentation: z.number().optional(),\n        patterns: z.number().optional(),\n      }),\n    ])\n    .optional()\n    .default(\"auto\"),\n  sourceTypePreferences: z\n    .object({\n      includePatterns: z.boolean().optional().default(true),\n      includeDocumentation: z.boolean().optional().default(true),\n      prioritizeTestCases: z.boolean().optional().default(false),\n      prioritizeExamples: z.boolean().optional().default(false),\n    })\n    .optional()\n    .default({}),\n};\n\n/**\n * Schema for retrieve_relevant_context tool output\n * Includes enhanced context snippets with confidence scoring,\n * source attribution, and relevance explanations\n */\nexport const retrieveRelevantContextOutputSchema = {\n  contextSnippets: z.array(\n    z.object({\n      type: z.string(), // 'code', 'conversation', 'documentation', 'pattern'\n      content: z.string(),\n      entity_id: z.string(),\n      relevanceScore: z.number(),\n      confidenceScore: z.number(),\n      metadata: z.any(), // Flexible metadata based on type\n      sourceAttribution: z.string(),\n      relevanceExplanation: z.string(),\n    })\n  ),\n  retrievalSummary: z.string(),\n  contextMetrics: z\n    .object({\n      totalFound: z.number(),\n      selected: z.number(),\n      averageConfidence: z.number(),\n      typeDistribution: z.object({\n        code: z.number(),\n        conversation: z.number(),\n        documentation: z.number(),\n        pattern: z.number(),\n      }),\n    })\n    .optional(),\n};\n\n/**\n * Schema for record_milestone_context tool input\n * Includes milestone categorization and impact assessment control\n */\nexport const recordMilestoneContextInputSchema = {\n  conversationId: z.string(),\n  name: z.string(),\n  description: z.string().optional(),\n  customData: z.any().optional(),\n  milestoneCategory: z\n    .enum([\n      \"bug_fix\",\n      \"feature_completion\",\n      \"refactoring\",\n      \"documentation\",\n      \"test\",\n      \"configuration\",\n      \"uncategorized\",\n    ])\n    .optional()\n    .default(\"uncategorized\"),\n  assessImpact: z.boolean().optional().default(true),\n};\n\n/**\n * Schema for record_milestone_context tool output\n * Includes milestone category, related entities count, and detailed impact assessment\n */\nexport const recordMilestoneContextOutputSchema = {\n  milestoneId: z.string(),\n  status: z.string(),\n  milestoneCategory: z.string(),\n  relatedEntitiesCount: z.number(),\n  impactAssessment: z\n    .object({\n      impactScore: z.number(),\n      impactLevel: z.string(),\n      impactSummary: z.string(),\n      scopeMetrics: z\n        .object({\n          directlyModifiedEntities: z.number(),\n          potentiallyImpactedEntities: z.number(),\n          impactedComponents: z.number(),\n          criticalPathsCount: z.number(),\n        })\n        .optional(),\n      stabilityRisk: z.number().optional(),\n      criticalPaths: z\n        .array(\n          z.object({\n            sourceId: z.string(),\n            path: z.string(),\n            dependencyCount: z.number(),\n          })\n        )\n        .optional(),\n      mostImpactedComponents: z\n        .array(\n          z.object({\n            name: z.string(),\n            count: z.number(),\n          })\n        )\n        .optional(),\n      error: z.string().optional(),\n    })\n    .nullable(),\n};\n\n/**\n * Schema for finalize_conversation_context tool input\n * Includes enhanced options for learning extraction, pattern promotion,\n * related topics synthesis, and next steps generation\n */\nexport const finalizeConversationContextInputSchema = {\n  conversationId: z.string(),\n  clearActiveContext: z.boolean().optional().default(false),\n  extractLearnings: z.boolean().optional().default(true),\n  promotePatterns: z.boolean().optional().default(true),\n  synthesizeRelatedTopics: z.boolean().optional().default(true),\n  generateNextSteps: z.boolean().optional().default(true),\n  outcome: z\n    .enum([\"completed\", \"abandoned\", \"paused\", \"reference_only\"])\n    .optional()\n    .default(\"completed\"),\n};\n\n/**\n * Schema for finalize_conversation_context tool output\n * Includes substantially richer output with extracted learnings, promoted patterns,\n * related conversations synthesis, and next steps suggestions\n */\nexport const finalizeConversationContextOutputSchema = {\n  status: z.string(),\n  summary: z.string(),\n  purpose: z.string(),\n\n  // Extracted learnings with confidence scores\n  extractedLearnings: z\n    .object({\n      learnings: z.array(\n        z.object({\n          type: z.string(),\n          content: z.string(),\n          confidence: z.number(),\n          // Other properties depend on learning type\n          patternId: z.string().optional(),\n          context: z.array(z.any()).optional(),\n          messageReference: z.string().optional(),\n          relatedIssues: z.array(z.any()).optional(),\n          alternatives: z.array(z.string()).optional(),\n          rationale: z.string().optional(),\n          codeReferences: z.array(z.any()).optional(),\n          applicability: z.number().optional(),\n        })\n      ),\n      count: z.number(),\n      byType: z.record(z.string(), z.number()),\n      averageConfidence: z.number(),\n      error: z.string().optional(),\n    })\n    .nullable(),\n\n  // Promoted patterns\n  promotedPatterns: z\n    .object({\n      promoted: z.number(),\n      patterns: z.array(\n        z.object({\n          patternId: z.string(),\n          name: z.string(),\n          type: z.string(),\n          promoted: z.boolean(),\n          confidence: z.number(),\n        })\n      ),\n      error: z.string().optional(),\n    })\n    .nullable(),\n\n  // Related conversations synthesis\n  relatedConversations: z\n    .object({\n      relatedCount: z.number(),\n      conversations: z.array(\n        z.object({\n          conversationId: z.string(),\n          summary: z.string(),\n          timestamp: z.number(),\n          similarityScore: z.number(),\n          commonTopics: z.array(z.string()),\n        })\n      ),\n      synthesizedInsights: z.array(\n        z.object({\n          topic: z.string(),\n          insight: z.string(),\n          conversationCount: z.number(),\n          sourceSummaries: z.array(\n            z.object({\n              conversationId: z.string(),\n              summary: z.string(),\n            })\n          ),\n        })\n      ),\n      error: z.string().optional(),\n    })\n    .nullable(),\n\n  // Next steps and follow-up suggestions\n  nextSteps: z\n    .object({\n      suggestedNextSteps: z.array(\n        z.object({\n          action: z.string(),\n          priority: z.enum([\"high\", \"medium\", \"low\"]),\n          rationale: z.string(),\n        })\n      ),\n      followUpTopics: z.array(\n        z.object({\n          topic: z.string(),\n          priority: z.enum([\"high\", \"medium\", \"low\"]),\n          rationale: z.string(),\n        })\n      ),\n      referenceMaterials: z.array(\n        z.object({\n          title: z.string(),\n          path: z.string(),\n          type: z.string(),\n          relevance: z.number(),\n        })\n      ),\n      error: z.string().optional(),\n    })\n    .nullable(),\n};\n", "/**\n * updateConversationContext.tool.js\n *\n * MCP tool implementation for updating an existing conversation context\n * This tool processes new messages and code changes, manages topic shifts,\n * and ensures context continuity throughout the conversation\n */\n\nimport { z } from \"zod\";\nimport { executeQuery } from \"../db.js\";\nimport * as ConversationIntelligence from \"../logic/ConversationIntelligence.js\";\nimport * as KnowledgeProcessor from \"../logic/KnowledgeProcessor.js\";\nimport * as TimelineManagerLogic from \"../logic/TimelineManagerLogic.js\";\nimport * as IntentPredictorLogic from \"../logic/IntentPredictorLogic.js\";\nimport * as ActiveContextManager from \"../logic/ActiveContextManager.js\";\nimport * as ConversationSegmenter from \"../logic/ConversationSegmenter.js\";\nimport * as ConversationPurposeDetector from \"../logic/ConversationPurposeDetector.js\";\nimport * as ContextCompressorLogic from \"../logic/ContextCompressorLogic.js\";\nimport { logMessage } from \"../utils/logger.js\";\n\nimport {\n  updateConversationContextInputSchema,\n  updateConversationContextOutputSchema,\n} from \"../schemas/toolSchemas.js\";\n\n/**\n * Handler for update_conversation_context tool\n *\n * @param {object} input - Tool input parameters\n * @param {object} sdkContext - SDK context\n * @returns {Promise<object>} Tool output\n */\nasync function handler(input, sdkContext) {\n  try {\n    logMessage(\"INFO\", `update_conversation_context tool started`, {\n      conversationId: input.conversationId,\n      messageCount: input.newMessages?.length || 0,\n      codeChangeCount: input.codeChanges?.length || 0,\n    });\n\n    // 1. Extract input parameters with defaults\n    const {\n      conversationId,\n      newMessages = [],\n      codeChanges = [],\n      preserveContextOnTopicShift = true,\n      contextIntegrationLevel = \"balanced\",\n      trackIntentTransitions = true,\n      tokenBudget = 4000,\n    } = input;\n\n    // Validate conversation ID is provided\n    if (!conversationId) {\n      const error = new Error(\"conversationId is required\");\n      error.code = \"MISSING_CONVERSATION_ID\";\n      throw error;\n    }\n\n    logMessage(\"DEBUG\", `Processing update with parameters`, {\n      preserveContextOnTopicShift,\n      contextIntegrationLevel,\n      trackIntentTransitions,\n    });\n\n    // 2. Initialize tracking variables for context transitions\n    let topicShift = false;\n    let intentTransition = false;\n    let previousIntent = null;\n    let currentIntent = null;\n    let contextPreserved = true;\n    let currentFocus = null;\n\n    // 3. Get current context state before changes\n    try {\n      const previousContextState =\n        await ActiveContextManager.getActiveContextState();\n      logMessage(\"DEBUG\", `Retrieved previous context state`, {\n        hasPreviousContext: !!previousContextState,\n      });\n\n      if (trackIntentTransitions) {\n        previousIntent = await ConversationPurposeDetector.getActivePurpose(\n          conversationId\n        );\n        logMessage(\"DEBUG\", `Retrieved previous intent`, { previousIntent });\n      }\n    } catch (err) {\n      logMessage(\n        \"WARN\",\n        `Failed to retrieve previous context state, continuing with defaults`,\n        {\n          error: err.message,\n        }\n      );\n      // Continue with defaults already initialized\n    }\n\n    // 4. Process new messages if any\n    if (newMessages.length > 0) {\n      logMessage(\"INFO\", `Processing ${newMessages.length} new messages`);\n      try {\n        const processedMessages = await processNewMessages(\n          conversationId,\n          newMessages,\n          {\n            trackIntentTransitions,\n          }\n        );\n\n        topicShift = processedMessages.topicShift;\n        logMessage(\"DEBUG\", `Message processing completed`, {\n          topicShift: topicShift,\n        });\n\n        if (trackIntentTransitions) {\n          intentTransition = processedMessages.intentTransition;\n          currentIntent = processedMessages.currentIntent;\n\n          if (intentTransition) {\n            logMessage(\"INFO\", `Intent transition detected`, {\n              from: previousIntent,\n              to: currentIntent,\n            });\n          }\n        }\n      } catch (err) {\n        logMessage(\"ERROR\", `Failed to process new messages`, {\n          error: err.message,\n          conversationId,\n        });\n        // Continue with code changes processing despite message error\n      }\n    }\n\n    // 5. Process code changes if any\n    if (codeChanges.length > 0) {\n      logMessage(\"INFO\", `Processing ${codeChanges.length} code changes`);\n      try {\n        const processedChanges = await processCodeChanges(\n          conversationId,\n          codeChanges\n        );\n\n        // Update tracking variables with results from code changes\n        if (processedChanges.focusChanged) {\n          logMessage(\"INFO\", `Focus changed due to code changes`, {\n            newFocus: processedChanges.newFocus,\n          });\n\n          // Code changes can also affect focus and sometimes intent\n          if (trackIntentTransitions && !intentTransition) {\n            try {\n              // Only update if we haven't already detected a transition from messages\n              const intentResult = await IntentPredictorLogic.updateIntent({\n                conversationId,\n                codeChanges,\n              });\n\n              if (intentResult.intentChanged) {\n                intentTransition = true;\n                currentIntent = intentResult.newIntent;\n                logMessage(\"INFO\", `Intent changed due to code changes`, {\n                  newIntent: currentIntent,\n                });\n              }\n            } catch (intentErr) {\n              logMessage(\"WARN\", `Failed to update intent from code changes`, {\n                error: intentErr.message,\n              });\n              // Continue without updating intent\n            }\n          }\n        }\n      } catch (err) {\n        logMessage(\"ERROR\", `Failed to process code changes`, {\n          error: err.message,\n          conversationId,\n        });\n        // Continue with context management despite code change error\n      }\n    }\n\n    // 6. Manage context continuity based on topic shifts and transitions\n    if (topicShift || intentTransition) {\n      logMessage(\n        \"INFO\",\n        `Topic shift or intent transition detected, managing context continuity`,\n        {\n          topicShift,\n          intentTransition,\n          preserveContextOnTopicShift,\n        }\n      );\n\n      // Determine if and how to preserve context\n      if (!preserveContextOnTopicShift) {\n        try {\n          // Clear previous context if preservation not requested\n          await ActiveContextManager.clearActiveContext();\n          contextPreserved = false;\n          logMessage(\"INFO\", `Cleared previous context due to topic shift`);\n\n          // Initialize fresh context for new topic/intent\n          if (currentIntent) {\n            try {\n              const recentEvents =\n                await TimelineManagerLogic.getRecentEventsForConversation(\n                  conversationId,\n                  10\n                );\n\n              const focusResult = await IntentPredictorLogic.predictFocusArea(\n                recentEvents,\n                codeChanges\n              );\n\n              if (focusResult) {\n                await ActiveContextManager.setActiveFocus(\n                  focusResult.type,\n                  focusResult.identifier\n                );\n                currentFocus = focusResult;\n                logMessage(\"INFO\", `Set new focus area based on intent`, {\n                  type: focusResult.type,\n                  identifier: focusResult.identifier,\n                });\n              }\n            } catch (focusErr) {\n              logMessage(\"WARN\", `Failed to set new focus area`, {\n                error: focusErr.message,\n              });\n              // Continue without setting focus\n            }\n          }\n        } catch (clearErr) {\n          logMessage(\"ERROR\", `Failed to clear context`, {\n            error: clearErr.message,\n          });\n          // Continue with next steps despite error\n        }\n      } else {\n        try {\n          // Integrate previous and new context\n          const previousContextState =\n            (await ActiveContextManager.getActiveContextState()) || {};\n\n          const integratedContext = await _integrateContexts(\n            previousContextState,\n            {\n              topicShift,\n              intentTransition,\n              previousIntent,\n              currentIntent,\n              codeChanges,\n            },\n            contextIntegrationLevel\n          );\n\n          await ActiveContextManager.updateActiveContext(integratedContext);\n          contextPreserved = true;\n          logMessage(\"INFO\", `Integrated previous and new context`, {\n            contextIntegrationLevel,\n          });\n        } catch (integrateErr) {\n          logMessage(\"ERROR\", `Failed to integrate contexts`, {\n            error: integrateErr.message,\n          });\n          // Continue with next steps despite error\n        }\n      }\n    } else {\n      logMessage(\n        \"DEBUG\",\n        `No topic shift or intent transition detected, preserving context`\n      );\n    }\n\n    // 7. Get final focus and context state\n    if (!currentFocus) {\n      try {\n        currentFocus = await ActiveContextManager.getActiveFocus();\n        logMessage(\"DEBUG\", `Retrieved current focus`, {\n          focus: currentFocus\n            ? `${currentFocus.type}:${currentFocus.identifier}`\n            : \"none\",\n        });\n      } catch (focusErr) {\n        logMessage(\"WARN\", `Failed to get current focus`, {\n          error: focusErr.message,\n        });\n        // Continue without focus\n      }\n    }\n\n    // 8. Generate context synthesis\n    let contextSynthesis;\n    try {\n      contextSynthesis = await generateContextSynthesis(\n        conversationId,\n        currentIntent,\n        topicShift || intentTransition\n      );\n      logMessage(\"DEBUG\", `Generated context synthesis`, {\n        synthesisLength: contextSynthesis?.length || 0,\n      });\n    } catch (synthesisErr) {\n      logMessage(\"WARN\", `Failed to generate context synthesis`, {\n        error: synthesisErr.message,\n      });\n      contextSynthesis = null;\n    }\n\n    // 9. Update timeline with context update event\n    try {\n      await TimelineManagerLogic.recordEvent(\n        \"context_updated\",\n        {\n          newMessagesCount: newMessages.length,\n          codeChangesCount: codeChanges.length,\n          topicShift,\n          intentTransition: intentTransition\n            ? {\n                from: previousIntent,\n                to: currentIntent,\n              }\n            : null,\n          contextPreserved,\n          contextIntegrationLevel: contextPreserved\n            ? contextIntegrationLevel\n            : \"none\",\n        },\n        [], // No specific entity IDs\n        conversationId\n      );\n      logMessage(\"DEBUG\", `Recorded context update in timeline`);\n    } catch (timelineErr) {\n      logMessage(\"WARN\", `Failed to record context update in timeline`, {\n        error: timelineErr.message,\n      });\n      // Non-critical error, continue\n    }\n\n    // 10. Return the tool response\n    logMessage(\n      \"INFO\",\n      `update_conversation_context tool completed successfully`\n    );\n\n    const responseData = {\n      status: \"success\",\n      message: `Conversation context updated for ${conversationId}`,\n      updatedFocus: currentFocus\n        ? {\n            type: currentFocus.type,\n            identifier: currentFocus.identifier,\n          }\n        : undefined,\n      contextContinuity: {\n        topicShift,\n        intentTransition,\n        contextPreserved,\n      },\n      synthesis: contextSynthesis,\n    };\n\n    return {\n      content: [\n        {\n          type: \"text\",\n          text: JSON.stringify(responseData),\n        },\n      ],\n    };\n  } catch (error) {\n    // Log detailed error information\n    logMessage(\"ERROR\", `Error in update_conversation_context tool`, {\n      error: error.message,\n      stack: error.stack,\n      input: {\n        conversationId: input.conversationId,\n        messageCount: input.newMessages?.length || 0,\n        codeChangeCount: input.codeChanges?.length || 0,\n      },\n    });\n\n    // Return error response\n    const errorResponse = {\n      error: true,\n      errorCode: error.code || \"UPDATE_FAILED\",\n      errorDetails: error.message,\n    };\n\n    return {\n      content: [\n        {\n          type: \"text\",\n          text: JSON.stringify(errorResponse),\n        },\n      ],\n    };\n  }\n}\n\n/**\n * Process new messages and detect topic shifts or intent transitions\n *\n * @param {string} conversationId - Conversation ID\n * @param {Array} messages - New messages to process\n * @param {object} options - Processing options\n * @returns {Promise<object>} Processing results\n */\nasync function processNewMessages(conversationId, messages, options = {}) {\n  try {\n    logMessage(\n      \"DEBUG\",\n      `Processing ${messages.length} messages for conversation ${conversationId}`\n    );\n\n    const result = {\n      topicShift: false,\n      intentTransition: false,\n      currentIntent: null,\n    };\n\n    // Process each message\n    for (const message of messages) {\n      try {\n        // Extra debugging log to capture input parameters\n        console.log(\"RECORDING MESSAGE - Input params:\", {\n          content: message.content,\n          role: message.role,\n          conversationId,\n        });\n\n        // Record message in database\n        const messageId = await ConversationIntelligence.recordMessage(\n          message.content,\n          message.role,\n          conversationId,\n          [], // relatedContextEntityIds\n          null // topicSegmentId\n        );\n\n        // Extra debugging log to confirm success\n        console.log(\"RECORDING MESSAGE - Success:\", {\n          messageId,\n          role: message.role,\n        });\n\n        logMessage(\"DEBUG\", `Recorded message from ${message.role}`);\n      } catch (msgErr) {\n        // Extra detailed error logging\n        console.error(\"RECORDING MESSAGE - FAILED:\", {\n          error: msgErr.message,\n          stack: msgErr.stack,\n          messageRole: message.role,\n          messageContent:\n            message.content && message.content.substring(0, 50) + \"...\",\n        });\n\n        logMessage(\n          \"WARN\",\n          `Failed to record message in conversation intelligence`,\n          {\n            error: msgErr.message,\n            messageRole: message.role,\n          }\n        );\n        // Continue with next message\n      }\n    }\n\n    // Check for topic shifts using conversation segmenter\n    try {\n      const segmentationResult = await ConversationSegmenter.detectTopicShift(\n        conversationId,\n        messages\n      );\n      result.topicShift = segmentationResult.topicShift;\n\n      if (result.topicShift) {\n        logMessage(\"INFO\", `Topic shift detected`, {\n          previousTopic: segmentationResult.previousTopic,\n          newTopic: segmentationResult.newTopic,\n          confidence: segmentationResult.confidence,\n        });\n      }\n    } catch (segmentErr) {\n      logMessage(\"WARN\", `Failed to detect topic shift`, {\n        error: segmentErr.message,\n      });\n      // Continue with default value (false)\n    }\n\n    // If tracking intent transitions is enabled\n    if (options.trackIntentTransitions) {\n      try {\n        const previousIntent =\n          await ConversationPurposeDetector.getActivePurpose(conversationId);\n\n        // Update intent based on new messages\n        const intentUpdateResult = await IntentPredictorLogic.updateIntent({\n          conversationId,\n          messages,\n        });\n\n        if (intentUpdateResult.intentChanged) {\n          result.intentTransition = true;\n          result.currentIntent = intentUpdateResult.newIntent;\n\n          logMessage(\"INFO\", `Intent transition detected`, {\n            from: previousIntent,\n            to: result.currentIntent,\n            confidence: intentUpdateResult.confidence,\n          });\n\n          // Update the active purpose in the conversation detector\n          await ConversationPurposeDetector.setActivePurpose(\n            conversationId,\n            result.currentIntent\n          );\n        } else {\n          result.currentIntent = previousIntent;\n        }\n      } catch (intentErr) {\n        logMessage(\"WARN\", `Failed to track intent transition`, {\n          error: intentErr.message,\n        });\n        // Continue with default values\n      }\n    }\n\n    return result;\n  } catch (error) {\n    logMessage(\"ERROR\", `Error processing new messages`, {\n      error: error.message,\n      conversationId,\n    });\n    throw error; // Re-throw to be caught by the main handler\n  }\n}\n\n/**\n * Process code changes and update related context\n *\n * @param {string} conversationId - Conversation ID\n * @param {Array} codeChanges - Array of code changes\n * @returns {Promise<object>} Processing results\n */\nasync function processCodeChanges(conversationId, codeChanges) {\n  try {\n    logMessage(\n      \"DEBUG\",\n      `Processing ${codeChanges.length} code changes for conversation ${conversationId}`\n    );\n\n    const result = {\n      focusChanged: false,\n      newFocus: null,\n    };\n\n    // If there are no code changes, return early\n    if (!codeChanges.length) {\n      return result;\n    }\n\n    // Process each code change using the knowledge processor\n    for (const change of codeChanges) {\n      try {\n        await KnowledgeProcessor.processCodeChange(change);\n        logMessage(\"DEBUG\", `Processed code change for ${change.path}`);\n      } catch (processErr) {\n        logMessage(\"WARN\", `Failed to process code change`, {\n          error: processErr.message,\n          path: change.path,\n        });\n        // Continue with next change\n      }\n    }\n\n    // Calculate new focus area based on code changes\n    const mostSignificantChange = codeChanges.reduce((prev, current) => {\n      // Simple heuristic: more changed lines = more significant\n      const prevChangedLines = prev.changedLines?.length || 0;\n      const currentChangedLines = current.changedLines?.length || 0;\n      return currentChangedLines > prevChangedLines ? current : prev;\n    }, codeChanges[0]);\n\n    // Set focus to the most significantly changed file\n    try {\n      await ActiveContextManager.setActiveFocus(\n        \"file\",\n        mostSignificantChange.path\n      );\n      result.focusChanged = true;\n      result.newFocus = {\n        type: \"file\",\n        identifier: mostSignificantChange.path,\n      };\n\n      logMessage(\"INFO\", `Set focus to most significantly changed file`, {\n        path: mostSignificantChange.path,\n        changedLines: mostSignificantChange.changedLines?.length || \"N/A\",\n      });\n    } catch (focusErr) {\n      logMessage(\"WARN\", `Failed to set focus to changed file`, {\n        error: focusErr.message,\n        path: mostSignificantChange.path,\n      });\n      // Continue without changing focus\n    }\n\n    // Record code changes in timeline\n    try {\n      await TimelineManagerLogic.recordEvent(\n        \"code_changes\",\n        {\n          count: codeChanges.length,\n          paths: codeChanges.map((c) => c.path),\n        },\n        [], // No specific entity IDs\n        conversationId\n      );\n      logMessage(\"DEBUG\", `Recorded code changes in timeline`);\n    } catch (timelineErr) {\n      logMessage(\"WARN\", `Failed to record code changes in timeline`, {\n        error: timelineErr.message,\n      });\n      // Non-critical error, continue\n    }\n\n    return result;\n  } catch (error) {\n    logMessage(\"ERROR\", `Error processing code changes`, {\n      error: error.message,\n      conversationId,\n    });\n    throw error; // Re-throw to be caught by the main handler\n  }\n}\n\n/**\n * Integrates previous and new context states\n *\n * @param {Object} previousContextState - Previous context state\n * @param {Object} changes - Change indicators (topic shift, intent transition, etc.)\n * @param {string} integrationLevel - How aggressively to integrate contexts\n * @returns {Promise<Object>} Integrated context\n */\nasync function _integrateContexts(\n  previousContextState,\n  changes,\n  integrationLevel\n) {\n  const {\n    topicShift,\n    intentTransition,\n    previousIntent,\n    currentIntent,\n    codeChanges,\n  } = changes;\n\n  try {\n    logMessage(\"INFO\", `Integrating contexts with level: ${integrationLevel}`);\n\n    // Start with a copy of the previous context\n    const integratedContext = { ...previousContextState };\n\n    // Determine how much to preserve based on integration level\n    switch (integrationLevel) {\n      case \"minimal\":\n        // For minimal integration, only keep core focus and clear most context\n        if (topicShift) {\n          // Clear most context but keep current focus\n          const currentFocus = integratedContext.focus;\n          integratedContext.recentContextItems = [];\n          integratedContext.focus = currentFocus;\n        }\n        break;\n\n      case \"aggressive\":\n        // For aggressive integration, preserve all context even with transitions\n        // Just update the intent/purpose information\n        if (intentTransition) {\n          integratedContext.currentIntent = currentIntent;\n        }\n        break;\n\n      case \"balanced\":\n      default:\n        // For balanced integration, preserve relevant context\n        if (topicShift) {\n          // Reduce context items but keep those relevant to current focus\n          const currentFocus = integratedContext.focus;\n\n          // Keep items that are still relevant to current focus or code changes\n          if (integratedContext.recentContextItems) {\n            const changedFilePaths = codeChanges.map(\n              (change) => change.filePath\n            );\n\n            integratedContext.recentContextItems =\n              integratedContext.recentContextItems.filter((item) => {\n                // Keep items related to current focus\n                if (\n                  item.relatedTo &&\n                  item.relatedTo.includes(currentFocus?.identifier)\n                ) {\n                  return true;\n                }\n\n                // Keep items related to changed files\n                if (\n                  item.path &&\n                  changedFilePaths.some((path) => item.path.includes(path))\n                ) {\n                  return true;\n                }\n\n                // Keep very recent items\n                if (\n                  item.timestamp &&\n                  Date.now() - item.timestamp < 5 * 60 * 1000\n                ) {\n                  // 5 minutes\n                  return true;\n                }\n\n                return false;\n              });\n          }\n        }\n\n        // Always update intent information\n        if (intentTransition) {\n          integratedContext.currentIntent = currentIntent;\n\n          // If we have code changes, adjust priorities based on new intent\n          if (codeChanges.length > 0 && integratedContext.recentContextItems) {\n            // Re-prioritize based on new intent\n            integratedContext.recentContextItems.forEach((item) => {\n              if (item.contentType === \"code\" && currentIntent) {\n                // Adjust priority based on relevance to new intent\n                if (\n                  currentIntent === \"debugging\" &&\n                  item.path &&\n                  item.path.includes(\"test\")\n                ) {\n                  item.priority = Math.min(item.priority + 0.2, 1.0);\n                } else if (\n                  currentIntent === \"feature_planning\" &&\n                  item.path &&\n                  item.path.includes(\"docs\")\n                ) {\n                  item.priority = Math.min(item.priority + 0.2, 1.0);\n                }\n                // Add more intent-specific priority adjustments as needed\n              }\n            });\n\n            // Sort by adjusted priority\n            integratedContext.recentContextItems.sort(\n              (a, b) => b.priority - a.priority\n            );\n          }\n        }\n        break;\n    }\n\n    return integratedContext;\n  } catch (error) {\n    logMessage(\"ERROR\", `Error integrating contexts`, {\n      error: error.message,\n    });\n    // Fall back to previous context in case of error\n    return previousContextState;\n  }\n}\n\n/**\n * Generates a synthesis of the current context\n *\n * @param {string} conversationId - Conversation ID\n * @param {string} currentIntent - Current conversation intent\n * @param {boolean} contextChanged - Whether context has significantly changed\n * @returns {Promise<Object>} Context synthesis\n */\nasync function generateContextSynthesis(\n  conversationId,\n  currentIntent,\n  contextChanged\n) {\n  try {\n    logMessage(\"INFO\", `Generating context synthesis`);\n\n    // Get active context information\n    const activeContext = await ActiveContextManager.getActiveContextState();\n    const activeFocus = await ActiveContextManager.getActiveFocus();\n\n    // Get recent messages for context\n    const recentMessages = await ConversationIntelligence.getRecentMessages(\n      conversationId,\n      5\n    );\n\n    // Generate a summary appropriate to the current state\n    let summaryText = \"Current conversation context\";\n\n    if (contextChanged) {\n      // More detailed summary for changed context\n      if (activeFocus) {\n        summaryText = `The conversation is now focused on ${activeFocus.type} \"${activeFocus.identifier}\"`;\n\n        if (currentIntent) {\n          summaryText += ` with the purpose of ${currentIntent.replace(\n            /_/g,\n            \" \"\n          )}`;\n        }\n      } else if (currentIntent) {\n        summaryText = `The conversation is focused on ${currentIntent.replace(\n          /_/g,\n          \" \"\n        )}`;\n      }\n\n      // Add recent message summary if available\n      if (recentMessages.length > 0) {\n        const messageContent = recentMessages\n          .map((msg) => msg.content)\n          .join(\" \");\n        const messageSummary = await ContextCompressorLogic.summarizeText(\n          messageContent,\n          { targetLength: 150 }\n        );\n\n        summaryText += `. Recent discussion: ${messageSummary}`;\n      }\n    } else {\n      // Simpler summary for continued context\n      if (activeFocus) {\n        summaryText = `Continuing focus on ${activeFocus.type} \"${activeFocus.identifier}\"`;\n\n        if (currentIntent) {\n          summaryText += ` with ${currentIntent.replace(/_/g, \" \")}`;\n        }\n      } else if (currentIntent) {\n        summaryText = `Continuing with ${currentIntent.replace(/_/g, \" \")}`;\n      }\n    }\n\n    // Identify top priorities based on current context\n    const topPriorities = [];\n\n    if (activeFocus) {\n      topPriorities.push(\n        `Focus on ${activeFocus.type}: ${activeFocus.identifier}`\n      );\n    }\n\n    if (currentIntent) {\n      switch (currentIntent) {\n        case \"debugging\":\n          topPriorities.push(\"Identify and fix issues in the code\");\n          break;\n        case \"feature_planning\":\n          topPriorities.push(\"Design and plan new features\");\n          break;\n        case \"code_review\":\n          topPriorities.push(\"Review code for quality and correctness\");\n          break;\n        case \"learning\":\n          topPriorities.push(\"Explain concepts and provide information\");\n          break;\n        case \"code_generation\":\n          topPriorities.push(\"Generate or modify code\");\n          break;\n        default:\n          topPriorities.push(\"Address user's current needs\");\n      }\n    }\n\n    // Include active context items as priorities if available\n    if (activeContext && activeContext.recentContextItems) {\n      const priorityItems = activeContext.recentContextItems\n        .slice(0, 2)\n        .map((item) => {\n          if (item.type === \"file\") {\n            return `Maintain context on file: ${item.name || item.path}`;\n          } else if (item.type === \"entity\") {\n            return `Keep focus on: ${item.name}`;\n          }\n          return null;\n        })\n        .filter(Boolean);\n\n      topPriorities.push(...priorityItems);\n    }\n\n    return {\n      summary: summaryText,\n      topPriorities: topPriorities.length > 0 ? topPriorities : undefined,\n    };\n  } catch (error) {\n    logMessage(\"ERROR\", `Error generating context synthesis`, {\n      error: error.message,\n    });\n    // Return minimal synthesis in case of error\n    return {\n      summary: \"Context updated\",\n    };\n  }\n}\n\n// Export the tool definition for server registration\nexport default {\n  name: \"update_conversation_context\",\n  description:\n    \"Updates an existing conversation context with new messages, code changes, and context management\",\n  inputSchema: updateConversationContextInputSchema,\n  outputSchema: updateConversationContextOutputSchema,\n  handler,\n};\n", "/**\n * KnowledgeProcessor.js\n *\n * Processes and analyzes code changes in the codebase.\n * Orchestrates the indexing and knowledge extraction from changed files.\n */\n\nimport * as ContextIndexerLogic from \"./ContextIndexerLogic.js\";\nimport { executeQuery } from \"../db.js\";\n\n/**\n * @typedef {Object} CodeEntity\n * @property {string} id - Unique identifier for the code entity\n * @property {string} path - File path of the code entity\n * @property {string} type - Type of code entity ('file', 'function', 'class', etc.)\n * @property {string} name - Name of the code entity\n * @property {string} content - Content of the code entity\n * @property {string} symbol_path - Full symbol path of the entity\n * @property {number} version - Version number of the entity\n * @property {string} parent_id - ID of the parent entity, if any\n * @property {string} created_at - Timestamp when entity was created\n * @property {string} updated_at - Timestamp when entity was last updated\n */\n\n/**\n * Process a single code change\n *\n * @param {Object} change - Object containing file change information\n * @param {string} change.filePath - Path to the changed file\n * @param {string} change.newContent - New content of the file\n * @param {string} [change.languageHint] - Optional language hint for the file\n * @returns {Promise<Object>} Result of processing the code change\n */\nexport async function processCodeChange(change) {\n  if (!change || !change.filePath || !change.newContent) {\n    console.error(\"Invalid code change object:\", change);\n    throw new Error(\"Invalid code change: missing required fields\");\n  }\n\n  try {\n    console.log(`Processing code change for ${change.filePath}`);\n\n    // Index the updated file\n    await ContextIndexerLogic.indexCodeFile(\n      change.filePath,\n      change.newContent,\n      change.languageHint\n    );\n\n    // Get the entities associated with this file\n    const entities = await getEntitiesFromChangedFiles([change.filePath]);\n\n    return {\n      filePath: change.filePath,\n      success: true,\n      entityCount: entities.length,\n      timestamp: new Date().toISOString(),\n    };\n  } catch (error) {\n    console.error(\n      `Error processing code change for ${change.filePath}:`,\n      error\n    );\n    throw new Error(`Failed to process code change: ${error.message}`);\n  }\n}\n\n/**\n * Process changes to multiple files in the codebase\n *\n * @param {Array<{filePath: string, newContent: string, languageHint: string}>} changedFiles - Array of changed files with their content and language\n * @returns {Promise<void>}\n */\nexport async function processCodebaseChanges(changedFiles) {\n  if (!changedFiles || changedFiles.length === 0) {\n    console.log(\"No files to process\");\n    return;\n  }\n\n  console.log(`Processing ${changedFiles.length} changed files...`);\n\n  try {\n    // Process each file in parallel using Promise.all\n    // Each file gets its own try/catch to prevent one failure from stopping the entire process\n    const processingPromises = changedFiles.map(async (file) => {\n      try {\n        await ContextIndexerLogic.indexCodeFile(\n          file.filePath,\n          file.newContent,\n          file.languageHint\n        );\n        return { filePath: file.filePath, success: true };\n      } catch (error) {\n        console.error(`Error processing file ${file.filePath}:`, error);\n        return {\n          filePath: file.filePath,\n          success: false,\n          error: error.message,\n        };\n      }\n    });\n\n    // Wait for all processing to complete\n    const results = await Promise.all(processingPromises);\n\n    // Count successes and failures\n    const successCount = results.filter((r) => r.success).length;\n    const failureCount = results.filter((r) => !r.success).length;\n\n    console.log(\n      `Completed processing ${changedFiles.length} files. Success: ${successCount}, Failures: ${failureCount}`\n    );\n\n    // If there were any failures, log them in detail\n    if (failureCount > 0) {\n      const failures = results.filter((r) => !r.success);\n      console.error(\n        \"Failed files:\",\n        failures.map((f) => f.filePath).join(\", \")\n      );\n    }\n  } catch (error) {\n    console.error(\"Error during codebase change processing:\", error);\n    throw error; // Rethrow to allow caller to handle the error\n  }\n}\n\n/**\n * Retrieves all code entities related to the provided file paths\n *\n * @param {string[]} filePaths - Array of file paths that have changed\n * @returns {Promise<CodeEntity[]>} Array of code entities related to the changed files\n */\nexport async function getEntitiesFromChangedFiles(filePaths) {\n  if (!filePaths || filePaths.length === 0) {\n    return [];\n  }\n\n  try {\n    // First query: Get all entities directly matching the file paths\n    const placeholders = filePaths.map(() => \"?\").join(\",\");\n    const query = `SELECT * FROM code_entities WHERE path IN (${placeholders})`;\n\n    const fileEntities = await executeQuery(query, filePaths);\n\n    // Get the IDs of the file entities to query for child entities\n    const fileEntityIds = fileEntities\n      .filter((entity) => entity.type === \"file\")\n      .map((entity) => entity.id);\n\n    // If we have file entities, query for their children\n    if (fileEntityIds.length > 0) {\n      const childPlaceholders = fileEntityIds.map(() => \"?\").join(\",\");\n      const childQuery = `SELECT * FROM code_entities WHERE parent_id IN (${childPlaceholders})`;\n\n      const childEntities = await executeQuery(childQuery, fileEntityIds);\n\n      // Combine file entities and their children, removing duplicates by ID\n      const allEntities = [...fileEntities];\n\n      // Add child entities that aren't already in the result set\n      const existingIds = new Set(allEntities.map((entity) => entity.id));\n\n      for (const childEntity of childEntities) {\n        if (!existingIds.has(childEntity.id)) {\n          allEntities.push(childEntity);\n          existingIds.add(childEntity.id);\n        }\n      }\n\n      return allEntities;\n    }\n\n    // If no file entities were found, just return what we have\n    return fileEntities;\n  } catch (error) {\n    console.error(\"Error retrieving entities from changed files:\", error);\n    throw error;\n  }\n}\n", "/**\n * retrieveRelevantContext.tool.js\n *\n * MCP tool implementation for retrieving and blending relevant context\n * from multiple sources: code, conversations, documentation, and patterns.\n */\n\nimport { z } from \"zod\";\nimport { executeQuery } from \"../db.js\";\nimport * as ConversationIntelligence from \"../logic/ConversationIntelligence.js\";\nimport * as InsightEngine from \"../logic/InsightEngine.js\";\nimport * as TimelineManagerLogic from \"../logic/TimelineManagerLogic.js\";\nimport * as ActiveContextManager from \"../logic/ActiveContextManager.js\";\nimport * as SmartSearchServiceLogic from \"../logic/SmartSearchServiceLogic.js\";\nimport * as RelationshipContextManagerLogic from \"../logic/RelationshipContextManagerLogic.js\";\nimport * as ConversationSegmenter from \"../logic/ConversationSegmenter.js\";\nimport * as ConversationPurposeDetector from \"../logic/ConversationPurposeDetector.js\";\nimport { DEFAULT_TOKEN_BUDGET } from \"../config.js\";\nimport { logMessage } from \"../utils/logger.js\";\n\nimport {\n  retrieveRelevantContextInputSchema,\n  retrieveRelevantContextOutputSchema,\n} from \"../schemas/toolSchemas.js\";\n\n/**\n * Handler for retrieve_relevant_context tool\n *\n * @param {object} input - Tool input parameters\n * @param {object} sdkContext - SDK context\n * @returns {Promise<object>} Tool output\n */\nasync function handler(input, sdkContext) {\n  try {\n    logMessage(\"INFO\", `retrieve_relevant_context tool started`, {\n      query: input.query?.substring(0, 50),\n      conversationId: input.conversationId,\n      tokenBudget: input.tokenBudget || DEFAULT_TOKEN_BUDGET,\n    });\n\n    // 1. Extract input parameters with defaults\n    const {\n      conversationId,\n      query,\n      tokenBudget = DEFAULT_TOKEN_BUDGET,\n      constraints = {},\n      contextFilters = {},\n      weightingStrategy = \"balanced\",\n      balanceStrategy = \"proportional\",\n      contextBalance = \"auto\",\n      sourceTypePreferences = {},\n    } = input;\n\n    // Validation\n    if (!query) {\n      const error = new Error(\"Query is required\");\n      error.code = \"MISSING_QUERY\";\n      throw error;\n    }\n\n    if (!conversationId) {\n      const error = new Error(\"Conversation ID is required\");\n      error.code = \"MISSING_CONVERSATION_ID\";\n      throw error;\n    }\n\n    logMessage(\"DEBUG\", `Context retrieval parameters`, {\n      balanceStrategy,\n      contextBalance,\n      constraints: Object.keys(constraints),\n      filters: Object.keys(contextFilters),\n    });\n\n    // 2. Fetch conversation history, current topic and purpose\n    let conversationHistory = [];\n    let currentTopic = null;\n    let currentPurpose = null;\n\n    try {\n      conversationHistory =\n        await ConversationIntelligence.getConversationHistory(\n          conversationId,\n          20 // Get last 20 messages\n        );\n\n      logMessage(\"DEBUG\", `Retrieved conversation history`, {\n        messageCount: conversationHistory.length,\n      });\n    } catch (err) {\n      logMessage(\"WARN\", `Failed to retrieve conversation history`, {\n        error: err.message,\n        conversationId,\n      });\n      // Continue with empty history\n    }\n\n    // Simple fallback approach - just return conversation history as context\n    // This bypasses the buggy search functionality for now\n    const simplifiedResult = {\n      relevantContext: [],\n      conversationContext: conversationHistory.map((msg) => ({\n        type: \"conversation\",\n        content: msg.content,\n        metadata: {\n          role: msg.role,\n          messageId: msg.messageId,\n        },\n        relevanceScore: 0.9,\n      })),\n      currentTopic,\n      currentPurpose,\n      statusMessage: \"Retrieved conversation context successfully\",\n      metrics: {\n        totalSnippets: conversationHistory.length,\n        relevanceThreshold: 0.5,\n        tokenUsage: conversationHistory.reduce(\n          (acc, msg) => acc + _estimateTokenCount(msg.content),\n          0\n        ),\n      },\n    };\n\n    logMessage(\n      \"INFO\",\n      `Returning simplified context with ${simplifiedResult.conversationContext.length} conversation messages`\n    );\n\n    return simplifiedResult;\n  } catch (error) {\n    logMessage(\"ERROR\", `Error in retrieve_relevant_context handler`, {\n      error: error.message,\n      code: error.code,\n    });\n\n    throw error;\n  }\n}\n\n/**\n * Apply context balance adjustments based on balance type and conversation purpose\n *\n * @param {object} contextSources - Context sources with target percentages\n * @param {string} contextBalance - Context balance strategy\n * @param {string} currentPurpose - Current conversation purpose\n */\nfunction _applyContextBalance(contextSources, contextBalance, currentPurpose) {\n  try {\n    logMessage(\"DEBUG\", `Applying context balance: ${contextBalance}`, {\n      currentPurpose,\n    });\n\n    if (contextBalance === \"auto\") {\n      // Automatic balancing based on purpose\n      if (currentPurpose) {\n        switch (currentPurpose) {\n          case \"code_explanation\":\n          case \"debugging\":\n            // More code when explaining or debugging\n            contextSources.code.targetPercentage = 0.7;\n            contextSources.conversation.targetPercentage = 0.1;\n            contextSources.documentation.targetPercentage = 0.15;\n            contextSources.patterns.targetPercentage = 0.05;\n            logMessage(\"DEBUG\", `Applied 'code_explanation/debugging' balance`);\n            break;\n          case \"implementation\":\n          case \"feature_development\":\n            // Balance between code and patterns when implementing\n            contextSources.code.targetPercentage = 0.6;\n            contextSources.conversation.targetPercentage = 0.1;\n            contextSources.documentation.targetPercentage = 0.1;\n            contextSources.patterns.targetPercentage = 0.2;\n            logMessage(\n              \"DEBUG\",\n              `Applied 'implementation/feature_development' balance`\n            );\n            break;\n          case \"architecture_discussion\":\n          case \"design\":\n            // More documentation and patterns for architecture\n            contextSources.code.targetPercentage = 0.4;\n            contextSources.conversation.targetPercentage = 0.15;\n            contextSources.documentation.targetPercentage = 0.25;\n            contextSources.patterns.targetPercentage = 0.2;\n            logMessage(\n              \"DEBUG\",\n              `Applied 'architecture_discussion/design' balance`\n            );\n            break;\n          case \"requirements_gathering\":\n          case \"clarification\":\n            // More conversation for requirements\n            contextSources.code.targetPercentage = 0.3;\n            contextSources.conversation.targetPercentage = 0.4;\n            contextSources.documentation.targetPercentage = 0.2;\n            contextSources.patterns.targetPercentage = 0.1;\n            logMessage(\n              \"DEBUG\",\n              `Applied 'requirements_gathering/clarification' balance`\n            );\n            break;\n          default:\n            // Default balance preserved\n            logMessage(\n              \"DEBUG\",\n              `No specific balance for purpose '${currentPurpose}', using defaults`\n            );\n            break;\n        }\n      } else {\n        logMessage(\"DEBUG\", `No current purpose, using default balance`);\n      }\n    } else if (contextBalance === \"code_heavy\") {\n      // Code-heavy balance\n      contextSources.code.targetPercentage = 0.8;\n      contextSources.conversation.targetPercentage = 0.1;\n      contextSources.documentation.targetPercentage = 0.05;\n      contextSources.patterns.targetPercentage = 0.05;\n      logMessage(\"DEBUG\", `Applied 'code_heavy' balance`);\n    } else if (contextBalance === \"conversation_focused\") {\n      // Conversation-focused balance\n      contextSources.code.targetPercentage = 0.3;\n      contextSources.conversation.targetPercentage = 0.5;\n      contextSources.documentation.targetPercentage = 0.1;\n      contextSources.patterns.targetPercentage = 0.1;\n      logMessage(\"DEBUG\", `Applied 'conversation_focused' balance`);\n    } else if (contextBalance === \"documentation_focused\") {\n      // Documentation-focused balance\n      contextSources.code.targetPercentage = 0.3;\n      contextSources.conversation.targetPercentage = 0.1;\n      contextSources.documentation.targetPercentage = 0.5;\n      contextSources.patterns.targetPercentage = 0.1;\n      logMessage(\"DEBUG\", `Applied 'documentation_focused' balance`);\n    } else if (contextBalance === \"pattern_focused\") {\n      // Pattern-focused balance\n      contextSources.code.targetPercentage = 0.3;\n      contextSources.conversation.targetPercentage = 0.1;\n      contextSources.documentation.targetPercentage = 0.1;\n      contextSources.patterns.targetPercentage = 0.5;\n      logMessage(\"DEBUG\", `Applied 'pattern_focused' balance`);\n    } else if (contextBalance === \"balanced\") {\n      // Balanced settings - already default\n      logMessage(\"DEBUG\", `Using balanced settings (default)`);\n    } else {\n      // If contextBalance doesn't match known values, log and keep defaults\n      logMessage(\n        \"WARN\",\n        `Unknown context balance type '${contextBalance}', using defaults`\n      );\n    }\n  } catch (error) {\n    logMessage(\"ERROR\", `Error applying context balance`, {\n      error: error.message,\n      contextBalance,\n    });\n    throw error;\n  }\n}\n\n/**\n * Integrate contexts from multiple sources respecting token budget\n *\n * @param {object} contextSources - Context sources with snippets\n * @param {number} tokenBudget - Total token budget\n * @param {string} balanceStrategy - Strategy for balancing context\n * @param {string} query - Original query for relevance calculations\n * @returns {Array} Integrated context snippets\n */\nfunction _integrateContexts(\n  contextSources,\n  tokenBudget,\n  balanceStrategy,\n  query\n) {\n  try {\n    logMessage(\n      \"DEBUG\",\n      `Integrating contexts with strategy: ${balanceStrategy}`\n    );\n\n    let integratedSnippets = [];\n\n    // Apply the selected balance strategy\n    switch (balanceStrategy) {\n      case \"proportional\":\n        integratedSnippets = _applyProportionalStrategy(\n          contextSources,\n          tokenBudget\n        );\n        break;\n      case \"equal_representation\":\n        integratedSnippets = _applyEqualRepresentationStrategy(\n          contextSources,\n          tokenBudget\n        );\n        break;\n      case \"priority_based\":\n        // Collect all snippets and sort by relevance\n        const allSnippets = [\n          ...contextSources.code.snippets,\n          ...contextSources.conversation.snippets,\n          ...contextSources.documentation.snippets,\n          ...contextSources.patterns.snippets,\n        ].sort((a, b) => b.relevanceScore - a.relevanceScore);\n\n        integratedSnippets = _applyPriorityBasedStrategy(\n          allSnippets,\n          tokenBudget\n        );\n        break;\n      default:\n        // Fallback to proportional\n        logMessage(\n          \"WARN\",\n          `Unknown balance strategy '${balanceStrategy}', falling back to proportional`\n        );\n        integratedSnippets = _applyProportionalStrategy(\n          contextSources,\n          tokenBudget\n        );\n    }\n\n    // Log the result\n    const typeBreakdown = {\n      code: integratedSnippets.filter((s) => s.type === \"code\").length,\n      conversation: integratedSnippets.filter((s) => s.type === \"conversation\")\n        .length,\n      documentation: integratedSnippets.filter(\n        (s) => s.type === \"documentation\"\n      ).length,\n      patterns: integratedSnippets.filter((s) => s.type === \"pattern\").length,\n    };\n\n    logMessage(\n      \"DEBUG\",\n      `Integrated ${integratedSnippets.length} snippets`,\n      typeBreakdown\n    );\n\n    return integratedSnippets;\n  } catch (error) {\n    logMessage(\"ERROR\", `Error integrating contexts`, {\n      error: error.message,\n      balanceStrategy,\n    });\n    throw error;\n  }\n}\n\n/**\n * Apply proportional balancing strategy for context integration\n *\n * @param {object} contextSources - Context sources with snippets\n * @param {number} tokenBudget - Total token budget\n * @returns {Array} Balanced context snippets\n */\nfunction _applyProportionalStrategy(contextSources, tokenBudget) {\n  try {\n    logMessage(\n      \"DEBUG\",\n      `Applying proportional strategy with budget: ${tokenBudget}`\n    );\n\n    const result = [];\n    let remainingBudget = tokenBudget;\n    const unusedBudgets = {};\n\n    // First pass: allocate tokens based on target percentages\n    for (const [sourceType, source] of Object.entries(contextSources)) {\n      // Skip empty sources\n      if (!source.snippets || source.snippets.length === 0) {\n        unusedBudgets[sourceType] = Math.floor(\n          tokenBudget * source.targetPercentage\n        );\n        logMessage(\n          \"DEBUG\",\n          `No snippets for ${sourceType}, reserving ${unusedBudgets[sourceType]} tokens`\n        );\n        continue;\n      }\n\n      // Calculate token budget for this source\n      const sourceBudget = Math.floor(tokenBudget * source.targetPercentage);\n\n      // Sort snippets by relevance score\n      const sortedSnippets = [...source.snippets].sort(\n        (a, b) => b.relevanceScore - a.relevanceScore\n      );\n\n      // Add snippets until budget is exhausted\n      let usedBudget = 0;\n      for (const snippet of sortedSnippets) {\n        if (usedBudget + snippet.tokenEstimate <= sourceBudget) {\n          result.push(snippet);\n          usedBudget += snippet.tokenEstimate;\n        } else {\n          // If the snippet doesn't fit, try the next one (might be smaller)\n          continue;\n        }\n      }\n\n      // Track unused budget for redistribution\n      if (usedBudget < sourceBudget) {\n        unusedBudgets[sourceType] = sourceBudget - usedBudget;\n        logMessage(\n          \"DEBUG\",\n          `${sourceType} used ${usedBudget}/${sourceBudget} tokens, ${unusedBudgets[sourceType]} unused`\n        );\n      } else {\n        unusedBudgets[sourceType] = 0;\n      }\n\n      remainingBudget -= usedBudget;\n    }\n\n    // Second pass: redistribute unused budget\n    if (remainingBudget > 0) {\n      logMessage(\"DEBUG\", `Redistributing ${remainingBudget} unused tokens`);\n\n      // Collect all remaining snippets\n      const remainingSnippets = [];\n      for (const [sourceType, source] of Object.entries(contextSources)) {\n        if (!source.snippets) continue;\n\n        const usedSnippetIds = new Set(\n          result.filter((s) => s.type === sourceType).map((s) => s.entity_id)\n        );\n\n        const unusedSnippets = source.snippets.filter(\n          (s) => !usedSnippetIds.has(s.entity_id)\n        );\n\n        remainingSnippets.push(...unusedSnippets);\n      }\n\n      // Sort by relevance score\n      remainingSnippets.sort((a, b) => b.relevanceScore - a.relevanceScore);\n\n      // Add snippets until remaining budget is exhausted\n      for (const snippet of remainingSnippets) {\n        if (snippet.tokenEstimate <= remainingBudget) {\n          result.push(snippet);\n          remainingBudget -= snippet.tokenEstimate;\n        }\n\n        if (remainingBudget <= 0) break;\n      }\n    }\n\n    logMessage(\n      \"DEBUG\",\n      `Applied proportional strategy, selected ${result.length} snippets with ${remainingBudget} tokens remaining`\n    );\n    return result;\n  } catch (error) {\n    logMessage(\"ERROR\", `Error applying proportional strategy`, {\n      error: error.message,\n      tokenBudget,\n    });\n    throw error;\n  }\n}\n\n/**\n * Apply equal representation balancing strategy for context integration\n *\n * @param {object} contextSources - Context sources with snippets\n * @param {number} tokenBudget - Total token budget\n * @returns {Array} Balanced context snippets\n */\nfunction _applyEqualRepresentationStrategy(contextSources, tokenBudget) {\n  try {\n    logMessage(\n      \"DEBUG\",\n      `Applying equal representation strategy with budget: ${tokenBudget}`\n    );\n\n    const result = [];\n    let remainingBudget = tokenBudget;\n\n    // Count non-empty sources\n    const nonEmptySources = Object.values(contextSources).filter(\n      (source) => source.snippets && source.snippets.length > 0\n    ).length;\n\n    if (nonEmptySources === 0) {\n      logMessage(\"WARN\", `No non-empty sources found for equal representation`);\n      return [];\n    }\n\n    // Allocate equal budget to each non-empty source\n    const budgetPerSource = Math.floor(tokenBudget / nonEmptySources);\n    logMessage(\"DEBUG\", `Allocating ${budgetPerSource} tokens per source`);\n\n    // First pass: add snippets from each source up to its equal share\n    for (const [sourceType, source] of Object.entries(contextSources)) {\n      if (!source.snippets || source.snippets.length === 0) continue;\n\n      // Sort snippets by relevance score\n      const sortedSnippets = [...source.snippets].sort(\n        (a, b) => b.relevanceScore - a.relevanceScore\n      );\n\n      // Add snippets until budget is exhausted\n      let usedBudget = 0;\n      for (const snippet of sortedSnippets) {\n        if (usedBudget + snippet.tokenEstimate <= budgetPerSource) {\n          result.push(snippet);\n          usedBudget += snippet.tokenEstimate;\n          remainingBudget -= snippet.tokenEstimate;\n        }\n      }\n    }\n\n    // Second pass: use remaining budget for highest relevance snippets\n    if (remainingBudget > 0) {\n      logMessage(\n        \"DEBUG\",\n        `Redistributing ${remainingBudget} unused tokens based on relevance`\n      );\n\n      // Collect all remaining snippets\n      const remainingSnippets = [];\n      for (const source of Object.values(contextSources)) {\n        if (!source.snippets) continue;\n\n        const usedSnippetIds = new Set(result.map((s) => s.entity_id));\n\n        const unusedSnippets = source.snippets.filter(\n          (s) => !usedSnippetIds.has(s.entity_id)\n        );\n\n        remainingSnippets.push(...unusedSnippets);\n      }\n\n      // Sort by relevance score\n      remainingSnippets.sort((a, b) => b.relevanceScore - a.relevanceScore);\n\n      // Add snippets until remaining budget is exhausted\n      for (const snippet of remainingSnippets) {\n        if (snippet.tokenEstimate <= remainingBudget) {\n          result.push(snippet);\n          remainingBudget -= snippet.tokenEstimate;\n        }\n\n        if (remainingBudget <= 0) break;\n      }\n    }\n\n    logMessage(\n      \"DEBUG\",\n      `Applied equal representation strategy, selected ${result.length} snippets with ${remainingBudget} tokens remaining`\n    );\n    return result;\n  } catch (error) {\n    logMessage(\"ERROR\", `Error applying equal representation strategy`, {\n      error: error.message,\n      tokenBudget,\n    });\n    throw error;\n  }\n}\n\n/**\n * Apply priority-based balancing strategy for context integration\n *\n * @param {Array} allSnippets - All snippets sorted by relevance\n * @param {number} tokenBudget - Total token budget\n * @returns {Array} Selected context snippets\n */\nfunction _applyPriorityBasedStrategy(allSnippets, tokenBudget) {\n  try {\n    logMessage(\n      \"DEBUG\",\n      `Applying priority-based strategy with budget: ${tokenBudget}`\n    );\n\n    const result = [];\n    let usedBudget = 0;\n\n    // Add snippets until budget is exhausted\n    for (const snippet of allSnippets) {\n      if (usedBudget + snippet.tokenEstimate <= tokenBudget) {\n        result.push(snippet);\n        usedBudget += snippet.tokenEstimate;\n      } else {\n        // If the snippet doesn't fit, try the next one (might be smaller)\n        continue;\n      }\n    }\n\n    logMessage(\n      \"DEBUG\",\n      `Applied priority-based strategy, selected ${result.length} snippets, using ${usedBudget}/${tokenBudget} tokens`\n    );\n    return result;\n  } catch (error) {\n    logMessage(\"ERROR\", `Error applying priority-based strategy`, {\n      error: error.message,\n      tokenBudget,\n    });\n    throw error;\n  }\n}\n\n/**\n * Estimate token count for a text\n *\n * @param {string} text - Text to estimate token count for\n * @returns {number} Estimated token count\n */\nfunction _estimateTokenCount(text) {\n  try {\n    if (!text) return 0;\n    // Simple estimation: ~4 characters per token on average\n    return Math.ceil(text.length / 4);\n  } catch (error) {\n    logMessage(\"WARN\", `Error estimating token count`, {\n      error: error.message,\n      textLength: text?.length || 0,\n    });\n    // Return a safe default\n    return text ? Math.ceil(text.length / 4) : 0;\n  }\n}\n\n/**\n * Generate source attribution for a snippet\n *\n * @param {object} snippet - Context snippet\n * @returns {string} Source attribution\n */\nfunction _generateSourceAttribution(snippet) {\n  try {\n    switch (snippet.type) {\n      case \"code\":\n        return `Source: ${snippet.metadata.path || \"Code\"} (${\n          snippet.metadata.entityType || \"entity\"\n        })`;\n      case \"conversation\":\n        const timestampStr = snippet.metadata.timestamp\n          ? new Date(snippet.metadata.timestamp).toLocaleString()\n          : \"Unknown time\";\n        return `From ${\n          snippet.metadata.role || \"conversation\"\n        } (${timestampStr})`;\n      case \"documentation\":\n        return `Documentation: ${\n          snippet.metadata.title || snippet.metadata.path || \"Unknown\"\n        }`;\n      case \"pattern\":\n        return `Pattern: ${snippet.metadata.name || \"Unknown\"} (${\n          snippet.metadata.patternType || \"general\"\n        })`;\n      default:\n        return `Source: ${snippet.type}`;\n    }\n  } catch (error) {\n    logMessage(\"WARN\", `Error generating source attribution`, {\n      error: error.message,\n      snippetType: snippet?.type,\n    });\n    // Return a generic attribution as fallback\n    return \"Source information unavailable\";\n  }\n}\n\n/**\n * Generate relevance explanation for a snippet\n *\n * @param {object} snippet - Context snippet\n * @param {string} query - Original query\n * @returns {string} Relevance explanation\n */\nfunction _generateRelevanceExplanation(snippet, query) {\n  try {\n    const relevanceScore = snippet.relevanceScore || 0;\n    const formattedScore = (relevanceScore * 100).toFixed(0);\n\n    let explanation = `Relevance: ${formattedScore}%`;\n\n    // Add type-specific explanations\n    switch (snippet.type) {\n      case \"code\":\n        explanation += ` - This code ${\n          relevanceScore > 0.8\n            ? \"directly addresses\"\n            : relevanceScore > 0.6\n            ? \"is closely related to\"\n            : \"may be relevant to\"\n        } your query about \"${query.substring(0, 30)}${\n          query.length > 30 ? \"...\" : \"\"\n        }\"`;\n        break;\n      case \"conversation\":\n        explanation += ` - This prior conversation ${\n          relevanceScore > 0.8\n            ? \"directly addresses\"\n            : relevanceScore > 0.6\n            ? \"discusses\"\n            : \"mentions\"\n        } similar topics to your current query`;\n        break;\n      case \"documentation\":\n        explanation += ` - This documentation ${\n          relevanceScore > 0.8\n            ? \"provides key information about\"\n            : relevanceScore > 0.6\n            ? \"explains\"\n            : \"contains information related to\"\n        } concepts in your query`;\n        break;\n      case \"pattern\":\n        explanation += ` - This pattern ${\n          relevanceScore > 0.8\n            ? \"is highly applicable to\"\n            : relevanceScore > 0.6\n            ? \"may be useful for\"\n            : \"could provide insights for\"\n        } your current task`;\n        break;\n      default:\n        explanation += ` - This content appears relevant to your query`;\n    }\n\n    return explanation;\n  } catch (error) {\n    logMessage(\"WARN\", `Error generating relevance explanation`, {\n      error: error.message,\n      snippetType: snippet?.type,\n    });\n    // Return a generic explanation as fallback\n    return `Relevance score: ${((snippet?.relevanceScore || 0) * 100).toFixed(\n      0\n    )}%`;\n  }\n}\n\n// Export the tool definition for server registration\nexport default {\n  name: \"retrieve_relevant_context\",\n  description:\n    \"Retrieves context from multiple sources that is relevant to the current query or conversation\",\n  inputSchema: retrieveRelevantContextInputSchema,\n  outputSchema: retrieveRelevantContextOutputSchema,\n  handler,\n};\n", "/**\n * InsightEngine.js\n *\n * Orchestrates the intelligent retrieval pipeline, combining intent prediction,\n * smart search, entity relationships, context prioritization, and compression\n * to provide the most relevant context for a given query.\n */\n\nimport * as IntentPredictorLogic from \"./IntentPredictorLogic.js\";\nimport * as SmartSearchServiceLogic from \"./SmartSearchServiceLogic.js\";\nimport * as RelationshipContextManagerLogic from \"./RelationshipContextManagerLogic.js\";\nimport * as ContextPrioritizerLogic from \"./ContextPrioritizerLogic.js\";\nimport * as ContextCompressorLogic from \"./ContextCompressorLogic.js\";\nimport * as ActiveContextManager from \"./ActiveContextManager.js\";\n\n/**\n * @typedef {Object} Message\n * @property {string} messageId - Unique identifier for the message\n * @property {string} conversationId - ID of the conversation this message belongs to\n * @property {string} role - Role of the message sender (e.g., 'user', 'assistant')\n * @property {string} content - Content of the message\n * @property {Date} timestamp - When the message was sent\n * @property {string[]} [relatedContextEntityIds] - IDs of related code entities\n * @property {string} [summary] - Summary of the message content\n * @property {string} [userIntent] - Inferred user intent\n * @property {string} [topicSegmentId] - ID of topic segment this message belongs to\n * @property {string[]} [semanticMarkers] - Semantic markers for enhanced retrieval\n * @property {Object} [sentimentIndicators] - Sentiment analysis results\n */\n\n/**\n * @typedef {Object} FocusArea\n * @property {string} focus_id - Unique identifier for the focus area\n * @property {string} focus_type - Type of focus area ('file', 'directory', 'task_type')\n * @property {string} identifier - Primary identifier for the focus area (e.g., file path)\n * @property {string} description - Human-readable description of the focus area\n * @property {string[]} related_entity_ids - Array of related entity IDs\n * @property {string[]} keywords - Array of keywords related to this focus area\n * @property {number} last_activated_at - Timestamp when this focus area was last active\n * @property {boolean} is_active - Whether this focus area is currently active\n */\n\n/**\n * @typedef {Object} Snippet\n * @property {string} entity_id - ID of the entity\n * @property {string} summarizedContent - Compressed/summarized content\n * @property {number} originalScore - Original relevance score\n * @property {string} type - Type of snippet\n */\n\n/**\n * @typedef {Object} SearchResult\n * @property {Object} entity - The found code entity\n * @property {number} relevanceScore - Relevance score for the search result\n */\n\n/**\n * @typedef {Object} ContextSnippet\n * @property {string} entity_id - ID of the entity\n * @property {string} content - Content of the entity\n * @property {string} type - Type of entity\n * @property {string} path - Path to the entity file\n * @property {string} name - Name of the entity\n * @property {number} baseRelevance - Base relevance score\n * @property {Object} metadata - Additional entity metadata\n */\n\n/**\n * Orchestrates the full retrieval pipeline to get the most relevant snippets for a query\n *\n * @param {string} query - The user's query\n * @param {Message[]} conversationHistory - Array of conversation messages\n * @param {FocusArea|null} currentFocusOverride - Optional override for the current focus area\n * @param {number} tokenBudget - Maximum token budget for returned context\n * @param {Object} [constraints] - Optional constraints for search\n * @returns {Promise<Snippet[]>} Array of relevant context snippets\n */\nexport async function orchestrateRetrieval(\n  query,\n  conversationHistory,\n  currentFocusOverride,\n  tokenBudget,\n  constraints = {}\n) {\n  try {\n    console.log(\n      `[InsightEngine] Orchestrating retrieval for query: \"${query}\"`\n    );\n\n    // 1. Determine current focus\n    const currentFocus =\n      currentFocusOverride || (await ActiveContextManager.getActiveFocus());\n    console.log(\n      `[InsightEngine] Using focus: ${\n        currentFocus ? currentFocus.identifier : \"None\"\n      }`\n    );\n\n    // 2. Get intent and refined keywords from query and conversation history\n    const { intent, keywords } =\n      await IntentPredictorLogic.inferIntentFromQuery(\n        query,\n        conversationHistory.slice(-5) // Use last 5 messages for context\n      );\n\n    // Ensure keywords are strings (they might be objects or other types)\n    let processedKeywords = [];\n    if (Array.isArray(keywords)) {\n      processedKeywords = keywords\n        .map((kw) => {\n          // Check if it's a string already\n          if (typeof kw === \"string\") {\n            return kw;\n          }\n          // If it's an object with a 'token' property\n          if (kw && typeof kw === \"object\" && kw.token) {\n            return kw.token;\n          }\n          // Convert to string if possible\n          return String(kw);\n        })\n        .filter((kw) => kw && kw.length > 0); // Remove empty entries\n    }\n\n    // If we ended up with no keywords, extract directly from the query\n    if (processedKeywords.length === 0) {\n      processedKeywords = query.split(/\\s+/).filter((word) => word.length > 2);\n    }\n\n    console.log(\n      `[InsightEngine] Detected intent: ${intent}, extracted keywords: ${processedKeywords.join(\n        \", \"\n      )}`\n    );\n\n    // 3. Call SmartSearchServiceLogic.searchByKeywords with refined keywords\n    const searchOptions = {\n      ...constraints,\n      limit: constraints.limit || 20, // Default to 20 results initially\n      strategy: constraints.strategy || \"combined\", // Default to combined search strategy\n    };\n\n    const searchResults = await SmartSearchServiceLogic.searchByKeywords(\n      processedKeywords,\n      searchOptions\n    );\n\n    console.log(\n      `[InsightEngine] Initial search returned ${searchResults.length} results`\n    );\n\n    // Skip further processing if no results\n    if (!searchResults || searchResults.length === 0) {\n      return [];\n    }\n\n    // 4. For highly relevant candidates, explore related entities\n    const relatedEntities = [];\n\n    // Process most relevant candidates (top 5 or 25% of results, whichever is smaller)\n    const topResultsCount = Math.min(5, Math.ceil(searchResults.length * 0.25));\n    const topResults = searchResults.slice(0, topResultsCount);\n\n    // For each top result, explore relationships\n    for (const result of topResults) {\n      const entityId = result.entity.entity_id;\n\n      // Use appropriate method based on entity type\n      if (\n        result.entity.entity_type === \"function\" ||\n        result.entity.entity_type === \"method\"\n      ) {\n        try {\n          // Get call graph for functions/methods\n          const callGraph =\n            await RelationshipContextManagerLogic.buildCallGraphSnippet(\n              entityId,\n              1 // Depth of 1 to avoid too much expansion\n            );\n\n          // Extract entities from call graph\n          if (callGraph && callGraph.nodes) {\n            for (const node of callGraph.nodes) {\n              // Skip if it's the same as the source entity\n              if (node.id !== entityId) {\n                relatedEntities.push({\n                  entity_id: node.id,\n                  relationship: \"call_graph\",\n                  source_entity_id: entityId,\n                });\n              }\n            }\n          }\n        } catch (error) {\n          console.warn(\n            `[InsightEngine] Error building call graph for ${entityId}:`,\n            error\n          );\n        }\n      } else {\n        try {\n          // Get general relationships for other entity types\n          const relationships =\n            await RelationshipContextManagerLogic.getRelationships(\n              entityId,\n              [\"imports\", \"uses\", \"inherits\", \"implements\"],\n              2 // Limit to 2 relationships per type\n            );\n\n          // Add related entities\n          for (const rel of relationships) {\n            relatedEntities.push({\n              entity_id: rel.target_entity_id,\n              relationship: rel.relationship_type,\n              source_entity_id: entityId,\n            });\n          }\n        } catch (error) {\n          console.warn(\n            `[InsightEngine] Error getting relationships for ${entityId}:`,\n            error\n          );\n        }\n      }\n    }\n\n    console.log(\n      `[InsightEngine] Found ${relatedEntities.length} related entities`\n    );\n\n    // 5. Fetch full details of related entities and combine with search results\n    const relatedEntityResults = [];\n\n    // Create a Set of existing entity IDs to avoid duplicates\n    const existingEntityIds = new Set(\n      searchResults.map((result) => result.entity.entity_id)\n    );\n\n    // Process each related entity\n    for (const relatedEntity of relatedEntities) {\n      // Skip if we already have this entity\n      if (existingEntityIds.has(relatedEntity.entity_id)) {\n        continue;\n      }\n\n      try {\n        // Query for entity details\n        const entities = await SmartSearchServiceLogic.searchByEntityIds([\n          relatedEntity.entity_id,\n        ]);\n\n        if (entities && entities.length > 0) {\n          // Add to results list with a slightly lower score\n          const sourceResult = searchResults.find(\n            (r) => r.entity.entity_id === relatedEntity.source_entity_id\n          );\n\n          // Base the relevance on the source entity, but reduce it\n          const relevanceScore = sourceResult\n            ? Math.max(0.4, sourceResult.relevanceScore * 0.8)\n            : 0.4;\n\n          relatedEntityResults.push({\n            entity: entities[0],\n            relevanceScore,\n          });\n\n          // Add to set to avoid duplicates\n          existingEntityIds.add(relatedEntity.entity_id);\n        }\n      } catch (error) {\n        console.warn(\n          `[InsightEngine] Error fetching details for related entity ${relatedEntity.entity_id}:`,\n          error\n        );\n      }\n    }\n\n    // Combine original search results with related entities\n    const combinedResults = [...searchResults, ...relatedEntityResults];\n    console.log(\n      `[InsightEngine] Combined results: ${combinedResults.length} entities`\n    );\n\n    // 6. Convert all candidate entities to ContextSnippet format\n    const contextSnippets = combinedResults.map((result) => ({\n      entity_id: result.entity.entity_id,\n      content: result.entity.raw_content || \"\",\n      type: result.entity.entity_type || \"unknown\",\n      path: result.entity.file_path || \"\",\n      name: result.entity.name || \"\",\n      baseRelevance: result.relevanceScore,\n      metadata: {\n        symbolPath: result.entity.symbol_path,\n        parentId: result.entity.parent_entity_id,\n        version: result.entity.version,\n      },\n      entity: result.entity, // Include full entity for prioritization logic\n    }));\n\n    // 7. Prioritize the context snippets\n    const prioritizedSnippets =\n      await ContextPrioritizerLogic.prioritizeContexts(\n        contextSnippets,\n        processedKeywords,\n        currentFocus,\n        Math.max(50, contextSnippets.length) // Higher limit to ensure we consider all snippets\n      );\n\n    console.log(\n      `[InsightEngine] Prioritized ${prioritizedSnippets.length} context snippets`\n    );\n\n    // 8. Manage token budget to get final set of snippets\n    const processedSnippets = await ContextCompressorLogic.manageTokenBudget(\n      prioritizedSnippets.map((snippet) => ({\n        ...snippet,\n        score: snippet.relevanceScore || snippet.baseRelevance,\n      })),\n      tokenBudget,\n      processedKeywords\n    );\n\n    console.log(\n      `[InsightEngine] Final result: ${processedSnippets.length} processed snippets within token budget`\n    );\n\n    // 9. Map the processed snippets to the expected Snippet format\n    const finalSnippets = processedSnippets.map((snippet) => ({\n      entity_id: snippet.entity_id,\n      summarizedContent: snippet.summarizedContent,\n      originalScore: snippet.originalScore || snippet.score,\n      type:\n        snippet.type ||\n        (snippet.entity && snippet.entity.entity_type) ||\n        \"unknown\",\n    }));\n\n    return finalSnippets;\n  } catch (error) {\n    console.error(\"[InsightEngine] Error orchestrating retrieval:\", error);\n    return []; // Return empty array on error\n  }\n}\n", "/**\n * recordMilestoneContext.tool.js\n *\n * MCP tool implementation for recording milestone context\n * This tool creates a snapshot of the current context and performs\n * impact analysis for major milestones during development.\n */\n\nimport { z } from \"zod\";\nimport { v4 as uuidv4 } from \"uuid\";\nimport { executeQuery } from \"../db.js\";\nimport * as ActiveContextManager from \"../logic/ActiveContextManager.js\";\nimport * as TimelineManagerLogic from \"../logic/TimelineManagerLogic.js\";\nimport * as LearningSystem from \"../logic/LearningSystem.js\";\nimport * as RelationshipContextManagerLogic from \"../logic/RelationshipContextManagerLogic.js\";\nimport * as SmartSearchServiceLogic from \"../logic/SmartSearchServiceLogic.js\";\nimport { logMessage } from \"../utils/logger.js\";\n\nimport {\n  recordMilestoneContextInputSchema,\n  recordMilestoneContextOutputSchema,\n} from \"../schemas/toolSchemas.js\";\n\n/**\n * Handler for record_milestone_context tool\n *\n * @param {object} input - Tool input parameters\n * @param {object} sdkContext - SDK context\n * @returns {Promise<object>} Tool output\n */\nasync function handler(input, sdkContext) {\n  try {\n    logMessage(\"INFO\", `record_milestone_context tool started`, {\n      milestoneName: input.name,\n      category: input.milestoneCategory || \"uncategorized\",\n      conversationId: input.conversationId,\n    });\n\n    // 1. Extract input parameters\n    const {\n      conversationId,\n      name,\n      description = \"\",\n      customData = {},\n      milestoneCategory = \"uncategorized\",\n      assessImpact = true,\n    } = input;\n\n    // Validate essential parameters\n    if (!name) {\n      const error = new Error(\"Milestone name is required\");\n      error.code = \"MISSING_NAME\";\n      throw error;\n    }\n\n    // 2. Gather active context\n    let activeContextEntities = [];\n    let activeFocus = null;\n    let activeContextIds = [];\n\n    try {\n      activeContextEntities =\n        await ActiveContextManager.getActiveContextAsEntities();\n      activeFocus = await ActiveContextManager.getActiveFocus();\n      activeContextIds = activeContextEntities.map((entity) => entity.id);\n\n      logMessage(\"DEBUG\", `Retrieved active context`, {\n        entityCount: activeContextIds.length,\n        hasFocus: !!activeFocus,\n      });\n    } catch (contextErr) {\n      logMessage(\n        \"WARN\",\n        `Error retrieving active context, continuing with empty context`,\n        {\n          error: contextErr.message,\n        }\n      );\n      // Continue with empty context instead of failing\n    }\n\n    // 3. Create the snapshot data\n    const snapshotData = {\n      milestoneCategory,\n      name,\n      description,\n      activeFocus,\n      entityIds: activeContextIds,\n      customData,\n      timestamp: Date.now(),\n      conversationId,\n    };\n\n    // 4. Record the milestone event in the timeline\n    let milestoneEventId;\n    try {\n      milestoneEventId = await TimelineManagerLogic.recordEvent(\n        \"milestone_created\",\n        {\n          name,\n          category: milestoneCategory,\n          entityCount: activeContextIds.length,\n          timestamp: Date.now(),\n        },\n        activeContextIds,\n        conversationId\n      );\n      logMessage(\"DEBUG\", `Recorded milestone event in timeline`, {\n        eventId: milestoneEventId,\n      });\n    } catch (timelineErr) {\n      logMessage(\"ERROR\", `Failed to record milestone event in timeline`, {\n        error: timelineErr.message,\n        name,\n        category: milestoneCategory,\n      });\n      // This is a critical failure, rethrow\n      throw timelineErr;\n    }\n\n    // 5. Create snapshot in the database\n    let milestoneId;\n    try {\n      milestoneId = await TimelineManagerLogic.createSnapshot(\n        snapshotData,\n        name,\n        description,\n        milestoneEventId\n      );\n      logMessage(\"INFO\", `Created milestone with ID: ${milestoneId}`);\n    } catch (snapshotErr) {\n      logMessage(\"ERROR\", `Failed to create milestone snapshot`, {\n        error: snapshotErr.message,\n        name,\n        eventId: milestoneEventId,\n      });\n      // This is a critical failure, rethrow\n      throw snapshotErr;\n    }\n\n    // 6. Initialize impact assessment result\n    let impactAssessment = null;\n\n    // 7. If impact assessment is requested, perform it\n    if (assessImpact) {\n      try {\n        logMessage(\"INFO\", `Starting impact assessment for milestone`, {\n          milestoneId,\n          category: milestoneCategory,\n        });\n        impactAssessment = await _assessMilestoneImpact(\n          milestoneId,\n          milestoneCategory,\n          activeContextIds\n        );\n      } catch (impactErr) {\n        logMessage(\"WARN\", `Failed to assess milestone impact`, {\n          error: impactErr.message,\n          milestoneId,\n        });\n        // Set a basic impact assessment with error information\n        impactAssessment = {\n          impactScore: 0,\n          impactLevel: \"unknown\",\n          impactSummary: `Unable to assess impact: ${impactErr.message}`,\n          error: impactErr.message,\n          scopeMetrics: {\n            directlyModifiedEntities: activeContextIds.length,\n            potentiallyImpactedEntities: 0,\n            impactedComponents: 0,\n            criticalPathsCount: 0,\n          },\n        };\n      }\n    } else {\n      logMessage(\"DEBUG\", `Skipping impact assessment (not requested)`);\n    }\n\n    // 8. Trigger background pattern analysis (don't await)\n    setTimeout(() => {\n      logMessage(\n        \"DEBUG\",\n        `Starting background pattern analysis for milestone: ${milestoneId}`\n      );\n      LearningSystem.analyzePatternsAroundMilestone(milestoneId).catch(\n        (error) => {\n          logMessage(\"ERROR\", `Error in background pattern analysis`, {\n            error: error.message,\n            milestoneId,\n          });\n        }\n      );\n    }, 100);\n\n    // 9. Return the tool response\n    logMessage(\"INFO\", `record_milestone_context tool completed successfully`, {\n      milestoneId,\n      entityCount: activeContextIds.length,\n      hasImpactAssessment: !!impactAssessment,\n    });\n\n    const responseData = {\n      message: `Milestone \"${name}\" recorded successfully with ${activeContextIds.length} related entities.`,\n      milestoneId,\n      status: \"success\",\n      milestoneCategory,\n      relatedEntitiesCount: activeContextIds.length,\n      impactAssessment,\n    };\n\n    return {\n      content: [\n        {\n          type: \"text\",\n          text: JSON.stringify(responseData),\n        },\n      ],\n    };\n  } catch (error) {\n    // Log detailed error information\n    logMessage(\"ERROR\", `Error in record_milestone_context tool`, {\n      error: error.message,\n      stack: error.stack,\n      input: {\n        name: input.name,\n        category: input.milestoneCategory,\n        conversationId: input.conversationId,\n      },\n    });\n\n    // Return error response\n    const errorResponse = {\n      error: true,\n      errorCode: error.code || \"MILESTONE_RECORDING_FAILED\",\n      errorDetails: error.message,\n      milestoneId: null,\n      status: \"error\",\n      milestoneCategory: input.milestoneCategory || \"uncategorized\",\n      relatedEntitiesCount: 0,\n      impactAssessment: {\n        error: error.message,\n      },\n    };\n\n    return {\n      content: [\n        {\n          type: \"text\",\n          text: JSON.stringify(errorResponse),\n        },\n      ],\n    };\n  }\n}\n\n/**\n * Assesses the impact of a milestone by analyzing relationships and dependencies\n *\n * @param {string} milestoneId - ID of the milestone\n * @param {string} category - Category of the milestone\n * @param {string[]} activeContextIds - IDs of entities in the active context\n * @returns {Promise<Object>} Impact assessment results\n * @private\n */\nasync function _assessMilestoneImpact(milestoneId, category, activeContextIds) {\n  try {\n    logMessage(\"DEBUG\", `Assessing impact for milestone: ${milestoneId}`, {\n      category,\n      entityCount: activeContextIds?.length || 0,\n    });\n\n    // Skip if no active context IDs\n    if (!activeContextIds || activeContextIds.length === 0) {\n      logMessage(\n        \"DEBUG\",\n        `No active context entities, skipping detailed impact assessment`\n      );\n      return {\n        impactScore: 0,\n        impactLevel: \"none\",\n        impactSummary: \"No code entities were modified in this milestone.\",\n        scopeMetrics: {\n          directlyModifiedEntities: 0,\n          potentiallyImpactedEntities: 0,\n          impactedComponents: 0,\n          criticalPathsCount: 0,\n        },\n      };\n    }\n\n    // 1. Fetch full details of active context entities\n    let entities = [];\n    try {\n      logMessage(\n        \"DEBUG\",\n        `Fetching details for ${activeContextIds.length} entities`\n      );\n\n      const entityDetails = await Promise.all(\n        activeContextIds.map(async (id) => {\n          try {\n            // Fetch entity details from database\n            const query = `SELECT * FROM code_entities WHERE entity_id = ?`;\n            const result = await executeQuery(query, [id]);\n            return result.length > 0 ? result[0] : null;\n          } catch (queryErr) {\n            logMessage(\"WARN\", `Failed to fetch entity details`, {\n              error: queryErr.message,\n              entityId: id,\n            });\n            return null;\n          }\n        })\n      );\n\n      entities = entityDetails.filter(Boolean);\n      logMessage(\n        \"DEBUG\",\n        `Retrieved details for ${entities.length}/${activeContextIds.length} entities`\n      );\n    } catch (fetchErr) {\n      logMessage(\"ERROR\", `Failed to fetch entity details`, {\n        error: fetchErr.message,\n      });\n      // Return a minimal assessment\n      return {\n        impactScore: 0.1,\n        impactLevel: \"unknown\",\n        impactSummary: `Impact could not be fully assessed due to database error: ${fetchErr.message}`,\n        scopeMetrics: {\n          directlyModifiedEntities: activeContextIds.length,\n          potentiallyImpactedEntities: 0,\n          impactedComponents: 0,\n          criticalPathsCount: 0,\n        },\n        error: fetchErr.message,\n      };\n    }\n\n    // 2. Analyze relationships to find potentially impacted entities\n    const impactedEntityIds = new Set(activeContextIds);\n    const criticalPaths = [];\n    const componentImpacts = new Map(); // Map to track impacts by component/directory\n\n    // Build a map of entity types for quick reference\n    const entityTypeMap = new Map();\n    entities.forEach((entity) => {\n      entityTypeMap.set(entity.entity_id, entity.entity_type);\n    });\n\n    // 3. For each entity, find its relationships\n    try {\n      for (const entity of entities) {\n        // Get outgoing relationships (dependencies on other entities)\n        const outgoingRelationships =\n          await RelationshipContextManagerLogic.getRelationships(\n            entity.entity_id,\n            \"outgoing\"\n          );\n\n        logMessage(\n          \"DEBUG\",\n          `Retrieved ${outgoingRelationships.length} outgoing relationships for entity`,\n          {\n            entityId: entity.entity_id,\n            entityType: entity.entity_type,\n          }\n        );\n\n        // For each relationship, add the target to potentially impacted entities\n        for (const rel of outgoingRelationships) {\n          // Only add if it's not already in the active context\n          if (!impactedEntityIds.has(rel.target_entity_id)) {\n            impactedEntityIds.add(rel.target_entity_id);\n\n            // Check if this forms a critical path\n            if (\n              rel.relationship_type === \"calls\" ||\n              rel.relationship_type === \"extends\" ||\n              rel.relationship_type === \"implements\"\n            ) {\n              criticalPaths.push({\n                source: entity.entity_id,\n                target: rel.target_entity_id,\n                type: rel.relationship_type,\n                criticality: 0.8, // Default high criticality for these types\n              });\n            }\n          }\n        }\n\n        // Track component impacts\n        // Extract component/directory from file path\n        const filePath = entity.file_path || \"\";\n        const component = filePath.split(\"/\").slice(0, 2).join(\"/\");\n        if (component) {\n          const currentCount = componentImpacts.get(component) || 0;\n          componentImpacts.set(component, currentCount + 1);\n        }\n      }\n    } catch (relErr) {\n      logMessage(\"WARN\", `Error analyzing relationships`, {\n        error: relErr.message,\n        milestoneId,\n      });\n      // Continue with partial data\n    }\n\n    logMessage(\"DEBUG\", `Completed relationship analysis`, {\n      impactedEntities: impactedEntityIds.size,\n      criticalPaths: criticalPaths.length,\n      componentCount: componentImpacts.size,\n    });\n\n    // 4. Calculate impact metrics\n    const directlyModifiedCount = activeContextIds.length;\n    const potentiallyImpactedCount =\n      impactedEntityIds.size - directlyModifiedCount;\n    const impactedComponentsCount = componentImpacts.size;\n    const criticalPathsCount = criticalPaths.length;\n\n    // 5. Calculate impact score based on metrics\n    let impactScore;\n    let impactLevel;\n\n    try {\n      // Calculate base impact score (0-1)\n      const baseImpactScore = Math.min(\n        1,\n        directlyModifiedCount * 0.02 +\n          potentiallyImpactedCount * 0.01 +\n          impactedComponentsCount * 0.1 +\n          criticalPathsCount * 0.05\n      );\n\n      // Adjust based on milestone category\n      let categoryMultiplier = 1;\n      switch (category) {\n        case \"major_feature\":\n          categoryMultiplier = 1.2;\n          break;\n        case \"refactoring\":\n          categoryMultiplier = 1.5; // Refactorings often have wide impact\n          break;\n        case \"bug_fix\":\n          categoryMultiplier = 0.7; // Bug fixes typically have more limited scope\n          break;\n        case \"critical_fix\":\n          categoryMultiplier = 1.3; // Critical fixes may touch core parts\n          break;\n        default:\n          categoryMultiplier = 1;\n      }\n\n      impactScore = Math.min(1, baseImpactScore * categoryMultiplier);\n\n      // Determine impact level\n      if (impactScore < 0.2) {\n        impactLevel = \"low\";\n      } else if (impactScore < 0.5) {\n        impactLevel = \"medium\";\n      } else if (impactScore < 0.8) {\n        impactLevel = \"high\";\n      } else {\n        impactLevel = \"critical\";\n      }\n\n      logMessage(\"INFO\", `Calculated impact assessment`, {\n        impactScore,\n        impactLevel,\n        directlyModified: directlyModifiedCount,\n        potentiallyImpacted: potentiallyImpactedCount,\n        components: impactedComponentsCount,\n      });\n    } catch (calcErr) {\n      logMessage(\"ERROR\", `Error calculating impact score`, {\n        error: calcErr.message,\n      });\n      // Provide default values\n      impactScore = 0.3;\n      impactLevel = \"medium\";\n    }\n\n    // 6. Generate impact summary text\n    let impactSummary;\n    try {\n      impactSummary = _generateImpactSummary(\n        impactLevel,\n        directlyModifiedCount,\n        potentiallyImpactedCount,\n        impactedComponentsCount,\n        criticalPathsCount,\n        category\n      );\n    } catch (summaryErr) {\n      logMessage(\"WARN\", `Error generating impact summary`, {\n        error: summaryErr.message,\n      });\n      // Provide default summary\n      impactSummary = `This milestone has a ${impactLevel} impact, affecting ${directlyModifiedCount} entities directly and potentially impacting ${potentiallyImpactedCount} others.`;\n    }\n\n    // 7. Return the complete assessment\n    return {\n      impactScore,\n      impactLevel,\n      impactSummary,\n      scopeMetrics: {\n        directlyModifiedEntities: directlyModifiedCount,\n        potentiallyImpactedEntities: potentiallyImpactedCount,\n        impactedComponents: impactedComponentsCount,\n        criticalPathsCount,\n      },\n      componentBreakdown: Object.fromEntries(componentImpacts),\n      criticalPathsTop: criticalPaths.slice(0, 5), // Only include top 5 critical paths\n    };\n  } catch (error) {\n    logMessage(\"ERROR\", `Error in impact assessment`, {\n      error: error.message,\n      stack: error.stack,\n      milestoneId,\n      category,\n    });\n\n    // Return a minimal assessment with error info\n    return {\n      impactScore: 0.1,\n      impactLevel: \"unknown\",\n      impactSummary: `Impact assessment encountered an error: ${error.message}`,\n      error: error.message,\n      scopeMetrics: {\n        directlyModifiedEntities: activeContextIds\n          ? activeContextIds.length\n          : 0,\n        potentiallyImpactedEntities: 0,\n        impactedComponents: 0,\n        criticalPathsCount: 0,\n      },\n    };\n  }\n}\n\n/**\n * Generates an impact summary text based on assessment metrics\n *\n * @param {string} impactLevel - Level of impact (low, medium, high, critical)\n * @param {number} directCount - Count of directly modified entities\n * @param {number} indirectCount - Count of indirectly impacted entities\n * @param {number} componentCount - Count of impacted components\n * @param {number} criticalPathCount - Count of critical paths\n * @param {string} category - Milestone category\n * @returns {string} Human-readable impact summary\n * @private\n */\nfunction _generateImpactSummary(\n  impactLevel,\n  directCount,\n  indirectCount,\n  componentCount,\n  criticalPathCount,\n  category\n) {\n  try {\n    // Start with impact level\n    let summary = `This ${category} milestone has a ${impactLevel} impact, `;\n\n    // Add direct and indirect counts\n    summary += `directly modifying ${directCount} entities and potentially affecting ${indirectCount} additional entities. `;\n\n    // Add component information\n    if (componentCount > 0) {\n      summary += `Changes span ${componentCount} component${\n        componentCount === 1 ? \"\" : \"s\"\n      }. `;\n    }\n\n    // Add critical path information if relevant\n    if (criticalPathCount > 0) {\n      summary += `Found ${criticalPathCount} critical dependency path${\n        criticalPathCount === 1 ? \"\" : \"s\"\n      } that may require careful testing. `;\n    }\n\n    // Add category-specific advice\n    switch (category) {\n      case \"refactoring\":\n        summary +=\n          \"Since this is a refactoring, consider comprehensive regression testing.\";\n        break;\n      case \"major_feature\":\n        summary +=\n          \"As a major feature, ensure adequate test coverage for new functionality.\";\n        break;\n      case \"bug_fix\":\n        summary +=\n          \"For this bug fix, focus testing on the specific issue resolution.\";\n        break;\n      case \"critical_fix\":\n        summary +=\n          \"This critical fix requires careful validation in production-like environments.\";\n        break;\n    }\n\n    return summary;\n  } catch (error) {\n    logMessage(\"WARN\", `Error generating impact summary text`, {\n      error: error.message,\n    });\n    // Return a simple fallback summary\n    return `This milestone has a ${impactLevel} impact, affecting ${directCount} entities directly.`;\n  }\n}\n\n// Export the tool definition for server registration\nexport default {\n  name: \"record_milestone_context\",\n  description:\n    \"Records a development milestone and its context, creating a snapshot for reference and learning\",\n  inputSchema: recordMilestoneContextInputSchema,\n  outputSchema: recordMilestoneContextOutputSchema,\n  handler,\n};\n", "/**\n * LearningSystem.js\n *\n * Background analysis of the codebase to extract and store reusable patterns.\n */\n\nimport { executeQuery } from \"../db.js\";\nimport * as SemanticPatternRecognizerLogic from \"./SemanticPatternRecognizerLogic.js\";\nimport * as CodeStructureAnalyzerLogic from \"./CodeStructureAnalyzerLogic.js\";\nimport * as ConversationIntelligence from \"./ConversationIntelligence.js\";\nimport * as GlobalPatternRepository from \"./GlobalPatternRepository.js\";\nimport * as TextTokenizerLogic from \"./TextTokenizerLogic.js\";\nimport { v4 as uuidv4 } from \"uuid\";\n\n/**\n * Performs background analysis of the entire project's codebase for patterns.\n *\n * @returns {Promise<void>}\n */\nexport async function extractPatternsFromCode() {\n  try {\n    console.log(\n      \"[LearningSystem] Starting pattern extraction from codebase...\"\n    );\n    // 1. Fetch all code entities of interest (functions, classes, methods)\n    const query = `\n      SELECT * FROM code_entities\n      WHERE type IN ('function', 'class', 'method')\n      LIMIT 1000\n    `;\n    const entities = await executeQuery(query, []);\n    if (!entities || entities.length === 0) {\n      console.log(\n        \"[LearningSystem] No code entities found for pattern extraction.\"\n      );\n      return;\n    }\n    console.log(\n      `[LearningSystem] Analyzing ${entities.length} code entities...`\n    );\n\n    // 2. Analyze each entity for patterns\n    for (const entity of entities) {\n      try {\n        // Recognize patterns in the entity\n        const patterns = await SemanticPatternRecognizerLogic.recognizePatterns(\n          entity\n        );\n        if (patterns && patterns.length > 0) {\n          for (const pattern of patterns) {\n            if (pattern.confidence && pattern.confidence >= 0.7) {\n              // Store the pattern if not already stored\n              await SemanticPatternRecognizerLogic.addPatternToRepository(\n                pattern\n              );\n              console.log(\n                `[LearningSystem] Pattern stored: ${pattern.name || pattern.id}`\n              );\n            }\n          }\n        }\n      } catch (entityErr) {\n        console.warn(\n          `[LearningSystem] Error analyzing entity ${entity.id}:`,\n          entityErr\n        );\n      }\n    }\n\n    // 3. Optionally, find groups of structurally similar entities\n    try {\n      const groups =\n        await CodeStructureAnalyzerLogic.findStructurallySimilarEntities(\n          entities\n        );\n      for (const group of groups) {\n        try {\n          const groupPattern =\n            await SemanticPatternRecognizerLogic.generatePatternFromExamples(\n              group\n            );\n          if (\n            groupPattern &&\n            groupPattern.confidence &&\n            groupPattern.confidence >= 0.7\n          ) {\n            await SemanticPatternRecognizerLogic.addPatternToRepository(\n              groupPattern\n            );\n            console.log(\n              `[LearningSystem] Group pattern stored: ${\n                groupPattern.name || groupPattern.id\n              }`\n            );\n          }\n        } catch (groupErr) {\n          console.warn(\n            \"[LearningSystem] Error generating pattern from group:\",\n            groupErr\n          );\n        }\n      }\n    } catch (groupingErr) {\n      console.warn(\n        \"[LearningSystem] Error finding structurally similar entity groups:\",\n        groupingErr\n      );\n    }\n\n    console.log(\"[LearningSystem] Pattern extraction complete.\");\n  } catch (error) {\n    console.error(\n      \"[LearningSystem] Fatal error during pattern extraction:\",\n      error\n    );\n  }\n}\n\n/**\n * Analyzes timeline_events and conversation_history to find patterns of tool usage, feature interaction, or problem-solving sequences.\n *\n * @returns {Promise<void>}\n */\nexport async function identifyUsagePatterns() {\n  try {\n    console.log(\"[LearningSystem] Starting usage pattern identification...\");\n    // 1. Fetch timeline events (limit for performance)\n    const timelineQuery = `\n      SELECT conversation_id, type, timestamp\n      FROM timeline_events\n      WHERE type IN ('search_query', 'file_edit', 'milestone_created', 'new_message', 'code_change', 'focus_change')\n      ORDER BY conversation_id, timestamp ASC\n      LIMIT 5000\n    `;\n    const events = await executeQuery(timelineQuery, []);\n    if (!events || events.length === 0) {\n      console.log(\n        \"[LearningSystem] No timeline events found for usage pattern analysis.\"\n      );\n      return;\n    }\n\n    // 2. Group events by conversation\n    const eventsByConversation = {};\n    for (const event of events) {\n      if (!eventsByConversation[event.conversation_id]) {\n        eventsByConversation[event.conversation_id] = [];\n      }\n      eventsByConversation[event.conversation_id].push(event);\n    }\n\n    // 3. Analyze event type transitions (simple Markov chain/frequency count)\n    const transitionCounts = {};\n    for (const convId in eventsByConversation) {\n      const convEvents = eventsByConversation[convId];\n      for (let i = 0; i < convEvents.length - 1; i++) {\n        const from = convEvents[i].type;\n        const to = convEvents[i + 1].type;\n        const key = `${from}=>${to}`;\n        transitionCounts[key] = (transitionCounts[key] || 0) + 1;\n      }\n    }\n\n    // 4. Find most common transitions (usage patterns)\n    const sortedTransitions = Object.entries(transitionCounts)\n      .sort((a, b) => b[1] - a[1])\n      .slice(0, 10); // Top 10 patterns\n\n    // 5. Store discovered patterns in project_patterns\n    for (const [transition, count] of sortedTransitions) {\n      const [from, to] = transition.split(\"=>\");\n      const pattern = {\n        pattern_type: \"usage_workflow\",\n        name: `Common transition: ${from} \u2192 ${to}`,\n        description: `Frequently observed transition from ${from} to ${to} in user workflow.`,\n        representation: JSON.stringify({ sequence: [from, to], count }),\n        is_global: true,\n        utility_score: count,\n        confidence_score: Math.min(1, count / 10),\n        created_at: new Date().toISOString(),\n        last_used: null,\n        use_count: 0,\n      };\n      // Insert into project_patterns (ignore duplicates for now)\n      try {\n        await executeQuery(\n          `INSERT INTO project_patterns (pattern_type, name, description, representation, is_global, utility_score, confidence_score, created_at, last_used, use_count)\n           VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n          `,\n          [\n            pattern.pattern_type,\n            pattern.name,\n            pattern.description,\n            pattern.representation,\n            pattern.is_global ? 1 : 0,\n            pattern.utility_score,\n            pattern.confidence_score,\n            pattern.created_at,\n            pattern.last_used,\n            pattern.use_count,\n          ]\n        );\n        console.log(`[LearningSystem] Usage pattern stored: ${pattern.name}`);\n      } catch (insertErr) {\n        // Ignore duplicate errors, log others\n        if (!/UNIQUE|duplicate/i.test(insertErr.message)) {\n          console.warn(\n            \"[LearningSystem] Error storing usage pattern:\",\n            insertErr\n          );\n        }\n      }\n    }\n\n    console.log(\"[LearningSystem] Usage pattern identification complete.\");\n  } catch (error) {\n    console.error(\n      \"[LearningSystem] Fatal error during usage pattern identification:\",\n      error\n    );\n  }\n}\n\n/**\n * Analyzes context and activity leading up to and following a recorded milestone.\n *\n * @param {string} milestoneSnapshotId - The ID of the milestone snapshot\n * @returns {Promise<void>}\n */\nexport async function analyzePatternsAroundMilestone(milestoneSnapshotId) {\n  try {\n    console.log(\n      `[LearningSystem] Analyzing patterns around milestone: ${milestoneSnapshotId}`\n    );\n    // 1. Retrieve the context_snapshots record\n    const snapshotQuery = `SELECT * FROM context_states WHERE milestone_id = ?`;\n    const snapshots = await executeQuery(snapshotQuery, [milestoneSnapshotId]);\n    if (!snapshots || snapshots.length === 0) {\n      console.warn(\n        `[LearningSystem] No context snapshot found for milestone ${milestoneSnapshotId}`\n      );\n      return;\n    }\n    const snapshot = snapshots[0];\n    const { created_at, focus_areas, conversation_id } = snapshot;\n    const milestoneTime = new Date(created_at).getTime();\n    const windowBeforeMs = 2 * 60 * 60 * 1000; // 2 hours before\n    const windowAfterMs = 1 * 60 * 60 * 1000; // 1 hour after\n    const windowStart = new Date(milestoneTime - windowBeforeMs).toISOString();\n    const windowEnd = new Date(milestoneTime + windowAfterMs).toISOString();\n\n    // 2. Fetch timeline events in the window\n    const eventsQuery = `\n      SELECT * FROM timeline_events\n      WHERE conversation_id = ?\n        AND timestamp >= ?\n        AND timestamp <= ?\n      ORDER BY timestamp ASC\n    `;\n    const events = await executeQuery(eventsQuery, [\n      conversation_id,\n      windowStart,\n      windowEnd,\n    ]);\n\n    // 3. Fetch conversation history in the window\n    const historyQuery = `\n      SELECT * FROM conversation_history\n      WHERE conversation_id = ?\n        AND timestamp >= ?\n        AND timestamp <= ?\n      ORDER BY timestamp ASC\n    `;\n    const messages = await executeQuery(historyQuery, [\n      conversation_id,\n      windowStart,\n      windowEnd,\n    ]);\n\n    // 4. Analyze for patterns\n    // a) Common code entities accessed\n    const entityAccessCounts = {};\n    for (const event of events) {\n      if (event.data) {\n        try {\n          const data =\n            typeof event.data === \"string\"\n              ? JSON.parse(event.data)\n              : event.data;\n          if (data.activeFile) {\n            entityAccessCounts[data.activeFile] =\n              (entityAccessCounts[data.activeFile] || 0) + 1;\n          }\n          if (data.relatedFiles && Array.isArray(data.relatedFiles)) {\n            for (const file of data.relatedFiles) {\n              entityAccessCounts[file] = (entityAccessCounts[file] || 0) + 1;\n            }\n          }\n        } catch (err) {\n          // Ignore parse errors\n        }\n      }\n    }\n    // b) Common search queries\n    const searchQueries = events\n      .filter((e) => e.type === \"search_query\")\n      .map((e) => {\n        try {\n          const data = typeof e.data === \"string\" ? JSON.parse(e.data) : e.data;\n          return data && data.query ? data.query : null;\n        } catch {\n          return null;\n        }\n      })\n      .filter(Boolean);\n    // c) Conversation topics/purposes\n    // For simplicity, just count topic_segment_id and purpose_type in messages\n    const topicCounts = {};\n    const purposeCounts = {};\n    for (const msg of messages) {\n      if (msg.topic_segment_id) {\n        topicCounts[msg.topic_segment_id] =\n          (topicCounts[msg.topic_segment_id] || 0) + 1;\n      }\n      if (msg.purpose_type) {\n        purposeCounts[msg.purpose_type] =\n          (purposeCounts[msg.purpose_type] || 0) + 1;\n      }\n    }\n\n    // 5. Log insights\n    console.log(\n      `[LearningSystem] Milestone ${milestoneSnapshotId} context analysis:`\n    );\n    console.log(\n      \"  Most accessed code entities:\",\n      Object.entries(entityAccessCounts)\n        .sort((a, b) => b[1] - a[1])\n        .slice(0, 5)\n    );\n    console.log(\"  Most common search queries:\", searchQueries.slice(0, 5));\n    console.log(\n      \"  Most discussed topics:\",\n      Object.entries(topicCounts)\n        .sort((a, b) => b[1] - a[1])\n        .slice(0, 3)\n    );\n    console.log(\n      \"  Most discussed purposes:\",\n      Object.entries(purposeCounts)\n        .sort((a, b) => b[1] - a[1])\n        .slice(0, 3)\n    );\n\n    // 6. Optionally reinforce patterns in project_patterns (not implemented in detail here)\n    // This could update utility_score/confidence_score for patterns related to these entities/topics\n    // ...\n\n    console.log(\n      `[LearningSystem] Analysis around milestone ${milestoneSnapshotId} complete.`\n    );\n  } catch (error) {\n    console.error(\n      `[LearningSystem] Error analyzing patterns around milestone ${milestoneSnapshotId}:`,\n      error\n    );\n  }\n}\n\n/**\n * Analyzes a conversation for patterns when the conversation concludes.\n *\n * @param {string} conversationId - The ID of the conversation to analyze\n * @param {string} outcome - The outcome of the conversation (e.g., 'successful_debug', 'feature_planned')\n * @returns {Promise<void>}\n */\nexport async function analyzeConversationForPatterns(conversationId, outcome) {\n  try {\n    console.log(\n      `[LearningSystem] Analyzing conversation ${conversationId} with outcome: ${outcome}`\n    );\n\n    // 1. Fetch the full conversation history\n    const conversationHistory =\n      await ConversationIntelligence.getConversationHistory(conversationId);\n    if (!conversationHistory || conversationHistory.length === 0) {\n      console.log(\n        `[LearningSystem] No conversation history found for ${conversationId}`\n      );\n      return;\n    }\n\n    console.log(\n      `[LearningSystem] Retrieved ${conversationHistory.length} messages for analysis`\n    );\n\n    // 2. Extract code entities mentioned in the conversation\n    const codeEntityIds = new Set();\n\n    for (const message of conversationHistory) {\n      if (\n        message.related_context_entity_ids &&\n        Array.isArray(message.related_context_entity_ids)\n      ) {\n        message.related_context_entity_ids.forEach((id) =>\n          codeEntityIds.add(id)\n        );\n      }\n    }\n\n    if (codeEntityIds.size === 0) {\n      console.log(\n        `[LearningSystem] No code entities found in conversation ${conversationId}`\n      );\n      return;\n    }\n\n    console.log(\n      `[LearningSystem] Found ${codeEntityIds.size} unique code entities to analyze for patterns`\n    );\n\n    // 3. Fetch full details of each code entity\n    const codeEntities = [];\n    for (const entityId of codeEntityIds) {\n      const entityQuery = `SELECT * FROM code_entities WHERE id = ?`;\n      const [entity] = await executeQuery(entityQuery, [entityId]);\n\n      if (entity) {\n        codeEntities.push(entity);\n      }\n    }\n\n    console.log(\n      `[LearningSystem] Retrieved ${codeEntities.length} code entities for pattern recognition`\n    );\n\n    // 4. For each code entity, recognize patterns\n    for (const entity of codeEntities) {\n      try {\n        // Recognize patterns in the entity\n        const { patterns, confidence } =\n          await SemanticPatternRecognizerLogic.recognizePatterns(entity);\n\n        if (patterns && patterns.length > 0) {\n          console.log(\n            `[LearningSystem] Found ${patterns.length} patterns in entity ${entity.id}`\n          );\n\n          // For each pattern, consider adding to repository if confidence is high\n          for (const pattern of patterns) {\n            if (pattern.confidence && pattern.confidence >= 0.7) {\n              // Add pattern to repository if it's new or interesting\n              await SemanticPatternRecognizerLogic.addPatternToRepository(\n                pattern\n              );\n              console.log(\n                `[LearningSystem] Added pattern ${\n                  pattern.id || pattern.name\n                } to repository`\n              );\n            }\n          }\n        }\n      } catch (error) {\n        console.warn(\n          `[LearningSystem] Error analyzing entity ${entity.id}:`,\n          error\n        );\n      }\n    }\n\n    // 5. If the outcome was successful, reinforce patterns that were likely used\n    if (\n      outcome &&\n      (outcome.includes(\"success\") ||\n        outcome === \"feature_planned\" ||\n        outcome === \"bug_fixed\")\n    ) {\n      // Get all patterns that were recognized during this conversation\n      const patternQuery = `\n        SELECT DISTINCT p.pattern_id \n        FROM project_patterns p\n        JOIN pattern_observations po ON p.pattern_id = po.pattern_id\n        WHERE po.conversation_id = ?\n      `;\n\n      const patternIds = await executeQuery(patternQuery, [conversationId]);\n\n      if (patternIds && patternIds.length > 0) {\n        console.log(\n          `[LearningSystem] Reinforcing ${patternIds.length} patterns based on successful outcome`\n        );\n\n        for (const { pattern_id } of patternIds) {\n          await GlobalPatternRepository.reinforcePattern(\n            pattern_id,\n            \"confirmation\",\n            { conversationId }\n          );\n          console.log(\n            `[LearningSystem] Reinforced pattern ${pattern_id} based on successful outcome`\n          );\n        }\n      }\n    }\n\n    console.log(\n      `[LearningSystem] Completed pattern analysis for conversation ${conversationId}`\n    );\n  } catch (error) {\n    console.error(\n      `[LearningSystem] Error analyzing conversation patterns:`,\n      error\n    );\n  }\n}\n\n/**\n * Promotes session-specific patterns to global patterns for reuse across multiple sessions.\n *\n * @param {string} sessionId - The ID of the session/conversation\n * @param {Object} [filterOptions] - Optional filtering options\n * @param {number} [filterOptions.minConfidence] - Minimum confidence score for patterns to be promoted\n * @returns {Promise<number>} The number of successfully promoted patterns\n */\nexport async function promoteSessionPatternsToGlobal(\n  sessionId,\n  filterOptions = {}\n) {\n  try {\n    console.log(\n      `[LearningSystem] Promoting session patterns to global for session ${sessionId}`\n    );\n\n    // Build the query with filtering options\n    let query = `\n      SELECT * FROM project_patterns \n      WHERE is_global = FALSE \n      AND session_origin_id = ?\n    `;\n\n    const params = [sessionId];\n\n    // Apply minimum confidence filter if provided\n    if (filterOptions.minConfidence !== undefined) {\n      query += ` AND confidence_score >= ?`;\n      params.push(filterOptions.minConfidence);\n    }\n\n    // Execute the query to get qualifying patterns\n    const patterns = await executeQuery(query, params);\n\n    if (!patterns || patterns.length === 0) {\n      console.log(\n        `[LearningSystem] No qualifying session patterns found for promotion in session ${sessionId}`\n      );\n      return 0;\n    }\n\n    console.log(\n      `[LearningSystem] Found ${patterns.length} session patterns qualifying for promotion to global`\n    );\n\n    // Counter for successfully promoted patterns\n    let promotedCount = 0;\n\n    // Promote each qualifying pattern\n    for (const pattern of patterns) {\n      try {\n        await GlobalPatternRepository.promotePatternToGlobal(\n          pattern.pattern_id,\n          pattern.confidence_score\n        );\n\n        promotedCount++;\n        console.log(\n          `[LearningSystem] Successfully promoted pattern ${pattern.pattern_id} to global`\n        );\n      } catch (error) {\n        console.warn(\n          `[LearningSystem] Error promoting pattern ${pattern.pattern_id} to global:`,\n          error\n        );\n        // Continue with the next pattern even if one fails\n      }\n    }\n\n    console.log(\n      `[LearningSystem] Completed pattern promotion. ${promotedCount}/${patterns.length} patterns promoted successfully`\n    );\n    return promotedCount;\n  } catch (error) {\n    console.error(\n      `[LearningSystem] Error promoting session patterns to global:`,\n      error\n    );\n    return 0;\n  }\n}\n\n/**\n * Enriches context with applicable global patterns\n *\n * @param {Array} context - An array of CodeEntity or ContextSnippet objects\n * @param {Object} [filterOptions] - Optional filtering options for patterns\n * @param {number} [filterOptions.minConfidence] - Minimum confidence score for patterns to consider\n * @returns {Promise<Array>} The enriched context with matched global patterns\n */\nexport async function applyGlobalPatternsToContext(\n  context,\n  filterOptions = {}\n) {\n  try {\n    console.log(`[LearningSystem] Enriching context with global patterns`);\n\n    if (!context || !Array.isArray(context) || context.length === 0) {\n      console.log(\n        `[LearningSystem] No context items provided for pattern enrichment`\n      );\n      return context;\n    }\n\n    // 1. Retrieve relevant global patterns using GlobalPatternRepository\n    const globalPatterns = await GlobalPatternRepository.retrieveGlobalPatterns(\n      filterOptions\n    );\n\n    if (!globalPatterns || globalPatterns.length === 0) {\n      console.log(\n        `[LearningSystem] No global patterns found with the specified criteria`\n      );\n      return context;\n    }\n\n    console.log(\n      `[LearningSystem] Retrieved ${globalPatterns.length} global patterns for matching`\n    );\n\n    // 2. For each item in the context, try to match global patterns\n    const enrichedContext = await Promise.all(\n      context.map(async (item) => {\n        // Create a copy of the item to avoid mutating the original\n        const enrichedItem = { ...item, matched_global_patterns: [] };\n\n        // Extract necessary content from item (could be a CodeEntity or ContextSnippet)\n        const content =\n          item.content || item.raw_content || item.summarizedContent;\n\n        if (!content) {\n          return enrichedItem; // Skip if no content available\n        }\n\n        // Get the item type (for type-specific pattern matching)\n        const itemType = item.type || \"unknown\";\n\n        // Extract textual features for pattern matching\n        const tokenizedContent = TextTokenizerLogic.tokenize(content);\n        const keywords = TextTokenizerLogic.extractKeywords(tokenizedContent);\n\n        // Try to extract or use provided structural features\n        let structuralFeatures = item.custom_metadata?.structuralFeatures || {};\n\n        // For each global pattern, attempt to match against this item\n        const matchPromises = globalPatterns.map(async (pattern) => {\n          try {\n            // Similar to matchPattern in SemanticPatternRecognizerLogic but simplified\n            const { detection_rules } = pattern;\n            let textualMatchScore = 0;\n            let structuralMatchScore = 0;\n            let typeMatchScore = 0;\n\n            // Check if pattern applies to this item type\n            if (\n              detection_rules.applicable_types &&\n              Array.isArray(detection_rules.applicable_types)\n            ) {\n              typeMatchScore = detection_rules.applicable_types.includes(\n                itemType\n              )\n                ? 1\n                : 0;\n\n              // If pattern explicitly doesn't apply to this type and strict matching is enabled, skip\n              if (\n                typeMatchScore === 0 &&\n                detection_rules.strict_type_matching\n              ) {\n                return { pattern, confidence: 0 };\n              }\n            } else {\n              // If no type restrictions, full score\n              typeMatchScore = 1;\n            }\n\n            // Perform textual matching with keywords\n            if (\n              detection_rules.keywords &&\n              Array.isArray(detection_rules.keywords)\n            ) {\n              const keywordMatches = detection_rules.keywords.filter(\n                (keyword) =>\n                  keywords.some((k) =>\n                    typeof k === \"string\"\n                      ? k === keyword\n                      : k.keyword === keyword\n                  )\n              );\n\n              textualMatchScore =\n                keywordMatches.length / detection_rules.keywords.length;\n            }\n\n            // Check for text patterns\n            if (\n              detection_rules.text_patterns &&\n              Array.isArray(detection_rules.text_patterns)\n            ) {\n              let patternMatchCount = 0;\n\n              for (const textPattern of detection_rules.text_patterns) {\n                if (typeof textPattern === \"string\") {\n                  if (content.includes(textPattern)) {\n                    patternMatchCount++;\n                  }\n                } else if (\n                  textPattern instanceof RegExp ||\n                  (typeof textPattern === \"object\" && textPattern.pattern)\n                ) {\n                  // Handle regex pattern objects\n                  const patternObj =\n                    textPattern instanceof RegExp\n                      ? textPattern\n                      : new RegExp(\n                          textPattern.pattern,\n                          textPattern.flags || \"\"\n                        );\n\n                  if (patternObj.test(content)) {\n                    patternMatchCount++;\n                  }\n                }\n              }\n\n              const textPatternScore =\n                detection_rules.text_patterns.length > 0\n                  ? patternMatchCount / detection_rules.text_patterns.length\n                  : 0;\n\n              // Combine with keyword score\n              textualMatchScore =\n                textualMatchScore > 0\n                  ? (textualMatchScore + textPatternScore) / 2\n                  : textPatternScore;\n            }\n\n            // Perform structural matching if we have structural rules and features\n            if (\n              detection_rules.structural_rules &&\n              Array.isArray(detection_rules.structural_rules) &&\n              Object.keys(structuralFeatures).length > 0\n            ) {\n              let structRuleMatchCount = 0;\n\n              for (const rule of detection_rules.structural_rules) {\n                const { feature, condition, value } = rule;\n\n                // Skip invalid rules\n                if (!feature || !condition || value === undefined) continue;\n\n                // Get the actual feature value\n                const featureValue = structuralFeatures[feature];\n\n                // Skip if feature doesn't exist\n                if (featureValue === undefined) continue;\n\n                // Evaluate condition\n                let matches = false;\n\n                switch (condition) {\n                  case \"equals\":\n                    matches = featureValue === value;\n                    break;\n                  case \"contains\":\n                    matches = Array.isArray(featureValue)\n                      ? featureValue.includes(value)\n                      : String(featureValue).includes(String(value));\n                    break;\n                  case \"greater_than\":\n                    matches = Number(featureValue) > Number(value);\n                    break;\n                  case \"less_than\":\n                    matches = Number(featureValue) < Number(value);\n                    break;\n                  case \"matches_regex\":\n                    matches = new RegExp(value).test(String(featureValue));\n                    break;\n                  default:\n                    matches = false;\n                }\n\n                if (matches) {\n                  structRuleMatchCount++;\n                }\n              }\n\n              structuralMatchScore =\n                detection_rules.structural_rules.length > 0\n                  ? structRuleMatchCount /\n                    detection_rules.structural_rules.length\n                  : 0;\n            }\n\n            // Calculate combined confidence\n            const weights = detection_rules.weights || {\n              textual: 0.6, // Give more weight to textual matching for context items\n              structural: 0.3,\n              type: 0.1,\n            };\n\n            // Calculate weighted average\n            const confidence =\n              textualMatchScore * weights.textual +\n              structuralMatchScore * weights.structural +\n              typeMatchScore * weights.type;\n\n            return {\n              pattern,\n              confidence,\n              textualMatchScore,\n              structuralMatchScore,\n              typeMatchScore,\n            };\n          } catch (error) {\n            console.warn(\n              `[LearningSystem] Error matching pattern ${pattern.pattern_id}:`,\n              error\n            );\n            return { pattern, confidence: 0 };\n          }\n        });\n\n        // Wait for all pattern matching to complete\n        const matchResults = await Promise.all(matchPromises);\n\n        // Filter for patterns with significant match confidence and sort by confidence\n        const significantMatches = matchResults\n          .filter((result) => result.confidence > 0.3) // Only include significant matches\n          .sort((a, b) => b.confidence - a.confidence);\n\n        // Add matched patterns to the enriched item\n        if (significantMatches.length > 0) {\n          enrichedItem.matched_global_patterns = significantMatches.map(\n            (match) => ({\n              pattern_id: match.pattern.pattern_id,\n              pattern_type: match.pattern.pattern_type,\n              name: match.pattern.name,\n              description: match.pattern.description,\n              confidence: match.confidence,\n              match_details: {\n                textual_score: match.textualMatchScore,\n                structural_score: match.structuralMatchScore,\n                type_score: match.typeMatchScore,\n              },\n            })\n          );\n\n          console.log(\n            `[LearningSystem] Found ${\n              enrichedItem.matched_global_patterns.length\n            } matching patterns for item ${\n              item.id || item.entity_id || \"unknown\"\n            }`\n          );\n        }\n\n        return enrichedItem;\n      })\n    );\n\n    console.log(\n      `[LearningSystem] Completed context enrichment with global patterns`\n    );\n    return enrichedContext;\n  } catch (error) {\n    console.error(\n      `[LearningSystem] Error applying global patterns to context:`,\n      error\n    );\n    // In case of error, return the original context to avoid data loss\n    return context;\n  }\n}\n\n/**\n * Records pattern application success or failure for reinforcement learning\n *\n * @param {string} patternId - The ID of the pattern that was applied\n * @param {boolean} successful - Whether the pattern application was successful\n * @param {string} [conversationId] - Optional ID of the conversation where pattern was applied\n * @param {string[]} [contextEntities] - Optional array of entity IDs related to the pattern application\n * @returns {Promise<void>}\n */\nexport async function registerPatternObservation(\n  patternId,\n  successful,\n  conversationId,\n  contextEntities\n) {\n  try {\n    console.log(\n      `[LearningSystem] Registering pattern observation for ${patternId} (successful: ${successful})`\n    );\n\n    // Determine observation type based on success\n    const observationType = successful ? \"confirmation\" : \"rejection\";\n\n    // Prepare context data object with any provided context information\n    const contextData = {\n      conversationId,\n      entities: contextEntities,\n    };\n\n    // Call GlobalPatternRepository to reinforce the pattern\n    await GlobalPatternRepository.reinforcePattern(\n      patternId,\n      observationType,\n      contextData\n    );\n\n    console.log(\n      `[LearningSystem] Successfully registered ${observationType} observation for pattern ${patternId}`\n    );\n  } catch (error) {\n    console.error(\n      `[LearningSystem] Error registering pattern observation:`,\n      error\n    );\n  }\n}\n\n/**\n * Extracts patterns from a conversation by analyzing messages and related code entities\n *\n * @param {string} conversationId - The ID of the conversation to analyze\n * @returns {Promise<Array>} Array of patterns found in the conversation\n */\nexport async function extractPatternsFromConversation(conversationId) {\n  try {\n    console.log(\n      `[LearningSystem] Extracting patterns from conversation ${conversationId}`\n    );\n\n    // 1. Fetch the complete conversation history\n    const conversationHistory =\n      await ConversationIntelligence.getConversationHistory(conversationId);\n    if (!conversationHistory || conversationHistory.length === 0) {\n      console.log(\n        `[LearningSystem] No conversation history found for ${conversationId}`\n      );\n      return [];\n    }\n\n    // 2. Extract unique code entity IDs mentioned in the conversation\n    const codeEntityIds = new Set();\n    for (const message of conversationHistory) {\n      if (\n        message.related_context_entity_ids &&\n        Array.isArray(message.related_context_entity_ids)\n      ) {\n        message.related_context_entity_ids.forEach((id) =>\n          codeEntityIds.add(id)\n        );\n      }\n    }\n\n    if (codeEntityIds.size === 0) {\n      console.log(\n        `[LearningSystem] No code entities found in conversation ${conversationId}`\n      );\n      return [];\n    }\n\n    // 3. Fetch details of each entity from the database\n    const codeEntities = [];\n    for (const entityId of codeEntityIds) {\n      const entityQuery = `SELECT * FROM code_entities WHERE id = ?`;\n      const entityResults = await executeQuery(entityQuery, [entityId]);\n\n      if (entityResults && entityResults.length > 0) {\n        codeEntities.push(entityResults[0]);\n      }\n    }\n\n    // 4. Use SemanticPatternRecognizerLogic to find patterns in each entity\n    const recognizedPatternIds = new Set();\n\n    for (const entity of codeEntities) {\n      try {\n        const { patterns } =\n          await SemanticPatternRecognizerLogic.recognizePatterns(entity);\n\n        if (patterns && patterns.length > 0) {\n          patterns.forEach((pattern) => {\n            if (pattern.pattern_id) {\n              recognizedPatternIds.add(pattern.pattern_id);\n            }\n          });\n        }\n      } catch (error) {\n        console.warn(\n          `[LearningSystem] Error recognizing patterns in entity ${entity.id}:`,\n          error\n        );\n      }\n    }\n\n    // 5. Fetch full pattern details from the project_patterns table\n    if (recognizedPatternIds.size === 0) {\n      console.log(\n        `[LearningSystem] No patterns recognized in conversation ${conversationId}`\n      );\n      return [];\n    }\n\n    // Convert Set to array for the IN clause\n    const patternIdArray = Array.from(recognizedPatternIds);\n\n    // Build placeholders for the IN clause\n    const placeholders = patternIdArray.map(() => \"?\").join(\",\");\n\n    const patternsQuery = `\n      SELECT * FROM project_patterns \n      WHERE pattern_id IN (${placeholders})\n      ORDER BY confidence_score DESC\n    `;\n\n    const patterns = await executeQuery(patternsQuery, patternIdArray);\n\n    console.log(\n      `[LearningSystem] Found ${patterns.length} patterns in conversation ${conversationId}`\n    );\n\n    // Parse detection_rules JSON field and return patterns\n    return patterns.map((pattern) => ({\n      ...pattern,\n      detection_rules: pattern.detection_rules\n        ? JSON.parse(pattern.detection_rules)\n        : {},\n      is_global: Boolean(pattern.is_global),\n    }));\n  } catch (error) {\n    console.error(\n      `[LearningSystem] Error extracting patterns from conversation:`,\n      error\n    );\n    return [];\n  }\n}\n\n/**\n * Extracts bug patterns from conversation messages by analyzing text for error messages and solutions\n *\n * @param {string} conversationId - The ID of the conversation to analyze\n * @returns {Promise<Array>} Array of bug patterns with descriptions and confidence scores\n */\nexport async function extractBugPatterns(conversationId) {\n  try {\n    console.log(\n      `[LearningSystem] Extracting bug patterns from conversation ${conversationId}`\n    );\n\n    // 1. Fetch all messages from the conversation\n    const messages = await ConversationIntelligence.getConversationHistory(\n      conversationId\n    );\n    if (!messages || messages.length === 0) {\n      return [];\n    }\n\n    // 2. Define regex patterns for identifying bug-related content\n    const errorPatterns = [\n      /error:?\\s+([^\\n.]+)/i,\n      /exception:?\\s+([^\\n.]+)/i,\n      /failed\\s+(?:to|with):?\\s+([^\\n.]+)/i,\n      /bug:?\\s+([^\\n.]+)/i,\n      /issue:?\\s+([^\\n.]+)/i,\n      /problem:?\\s+([^\\n.]+)/i,\n    ];\n\n    const fixPatterns = [\n      /fix(?:ed|ing)?:?\\s+([^\\n.]+)/i,\n      /solv(?:ed|ing)?:?\\s+([^\\n.]+)/i,\n      /resolv(?:ed|ing)?:?\\s+([^\\n.]+)/i,\n      /solutions?:?\\s+([^\\n.]+)/i,\n      /workaround:?\\s+([^\\n.]+)/i,\n      /(?:the\\s+)?(?:root\\s+)?cause\\s+(?:is|was):?\\s+([^\\n.]+)/i,\n    ];\n\n    // 3. Analyze each message for bug patterns\n    const bugDescriptions = [];\n    const bugSolutions = [];\n\n    for (const message of messages) {\n      const content = message.content;\n      if (!content) continue;\n\n      // Look for error/bug descriptions\n      for (const pattern of errorPatterns) {\n        const matches = content.match(pattern);\n        if (matches && matches[1]) {\n          bugDescriptions.push({\n            description: matches[1].trim(),\n            confidence: 0.7,\n            messageId: message.message_id,\n            type: \"error\",\n          });\n        }\n      }\n\n      // Look for fix/solution descriptions\n      for (const pattern of fixPatterns) {\n        const matches = content.match(pattern);\n        if (matches && matches[1]) {\n          bugSolutions.push({\n            description: matches[1].trim(),\n            confidence: 0.7,\n            messageId: message.message_id,\n            type: \"solution\",\n          });\n        }\n      }\n\n      // Look for code blocks with error messages or stacktraces\n      const codeBlockMatches = content.match(/```[\\s\\S]*?```/g);\n      if (codeBlockMatches) {\n        for (const codeBlock of codeBlockMatches) {\n          // If code block contains error-related keywords\n          if (/error|exception|traceback|fail|bug|issue/i.test(codeBlock)) {\n            bugDescriptions.push({\n              description:\n                codeBlock.replace(/```/g, \"\").trim().substring(0, 100) + \"...\",\n              confidence: 0.8,\n              messageId: message.message_id,\n              type: \"code_error\",\n            });\n          }\n        }\n      }\n    }\n\n    // 4. Match bug descriptions with their solutions when possible\n    const bugPatterns = [];\n\n    // For each bug description, try to find a related solution\n    for (const bug of bugDescriptions) {\n      // Create tokens from the bug description\n      const bugTokens = TextTokenizerLogic.tokenize(bug.description);\n      const bugKeywords = TextTokenizerLogic.extractKeywords(bugTokens);\n\n      // Find solutions with matching keywords\n      let bestSolution = null;\n      let bestScore = 0;\n\n      for (const solution of bugSolutions) {\n        const solutionTokens = TextTokenizerLogic.tokenize(\n          solution.description\n        );\n        const solutionKeywords =\n          TextTokenizerLogic.extractKeywords(solutionTokens);\n\n        // Calculate simple relevance score (keyword overlap)\n        let matchScore = 0;\n        for (const bugKeyword of bugKeywords) {\n          if (solutionKeywords.includes(bugKeyword)) {\n            matchScore++;\n          }\n        }\n\n        if (matchScore > bestScore) {\n          bestScore = matchScore;\n          bestSolution = solution;\n        }\n      }\n\n      // Add the bug pattern, with solution if found\n      bugPatterns.push({\n        description: bug.description,\n        confidence: bug.confidence,\n        solution: bestSolution ? bestSolution.description : undefined,\n        relatedIssues: [], // Would require additional lookup to find related issues\n      });\n    }\n\n    // Add any solutions without a matched bug as standalone patterns (with lower confidence)\n    for (const solution of bugSolutions) {\n      // Check if we've already used this solution\n      const alreadyUsed = bugPatterns.some(\n        (bp) => bp.solution === solution.description\n      );\n      if (!alreadyUsed) {\n        bugPatterns.push({\n          description: `Solution: ${solution.description}`,\n          confidence: 0.6,\n          relatedIssues: [],\n        });\n      }\n    }\n\n    // Remove duplicates by comparing descriptions\n    const seenDescriptions = new Set();\n    const uniquePatterns = [];\n\n    for (const pattern of bugPatterns) {\n      if (!seenDescriptions.has(pattern.description)) {\n        seenDescriptions.add(pattern.description);\n        uniquePatterns.push(pattern);\n      }\n    }\n\n    console.log(\n      `[LearningSystem] Extracted ${uniquePatterns.length} bug patterns from conversation ${conversationId}`\n    );\n    return uniquePatterns;\n  } catch (error) {\n    console.error(`[LearningSystem] Error extracting bug patterns:`, error);\n    return [];\n  }\n}\n\n/**\n * Extracts design decisions from conversation by identifying discussion of choices and tradeoffs\n *\n * @param {string} conversationId - The ID of the conversation to analyze\n * @returns {Promise<Array>} Array of design decisions with descriptions and confidence scores\n */\nexport async function extractDesignDecisions(conversationId) {\n  try {\n    console.log(\n      `[LearningSystem] Extracting design decisions from conversation ${conversationId}`\n    );\n\n    // 1. Fetch all messages from the conversation\n    const messages = await ConversationIntelligence.getConversationHistory(\n      conversationId\n    );\n    if (!messages || messages.length === 0) {\n      return [];\n    }\n\n    // 2. Define patterns for identifying design discussions\n    const designPatterns = [\n      /(?:I|we)\\s+(?:chose|decided|selected|opted|will use|should use)\\s+([^.]+)(?:\\s+because|for|to)\\s+([^.]+)/i,\n      /(?:the|a)\\s+(?:better|best|optimal|appropriate|right)\\s+(?:approach|design|solution|pattern|architecture)\\s+(?:is|would be)\\s+([^.]+)/i,\n      /(?:advantages|benefits|pros)\\s+of\\s+([^.]+)(?:\\s+(?:are|include))\\s+([^.]+)/i,\n      /(?:disadvantages|drawbacks|cons)\\s+of\\s+([^.]+)(?:\\s+(?:are|include))\\s+([^.]+)/i,\n      /(?:comparing|between)\\s+([^\\s.]+)\\s+and\\s+([^\\s.]+),\\s+([^.]+)/i,\n      /(?:I|we)\\s+recommend\\s+([^.]+)(?:\\s+because|for|to)\\s+([^.]+)/i,\n    ];\n\n    const alternativePatterns = [\n      /(?:alternative|another|other)\\s+(?:approach|option|solution)\\s+(?:would be|is|could be)\\s+([^.]+)/i,\n      /(?:instead of|rather than)\\s+([^,]+),\\s+(?:we could|we should|we might|you could|you should|you might)\\s+([^.]+)/i,\n      /(?:we|you)\\s+(?:could|should|might)\\s+(?:also|instead|alternatively)\\s+(?:consider|use|try)\\s+([^.]+)/i,\n    ];\n\n    const rationalePatterns = [\n      /(?:because|since|as|given that)\\s+([^,.]+)/i,\n      /(?:the|a)\\s+(?:reason|rationale|justification)\\s+(?:is|was|being)\\s+([^.]+)/i,\n      /(?:this|that)\\s+(?:approach|solution|design|pattern|choice|decision)\\s+(?:helps|allows|enables|provides|ensures)\\s+([^.]+)/i,\n    ];\n\n    // 3. Extract design decisions from messages\n    const designDecisions = [];\n\n    for (const message of messages) {\n      const content = message.content;\n      if (!content) continue;\n\n      // Split content into sentences for more precise pattern matching\n      const sentences = content.split(/[.!?]\\s+/);\n\n      for (let i = 0; i < sentences.length; i++) {\n        const sentence = sentences[i];\n\n        // Look for design decision patterns\n        for (const pattern of designPatterns) {\n          const matches = sentence.match(pattern);\n          if (matches && matches.length > 1) {\n            // Found a design decision\n            const description = matches[1].trim();\n\n            // Look for rationale in the same sentence or following sentences\n            let rationale = matches[2] ? matches[2].trim() : \"\";\n\n            // If no rationale found in the match, check the next sentence\n            if (!rationale && i < sentences.length - 1) {\n              const nextSentence = sentences[i + 1];\n              for (const rationalePattern of rationalePatterns) {\n                const rationaleMatch = nextSentence.match(rationalePattern);\n                if (rationaleMatch && rationaleMatch[1]) {\n                  rationale = rationaleMatch[1].trim();\n                  break;\n                }\n              }\n            }\n\n            // Look for alternatives in nearby sentences\n            let alternatives = [];\n\n            // Check a window of 3 sentences before and after\n            const windowStart = Math.max(0, i - 3);\n            const windowEnd = Math.min(sentences.length - 1, i + 3);\n\n            for (let j = windowStart; j <= windowEnd; j++) {\n              if (j === i) continue; // Skip the current sentence\n\n              const nearbySentence = sentences[j];\n              for (const altPattern of alternativePatterns) {\n                const altMatch = nearbySentence.match(altPattern);\n                if (altMatch && altMatch.length > 1) {\n                  // Found an alternative\n                  if (altMatch[2]) {\n                    alternatives.push(altMatch[2].trim());\n                  } else if (altMatch[1]) {\n                    alternatives.push(altMatch[1].trim());\n                  }\n                }\n              }\n            }\n\n            // Assign confidence based on completeness of the decision\n            let confidence = 0.7; // Base confidence\n            if (rationale) confidence += 0.1;\n            if (alternatives.length > 0) confidence += 0.1;\n\n            designDecisions.push({\n              description,\n              confidence,\n              rationale: rationale || undefined,\n              alternatives: alternatives.length > 0 ? alternatives : undefined,\n              messageId: message.message_id,\n            });\n          }\n        }\n      }\n    }\n\n    // Remove duplicates\n    const uniqueDecisions = [];\n    const seenDescriptions = new Set();\n\n    for (const decision of designDecisions) {\n      if (!seenDescriptions.has(decision.description)) {\n        seenDescriptions.add(decision.description);\n        uniqueDecisions.push(decision);\n      }\n    }\n\n    console.log(\n      `[LearningSystem] Extracted ${uniqueDecisions.length} design decisions from conversation ${conversationId}`\n    );\n    return uniqueDecisions;\n  } catch (error) {\n    console.error(`[LearningSystem] Error extracting design decisions:`, error);\n    return [];\n  }\n}\n\n/**\n * Extracts best practices mentioned in conversation\n *\n * @param {string} conversationId - The ID of the conversation to analyze\n * @returns {Promise<Array>} Array of best practices with descriptions and confidence scores\n */\nexport async function extractBestPractices(conversationId) {\n  try {\n    console.log(\n      `[LearningSystem] Extracting best practices from conversation ${conversationId}`\n    );\n\n    // 1. Fetch all messages from the conversation\n    const messages = await ConversationIntelligence.getConversationHistory(\n      conversationId\n    );\n    if (!messages || messages.length === 0) {\n      return [];\n    }\n\n    // 2. Define patterns for identifying best practices\n    const bestPracticePatterns = [\n      /(?:best|good)\\s+practice[s]?\\s+(?:is|are|for|to)\\s+([^.]+)/i,\n      /(?:recommended|suggested|advisable)\\s+(?:approach|practice|method|way)\\s+(?:is|would be)\\s+([^.]+)/i,\n      /(?:should|must|always|never)\\s+([^.]+)/i,\n      /(?:it['']s|its)\\s+(?:better|best|recommended)\\s+to\\s+([^.]+)/i,\n      /(?:convention|standard|norm|guideline|rule)\\s+(?:is|dictates|suggests|recommends|states)\\s+([^.]+)/i,\n      /(?:important|critical|essential|key)\\s+to\\s+([^.]+)/i,\n    ];\n\n    // 3. Extract best practices from messages\n    const bestPractices = [];\n    let messageIdToPractices = {};\n\n    for (const message of messages) {\n      const content = message.content;\n      if (!content) continue;\n\n      // Split content into sentences for more precise pattern matching\n      const sentences = content.split(/[.!?]\\s+/);\n\n      for (const sentence of sentences) {\n        for (const pattern of bestPracticePatterns) {\n          const matches = sentence.match(pattern);\n          if (matches && matches[1]) {\n            const description = matches[1].trim();\n\n            // Assign confidence based on the strength of the pattern\n            let confidence = 0.6; // Base confidence\n\n            // Adjust confidence based on keywords\n            if (/best practice|always|never|must|essential/i.test(sentence)) {\n              confidence += 0.2;\n            } else if (/should|recommended|better|important/i.test(sentence)) {\n              confidence += 0.1;\n            }\n\n            // Store practices with their message ID for later reference extraction\n            if (!messageIdToPractices[message.message_id]) {\n              messageIdToPractices[message.message_id] = [];\n            }\n\n            messageIdToPractices[message.message_id].push({\n              description,\n              confidence,\n              messageId: message.message_id,\n            });\n          }\n        }\n      }\n    }\n\n    // 4. For each message with best practices, look for code references\n    for (const messageId in messageIdToPractices) {\n      const message = messages.find((m) => m.message_id === messageId);\n      if (!message) continue;\n\n      // Look for code blocks in the message\n      const codeBlocks = message.content.match(/```[\\s\\S]*?```/g) || [];\n      const codeReferences = codeBlocks.map((block) =>\n        block.replace(/```/g, \"\").trim()\n      );\n\n      // Add code references to each practice from this message\n      for (const practice of messageIdToPractices[messageId]) {\n        if (codeReferences.length > 0) {\n          practice.codeReferences = codeReferences;\n          // Boost confidence if code examples are provided\n          practice.confidence = Math.min(0.9, practice.confidence + 0.1);\n        }\n\n        bestPractices.push(practice);\n      }\n    }\n\n    // 5. Remove duplicates\n    const uniquePractices = [];\n    const seenDescriptions = new Set();\n\n    for (const practice of bestPractices) {\n      if (!seenDescriptions.has(practice.description)) {\n        seenDescriptions.add(practice.description);\n        uniquePractices.push(practice);\n      }\n    }\n\n    console.log(\n      `[LearningSystem] Extracted ${uniquePractices.length} best practices from conversation ${conversationId}`\n    );\n    return uniquePractices;\n  } catch (error) {\n    console.error(`[LearningSystem] Error extracting best practices:`, error);\n    return [];\n  }\n}\n\n/**\n * Extracts general learning points from conversation messages\n *\n * @param {string[]} messageContents - Array of message contents to analyze\n * @returns {Promise<Array>} Array of general learnings with text and confidence scores\n */\nexport async function extractGeneralLearnings(messageContents) {\n  try {\n    console.log(\n      `[LearningSystem] Extracting general learnings from ${messageContents.length} messages`\n    );\n\n    if (\n      !messageContents ||\n      !Array.isArray(messageContents) ||\n      messageContents.length === 0\n    ) {\n      return [];\n    }\n\n    // 1. Define patterns for identifying factual statements and key takeaways\n    const learningPatterns = [\n      /(?:in\\s+(?:conclusion|summary)|to\\s+summarize|summing\\s+up|overall|in\\s+essence),\\s+([^.]+)/i,\n      /(?:the\\s+(?:key|main|important|critical|essential)\\s+(?:point|takeaway|learning|insight|fact|thing to remember))\\s+(?:is|was|would be)\\s+([^.]+)/i,\n      /(?:I|we|you)\\s+(?:learned|discovered|found out|realized|understand|know)\\s+(?:that|how|why)\\s+([^.]+)/i,\n      /(?:it['']s|its)\\s+(?:worth|important|useful|helpful)\\s+(?:noting|remembering|understanding|recognizing)\\s+(?:that|how|why)\\s+([^.]+)/i,\n      /(?:fact|truth|reality|principle|rule|concept|discovery|revelation|insight):\\s+([^.]+)/i,\n    ];\n\n    // Patterns that might indicate conclusion statements\n    const conclusionIndicators = [\n      /(?:in\\s+(?:conclusion|summary)|to\\s+summarize|summing\\s+up|finally|lastly)/i,\n      /(?:key|main|important|critical)\\s+(?:point|takeaway|lesson|learning)/i,\n      /(?:remember|note|understand|fundamental|essentially)/i,\n    ];\n\n    // 2. Extract learnings from each message\n    const generalLearnings = [];\n\n    for (let i = 0; i < messageContents.length; i++) {\n      const content = messageContents[i];\n      if (!content) continue;\n\n      // Split content into sentences\n      const sentences = content.split(/[.!?]\\s+/);\n\n      // Check each sentence for learning patterns\n      for (let j = 0; j < sentences.length; j++) {\n        const sentence = sentences[j];\n        let learning = null;\n        let confidenceScore = 0;\n\n        // Check for explicit learning patterns\n        for (const pattern of learningPatterns) {\n          const matches = sentence.match(pattern);\n          if (matches && matches[1]) {\n            learning = matches[1].trim();\n            confidenceScore = 0.8; // High confidence for explicit patterns\n            break;\n          }\n        }\n\n        // If no explicit pattern matched, check if this might be a conclusion in the last 25% of the message\n        if (!learning && j > sentences.length * 0.75) {\n          // Check for conclusion indicators\n          for (const indicator of conclusionIndicators) {\n            if (indicator.test(sentence)) {\n              learning = sentence.trim();\n              confidenceScore = 0.7; // Slightly lower confidence\n              break;\n            }\n          }\n        }\n\n        // If still no match, check for declarative statements that might be factual\n        if (\n          !learning &&\n          /^(?:the|a|this|these|those)\\s+\\w+\\s+(?:is|are|was|were|has|have|had)\\s+/i.test(\n            sentence\n          )\n        ) {\n          // Look for strong factual indicators\n          if (\n            /always|never|every|all|none|must|proven|demonstrated|shown|verified|confirmed/i.test(\n              sentence\n            )\n          ) {\n            learning = sentence.trim();\n            confidenceScore = 0.6; // Medium confidence\n          }\n        }\n\n        // Add the learning if found\n        if (learning) {\n          generalLearnings.push({\n            text: learning,\n            confidence: confidenceScore,\n            messageId: `message_${i}`, // Using index as we don't have actual messageIds\n          });\n        }\n      }\n    }\n\n    // 3. Remove duplicates and very short learnings\n    const uniqueLearnings = [];\n    const seenTexts = new Set();\n\n    for (const learning of generalLearnings) {\n      if (learning.text.length < 10) continue; // Skip very short learnings\n\n      if (!seenTexts.has(learning.text)) {\n        seenTexts.add(learning.text);\n        uniqueLearnings.push(learning);\n      }\n    }\n\n    console.log(\n      `[LearningSystem] Extracted ${uniqueLearnings.length} general learnings from messages`\n    );\n    return uniqueLearnings;\n  } catch (error) {\n    console.error(\n      `[LearningSystem] Error extracting general learnings:`,\n      error\n    );\n    return [];\n  }\n}\n\n/**\n * Extracts key-value pairs of knowledge from a collection of messages\n *\n * @param {Array<Object>} messages - Array of message objects\n * @param {string} conversationId - ID of the conversation\n * @returns {Promise<Array<{key: string, value: string, confidence: number}>>} Array of extracted key-value pairs\n */\nexport async function extractKeyValuePairs(messages, conversationId) {\n  try {\n    console.log(\n      `[LearningSystem] Extracting key-value pairs from conversation ${conversationId}`\n    );\n\n    if (!messages || messages.length === 0) {\n      console.log(\n        \"[LearningSystem] No messages provided for key-value extraction\"\n      );\n      return [];\n    }\n\n    // Get the text content from messages\n    const messageContents = messages\n      .map((msg) => msg.content || \"\")\n      .filter((content) => content.trim().length > 0);\n\n    if (messageContents.length === 0) {\n      return [];\n    }\n\n    // Simple heuristic extraction of key-value pairs\n    // Look for patterns like \"X: Y\", \"X - Y\", \"key is value\", etc.\n    const extractedPairs = [];\n\n    for (const content of messageContents) {\n      // Process common patterns\n      // Pattern 1: \"Key: Value\" or \"Key - Value\"\n      const colonPattern = /^([^:]+):\\s*(.+)$/gm;\n      let match;\n\n      while ((match = colonPattern.exec(content)) !== null) {\n        const key = match[1].trim();\n        const value = match[2].trim();\n\n        if (key && value && key.length < 100 && !key.includes(\"\\n\")) {\n          extractedPairs.push({\n            key,\n            value,\n            confidence: 0.8,\n          });\n        }\n      }\n\n      // Pattern 2: \"The X is Y\" or \"X is Y\"\n      const isPattern = /(?:The\\s+)?([A-Za-z0-9\\s_-]+)\\s+is\\s+([^.]+)/g;\n\n      while ((match = isPattern.exec(content)) !== null) {\n        const key = match[1].trim();\n        const value = match[2].trim();\n\n        if (key && value && key.length < 50 && !key.includes(\"\\n\")) {\n          extractedPairs.push({\n            key,\n            value,\n            confidence: 0.6,\n          });\n        }\n      }\n    }\n\n    // Deduplicate by key (keep highest confidence)\n    const keyMap = new Map();\n\n    for (const pair of extractedPairs) {\n      const existingPair = keyMap.get(pair.key.toLowerCase());\n\n      if (!existingPair || existingPair.confidence < pair.confidence) {\n        keyMap.set(pair.key.toLowerCase(), pair);\n      }\n    }\n\n    // Convert back to array\n    return Array.from(keyMap.values());\n  } catch (error) {\n    console.error(\"[LearningSystem] Error extracting key-value pairs:\", error);\n    return [];\n  }\n}\n\n/**\n * Stores a code pattern in the database\n *\n * @param {Object} pattern - The pattern to store\n * @param {string} pattern.name - Name of the pattern\n * @param {string} pattern.description - Description of the pattern\n * @param {string} pattern.representation - String representation of the pattern\n * @param {string} pattern.category - Category of the pattern\n * @param {string} [pattern.language] - Programming language (if applicable)\n * @param {number} [pattern.confidence=0.7] - Confidence score (0-1)\n * @param {string} [pattern.conversationId] - ID of conversation where pattern was discovered\n * @returns {Promise<Object>} The stored pattern\n */\nexport async function storePattern(pattern) {\n  try {\n    console.log(`[LearningSystem] Storing pattern: ${pattern.name}`);\n\n    if (\n      !pattern ||\n      !pattern.name ||\n      !pattern.description ||\n      !pattern.representation\n    ) {\n      throw new Error(\"Invalid pattern: missing required fields\");\n    }\n\n    const patternId = pattern.id || uuidv4();\n    const now = new Date().toISOString();\n    const confidence = pattern.confidence || 0.7;\n\n    // Insert into project_patterns table\n    const query = `\n      INSERT INTO project_patterns \n      (pattern_id, pattern_type, name, description, representation, language, confidence_score, created_at, updated_at, session_origin_id) \n      VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n      ON CONFLICT(pattern_id) DO UPDATE SET\n        name = excluded.name,\n        description = excluded.description,\n        representation = excluded.representation,\n        language = excluded.language,\n        confidence_score = excluded.confidence_score,\n        updated_at = excluded.updated_at\n    `;\n\n    await executeQuery(query, [\n      patternId,\n      pattern.category || \"code_pattern\",\n      pattern.name,\n      pattern.description,\n      pattern.representation,\n      pattern.language || null,\n      confidence,\n      now,\n      now,\n      pattern.conversationId || null,\n    ]);\n\n    return {\n      id: patternId,\n      ...pattern,\n      created_at: now,\n      updated_at: now,\n    };\n  } catch (error) {\n    console.error(\"[LearningSystem] Error storing pattern:\", error);\n    throw new Error(`Failed to store pattern: ${error.message}`);\n  }\n}\n\n/**\n * Stores a bug pattern in the database\n *\n * @param {Object} bugPattern - The bug pattern to store\n * @param {string} bugPattern.name - Name of the bug pattern\n * @param {string} bugPattern.description - Description of the bug pattern\n * @param {string} bugPattern.representation - String representation of the bug pattern\n * @param {string} [bugPattern.solution] - Solution for the bug\n * @param {string} [bugPattern.language] - Programming language (if applicable)\n * @param {number} [bugPattern.confidence=0.7] - Confidence score (0-1)\n * @param {string} [bugPattern.conversationId] - ID of conversation where bug pattern was discovered\n * @returns {Promise<Object>} The stored bug pattern\n */\nexport async function storeBugPattern(bugPattern) {\n  try {\n    console.log(`[LearningSystem] Storing bug pattern: ${bugPattern.name}`);\n\n    if (!bugPattern || !bugPattern.name || !bugPattern.description) {\n      throw new Error(\"Invalid bug pattern: missing required fields\");\n    }\n\n    const patternId = bugPattern.id || uuidv4();\n    const now = new Date().toISOString();\n    const confidence = bugPattern.confidence || 0.7;\n\n    // Enhance the representation with the solution if available\n    let representation = bugPattern.representation;\n    if (bugPattern.solution && typeof representation === \"object\") {\n      representation = {\n        ...JSON.parse(\n          typeof representation === \"string\"\n            ? representation\n            : JSON.stringify(representation)\n        ),\n        solution: bugPattern.solution,\n      };\n      representation = JSON.stringify(representation);\n    } else if (bugPattern.solution && typeof representation === \"string\") {\n      try {\n        const parsed = JSON.parse(representation);\n        parsed.solution = bugPattern.solution;\n        representation = JSON.stringify(parsed);\n      } catch (e) {\n        // Not valid JSON, keep as is\n      }\n    }\n\n    // Insert into project_patterns table with bug_pattern type\n    const query = `\n      INSERT INTO project_patterns \n      (pattern_id, pattern_type, name, description, representation, language, confidence_score, created_at, updated_at, session_origin_id) \n      VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n      ON CONFLICT(pattern_id) DO UPDATE SET\n        name = excluded.name,\n        description = excluded.description,\n        representation = excluded.representation,\n        language = excluded.language,\n        confidence_score = excluded.confidence_score,\n        updated_at = excluded.updated_at\n    `;\n\n    await executeQuery(query, [\n      patternId,\n      \"bug_pattern\",\n      bugPattern.name,\n      bugPattern.description,\n      representation,\n      bugPattern.language || null,\n      confidence,\n      now,\n      now,\n      bugPattern.conversationId || null,\n    ]);\n\n    return {\n      id: patternId,\n      ...bugPattern,\n      created_at: now,\n      updated_at: now,\n      pattern_type: \"bug_pattern\",\n    };\n  } catch (error) {\n    console.error(\"[LearningSystem] Error storing bug pattern:\", error);\n    throw new Error(`Failed to store bug pattern: ${error.message}`);\n  }\n}\n\n/**\n * Stores a key-value pair of knowledge in the database\n *\n * @param {Object} keyValuePair - The key-value pair to store\n * @param {string} keyValuePair.key - The key (concept, term, etc.)\n * @param {string} keyValuePair.value - The value (definition, explanation, etc.)\n * @param {number} [keyValuePair.confidence=0.7] - Confidence score (0-1)\n * @param {string} [keyValuePair.category=\"general\"] - Category of knowledge\n * @param {string} [keyValuePair.conversationId] - ID of conversation where knowledge was discovered\n * @returns {Promise<Object>} The stored key-value pair\n */\nexport async function storeKeyValuePair(keyValuePair) {\n  try {\n    console.log(\n      `[LearningSystem] Storing knowledge key-value pair: ${keyValuePair.key}`\n    );\n\n    if (!keyValuePair || !keyValuePair.key || !keyValuePair.value) {\n      throw new Error(\"Invalid key-value pair: missing required fields\");\n    }\n\n    const knowledgeId = keyValuePair.id || uuidv4();\n    const now = new Date().toISOString();\n    const confidence = keyValuePair.confidence || 0.7;\n    const category = keyValuePair.category || \"general\";\n\n    // Insert into knowledge_base table\n    const query = `\n      INSERT INTO knowledge_items \n      (item_id, item_type, name, content, metadata, confidence_score, created_at, updated_at, conversation_id) \n      VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n      ON CONFLICT(item_id) DO UPDATE SET\n        name = excluded.name,\n        content = excluded.content,\n        metadata = excluded.metadata,\n        confidence_score = excluded.confidence_score,\n        updated_at = excluded.updated_at\n    `;\n\n    // Create metadata\n    const metadata = JSON.stringify({\n      category,\n      source: keyValuePair.conversationId ? \"conversation\" : \"analysis\",\n      conversationId: keyValuePair.conversationId || null,\n    });\n\n    await executeQuery(query, [\n      knowledgeId,\n      \"concept_definition\",\n      keyValuePair.key,\n      keyValuePair.value,\n      metadata,\n      confidence,\n      now,\n      now,\n      keyValuePair.conversationId || null,\n    ]);\n\n    return {\n      id: knowledgeId,\n      ...keyValuePair,\n      created_at: now,\n      updated_at: now,\n      item_type: \"concept_definition\",\n    };\n  } catch (error) {\n    console.error(\"[LearningSystem] Error storing key-value pair:\", error);\n    throw new Error(`Failed to store key-value pair: ${error.message}`);\n  }\n}\n", "/**\n * SemanticPatternRecognizerLogic.js\n *\n * Logic for recognizing semantic patterns in code entities.\n * Uses both textual and structural analysis to identify patterns.\n */\n\nimport * as TextTokenizerLogic from \"./TextTokenizerLogic.js\";\nimport * as CodeStructureAnalyzerLogic from \"./CodeStructureAnalyzerLogic.js\";\nimport * as RelationshipContextManagerLogic from \"./RelationshipContextManagerLogic.js\";\nimport { executeQuery } from \"../db.js\";\nimport { v4 as uuidv4 } from \"uuid\";\n\n/**\n * @typedef {Object} Pattern\n * @property {string} id - Unique identifier for the pattern\n * @property {string} name - Human-readable name for the pattern\n * @property {string} description - Description of what the pattern represents\n * @property {string} language - Programming language this pattern applies to (e.g., 'javascript', 'python', or 'any' for language-agnostic patterns)\n * @property {string} category - Category of the pattern (e.g., 'design_pattern', 'antipattern', 'common_idiom')\n * @property {string} representation - Textual or structured representation of the pattern\n * @property {string} detection_rules - JSON string of rules used to detect this pattern\n * @property {number} importance - Importance score of this pattern (0-1)\n * @property {string} created_at - When this pattern was created\n * @property {string} updated_at - When this pattern was last updated\n */\n\n/**\n * @typedef {Object} PatternDefinition\n * @property {string} name - Human-readable name for the pattern\n * @property {string} description - Description of what the pattern represents\n * @property {string} language - Programming language this pattern applies to (e.g., 'javascript', 'python', or 'any' for language-agnostic patterns)\n * @property {string} category - Category of the pattern\n * @property {string} representation - Textual or structured representation of the pattern\n * @property {Object} detection_rules - Rules used to detect this pattern\n * @property {number} importance - Importance score of this pattern (0-1)\n */\n\n/**\n * @typedef {Object} CodeEntity\n * @property {string} id - Unique identifier for the code entity\n * @property {string} path - File path of the code entity\n * @property {string} type - Type of code entity ('file', 'function', 'class', etc.)\n * @property {string} name - Name of the code entity\n * @property {string} content - Content of the code entity\n * @property {string} raw_content - Raw unprocessed content of the entity\n * @property {string} language - Programming language of the entity\n * @property {Object} custom_metadata - Optional metadata including structural information\n */\n\n/**\n * Recognizes semantic patterns in a code entity\n *\n * @param {CodeEntity} entity - The code entity to analyze\n * @returns {Promise<{patterns: Pattern[], confidence: number}>} Matched patterns and overall confidence\n */\nexport async function recognizePatterns(entity) {\n  try {\n    // 1. Extract key information from the entity\n    const { content, raw_content, language, type, custom_metadata } = entity;\n\n    // If entity has no content, return empty result\n    if (!content && !raw_content) {\n      return { patterns: [], confidence: 0 };\n    }\n\n    const entityContent = raw_content || content;\n\n    // 2. Get structural features - either from metadata or by analyzing\n    let structuralFeatures = custom_metadata?.structuralFeatures;\n\n    if (!structuralFeatures) {\n      // Build AST and extract structural features\n      const ast = await CodeStructureAnalyzerLogic.buildAST(\n        entityContent,\n        language\n      );\n      structuralFeatures =\n        await CodeStructureAnalyzerLogic.extractStructuralFeatures(ast);\n    }\n\n    // 3. Get token-based features using TextTokenizerLogic\n    const tokenizedContent = TextTokenizerLogic.tokenize(entityContent);\n    const keywords = TextTokenizerLogic.extractKeywords(tokenizedContent);\n    const codeNgrams = TextTokenizerLogic.extractNGrams(tokenizedContent, 3); // Extract up to 3-grams\n\n    // 4. Retrieve known patterns from database\n    const knownPatterns = await getKnownPatterns({\n      language: language, // Filter by entity's language\n      minConfidence: 0.3, // Only get reasonably confident patterns\n    });\n\n    if (knownPatterns.length === 0) {\n      return { patterns: [], confidence: 0 };\n    }\n\n    // 5. Match patterns against the entity\n    const matchResults = await Promise.all(\n      knownPatterns.map((pattern) =>\n        matchPattern(\n          pattern,\n          entityContent,\n          structuralFeatures,\n          keywords,\n          codeNgrams,\n          type\n        )\n      )\n    );\n\n    // 6. Filter patterns with positive matches and sort by confidence\n    const matchedPatterns = matchResults\n      .filter((result) => result.confidence > 0.1) // Only include patterns with reasonable confidence\n      .sort((a, b) => b.confidence - a.confidence);\n\n    // 7. Calculate overall confidence (weighted average based on pattern importance)\n    let overallConfidence = 0;\n    let totalImportance = 0;\n\n    if (matchedPatterns.length > 0) {\n      for (const match of matchedPatterns) {\n        const importance = match.pattern.importance || 0.5; // Default importance if not specified\n        overallConfidence += match.confidence * importance;\n        totalImportance += importance;\n      }\n\n      overallConfidence =\n        totalImportance > 0\n          ? overallConfidence / totalImportance\n          : matchedPatterns[0].confidence; // If no importance values, use highest confidence\n    }\n\n    // 8. Return matched patterns and overall confidence\n    return {\n      patterns: matchedPatterns.map((match) => match.pattern),\n      confidence: overallConfidence,\n    };\n  } catch (error) {\n    console.error(\"Error in pattern recognition:\", error);\n    return { patterns: [], confidence: 0 };\n  }\n}\n\n/**\n * Retrieves known patterns from the database with optional filtering\n *\n * @param {Object} filterOptions - Options to filter the patterns\n * @param {string} [filterOptions.type] - Filter by pattern type\n * @param {number} [filterOptions.minConfidence] - Filter by minimum confidence score\n * @param {string} [filterOptions.language] - Filter by programming language\n * @returns {Promise<Pattern[]>} Array of patterns matching the filters\n */\nexport async function getKnownPatterns(filterOptions = {}) {\n  try {\n    const { type, minConfidence, language } = filterOptions;\n\n    // Build the query\n    let query = \"SELECT * FROM project_patterns WHERE 1=1\";\n    const params = [];\n\n    // Apply type filter\n    if (type) {\n      query += \" AND pattern_type = ?\";\n      params.push(type);\n    }\n\n    // Apply confidence filter\n    if (minConfidence !== undefined && !isNaN(minConfidence)) {\n      query += \" AND confidence_score >= ?\";\n      params.push(minConfidence);\n    }\n\n    // Apply language filter\n    if (language) {\n      query += \" AND (language = ? OR language = ? OR language IS NULL)\";\n      params.push(language, \"any\"); // Include language-specific, universal patterns, and legacy NULL values\n    }\n\n    // Order by confidence and frequency\n    query += \" ORDER BY confidence_score DESC, frequency DESC\";\n\n    // Execute the query\n    const patterns = await executeQuery(query, params);\n\n    // Parse detection_rules JSON for each pattern\n    return patterns.map((pattern) => ({\n      ...pattern,\n      detection_rules: JSON.parse(pattern.detection_rules || \"{}\"),\n    }));\n  } catch (error) {\n    console.error(\"Error retrieving patterns with filters:\", error);\n    throw new Error(`Failed to retrieve patterns: ${error.message}`);\n  }\n}\n\n/**\n * Retrieves known patterns from the database\n *\n * @param {string} language - Programming language to filter by (optional)\n * @returns {Promise<Pattern[]>} Array of known patterns\n * @private\n */\nasync function _getKnownPatternsInternal(language) {\n  try {\n    let query = \"SELECT * FROM project_patterns\";\n    const params = [];\n\n    // Filter by language if specified\n    if (language) {\n      query += \" WHERE language = ? OR language = ? OR language IS NULL\";\n      params.push(language, \"any\"); // Include language-specific, universal patterns, and legacy NULL values\n    }\n\n    const patterns = await executeQuery(query, params);\n\n    // Parse detection_rules JSON\n    return patterns.map((pattern) => ({\n      ...pattern,\n      detection_rules: JSON.parse(pattern.detection_rules || \"{}\"),\n    }));\n  } catch (error) {\n    console.error(\"Error retrieving known patterns:\", error);\n    return [];\n  }\n}\n\n/**\n * Matches a pattern against an entity\n *\n * @param {Pattern} pattern - The pattern to match\n * @param {string} content - The entity content\n * @param {Object} structuralFeatures - Structural features of the entity\n * @param {string[]} keywords - Extracted keywords from the entity\n * @param {Object[]} codeNgrams - N-grams extracted from the entity\n * @param {string} entityType - Type of the entity (file, function, class, etc.)\n * @returns {Promise<{pattern: Pattern, confidence: number}>} Match result with confidence\n * @private\n */\nasync function matchPattern(\n  pattern,\n  content,\n  structuralFeatures,\n  keywords,\n  codeNgrams,\n  entityType\n) {\n  try {\n    const { detection_rules } = pattern;\n    let textualMatchScore = 0;\n    let structuralMatchScore = 0;\n    let typeMatchScore = 0;\n\n    // Check if pattern applies to this entity type\n    if (\n      detection_rules.applicable_types &&\n      Array.isArray(detection_rules.applicable_types)\n    ) {\n      typeMatchScore = detection_rules.applicable_types.includes(entityType)\n        ? 1\n        : 0;\n\n      // If pattern explicitly doesn't apply to this type, return zero confidence\n      if (typeMatchScore === 0 && detection_rules.strict_type_matching) {\n        return { pattern, confidence: 0 };\n      }\n    } else {\n      // If no type restrictions, full score\n      typeMatchScore = 1;\n    }\n\n    // Perform textual matching\n    if (detection_rules.keywords && Array.isArray(detection_rules.keywords)) {\n      const keywordMatches = detection_rules.keywords.filter((keyword) =>\n        keywords.includes(keyword)\n      );\n\n      textualMatchScore =\n        keywordMatches.length / detection_rules.keywords.length;\n    }\n\n    // Check for text patterns\n    if (\n      detection_rules.text_patterns &&\n      Array.isArray(detection_rules.text_patterns)\n    ) {\n      let patternMatchCount = 0;\n\n      for (const textPattern of detection_rules.text_patterns) {\n        if (typeof textPattern === \"string\") {\n          if (content.includes(textPattern)) {\n            patternMatchCount++;\n          }\n        } else if (\n          textPattern instanceof RegExp ||\n          (typeof textPattern === \"object\" && textPattern.pattern)\n        ) {\n          // Handle regex pattern objects\n          const pattern =\n            textPattern instanceof RegExp\n              ? textPattern\n              : new RegExp(textPattern.pattern, textPattern.flags || \"\");\n\n          if (pattern.test(content)) {\n            patternMatchCount++;\n          }\n        }\n      }\n\n      const textPatternScore =\n        detection_rules.text_patterns.length > 0\n          ? patternMatchCount / detection_rules.text_patterns.length\n          : 0;\n\n      // Combine with keyword score\n      textualMatchScore =\n        textualMatchScore > 0\n          ? (textualMatchScore + textPatternScore) / 2\n          : textPatternScore;\n    }\n\n    // Perform structural matching\n    if (\n      detection_rules.structural_rules &&\n      Array.isArray(detection_rules.structural_rules)\n    ) {\n      let structRuleMatchCount = 0;\n\n      for (const rule of detection_rules.structural_rules) {\n        const { feature, condition, value } = rule;\n\n        // Skip invalid rules\n        if (!feature || !condition || value === undefined) continue;\n\n        // Get the actual feature value\n        const featureValue = structuralFeatures[feature];\n\n        // Skip if feature doesn't exist\n        if (featureValue === undefined) continue;\n\n        // Evaluate condition\n        let matches = false;\n\n        switch (condition) {\n          case \"equals\":\n            matches = featureValue === value;\n            break;\n          case \"contains\":\n            matches = Array.isArray(featureValue)\n              ? featureValue.includes(value)\n              : String(featureValue).includes(String(value));\n            break;\n          case \"greater_than\":\n            matches = Number(featureValue) > Number(value);\n            break;\n          case \"less_than\":\n            matches = Number(featureValue) < Number(value);\n            break;\n          case \"matches_regex\":\n            matches = new RegExp(value).test(String(featureValue));\n            break;\n          default:\n            matches = false;\n        }\n\n        if (matches) {\n          structRuleMatchCount++;\n        }\n      }\n\n      structuralMatchScore =\n        detection_rules.structural_rules.length > 0\n          ? structRuleMatchCount / detection_rules.structural_rules.length\n          : 0;\n    }\n\n    // Calculate combined confidence\n    const weights = detection_rules.weights || {\n      textual: 0.4,\n      structural: 0.4,\n      type: 0.2,\n    };\n\n    // Calculate weighted average\n    const confidence =\n      textualMatchScore * weights.textual +\n      structuralMatchScore * weights.structural +\n      typeMatchScore * weights.type;\n\n    return { pattern, confidence };\n  } catch (error) {\n    console.error(`Error matching pattern ${pattern.name}:`, error);\n    return { pattern, confidence: 0 };\n  }\n}\n\n/**\n * Adds a new pattern to the pattern repository\n *\n * @param {PatternDefinition} patternDefinition - The pattern definition to add\n * @returns {Promise<string>} The ID of the newly added pattern\n */\nexport async function addPatternToRepository(patternDefinition) {\n  try {\n    // 1. Generate a unique ID for the pattern\n    const pattern_id = uuidv4();\n\n    // 2. Extract and prepare pattern data with defaults\n    const {\n      pattern_type,\n      name = `Pattern_${pattern_id.substring(0, 8)}`,\n      description = \"\",\n      representation,\n      detection_rules = \"{}\",\n      language = \"any\",\n    } = patternDefinition;\n\n    // 3. Ensure representation and detection_rules are in string format for storage\n    const representationStr =\n      typeof representation === \"object\"\n        ? JSON.stringify(representation)\n        : representation;\n\n    const detectionRulesStr =\n      typeof detection_rules === \"object\"\n        ? JSON.stringify(detection_rules)\n        : detection_rules;\n\n    // 4. Set default scores and counters\n    const frequency = 1;\n    const utility_score = 0.1;\n    const confidence_score = 0.5;\n    const reinforcement_count = 1;\n    const created_at = new Date().toISOString();\n    const updated_at = created_at;\n\n    // 5. Insert the pattern into the database\n    const query = `\n      INSERT INTO project_patterns (\n        pattern_id, \n        pattern_type, \n        name, \n        description, \n        representation, \n        detection_rules,\n        language,\n        frequency,\n        utility_score,\n        confidence_score,\n        reinforcement_count,\n        created_at,\n        updated_at\n      ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n    `;\n\n    const params = [\n      pattern_id,\n      pattern_type,\n      name,\n      description,\n      representationStr,\n      detectionRulesStr,\n      language,\n      frequency,\n      utility_score,\n      confidence_score,\n      reinforcement_count,\n      created_at,\n      updated_at,\n    ];\n\n    await executeQuery(query, params);\n\n    console.log(`Added new pattern \"${name}\" (${pattern_id}) to repository`);\n\n    // 6. Return the generated pattern ID\n    return pattern_id;\n  } catch (error) {\n    console.error(\"Error adding pattern to repository:\", error);\n    throw new Error(`Failed to add pattern: ${error.message}`);\n  }\n}\n\n/**\n * Finds code entities that match a specific pattern\n *\n * @param {string} patternId - ID of the pattern to match against\n * @param {number} [limit=10] - Maximum number of matches to return\n * @returns {Promise<CodeEntity[]>} Array of code entities that match the pattern\n */\nexport async function findSimilarCodeByPattern(patternId, limit = 10) {\n  try {\n    // 1. Retrieve the pattern from the database\n    const patternQuery = \"SELECT * FROM project_patterns WHERE pattern_id = ?\";\n    const patterns = await executeQuery(patternQuery, [patternId]);\n\n    if (patterns.length === 0) {\n      console.warn(`Pattern with ID ${patternId} not found`);\n      return [];\n    }\n\n    const pattern = {\n      ...patterns[0],\n      detection_rules: JSON.parse(patterns[0].detection_rules || \"{}\"),\n    };\n\n    // 2. Determine if we can optimize by filtering entities\n    const preFilters = [];\n    const preFilterParams = [];\n\n    // Filter by language if the pattern is language-specific\n    if (pattern.language && pattern.language !== \"any\") {\n      preFilters.push(\"language = ?\");\n      preFilterParams.push(pattern.language);\n    }\n\n    // Filter by entity type if the pattern has applicable types\n    if (\n      pattern.detection_rules.applicable_types &&\n      Array.isArray(pattern.detection_rules.applicable_types) &&\n      pattern.detection_rules.applicable_types.length > 0\n    ) {\n      const typePlaceholders = pattern.detection_rules.applicable_types\n        .map(() => \"?\")\n        .join(\", \");\n      preFilters.push(`type IN (${typePlaceholders})`);\n      preFilterParams.push(...pattern.detection_rules.applicable_types);\n    }\n\n    // 3. Create a query to get candidate entities\n    let entityQuery = \"SELECT * FROM code_entities\";\n\n    if (preFilters.length > 0) {\n      entityQuery += \" WHERE \" + preFilters.join(\" AND \");\n    }\n\n    // 4. Perform keyword search optimization if possible\n    if (\n      pattern.detection_rules.keywords &&\n      Array.isArray(pattern.detection_rules.keywords) &&\n      pattern.detection_rules.keywords.length > 0\n    ) {\n      // Get the first few keywords to use as a pre-filter\n      // This optimization assumes there's a full-text search or that content is indexed\n      // We'll limit to 3 keywords to avoid over-filtering\n      const keywordsToUse = pattern.detection_rules.keywords.slice(0, 3);\n\n      // Search for entities with content containing any of these keywords\n      // This is a simplified approach - a real implementation might use a more sophisticated\n      // full-text search or entity_keywords table\n      if (keywordsToUse.length > 0) {\n        const keywordConditions = keywordsToUse\n          .map((keyword) => \"content LIKE ?\")\n          .join(\" OR \");\n\n        if (preFilters.length > 0) {\n          entityQuery += ` AND (${keywordConditions})`;\n        } else {\n          entityQuery += ` WHERE (${keywordConditions})`;\n        }\n\n        // Add the LIKE parameters with wildcards\n        keywordsToUse.forEach((keyword) => {\n          preFilterParams.push(`%${keyword}%`);\n        });\n      }\n    }\n\n    // Add a reasonable limit to avoid processing too many entities\n    // We'll process more than the requested limit since some might not match\n    const processingLimit = Math.min(limit * 5, 100);\n    entityQuery += ` LIMIT ${processingLimit}`;\n\n    // 5. Get candidate entities\n    const entities = await executeQuery(entityQuery, preFilterParams);\n\n    // 6. Check each entity for pattern matches\n    const matchResults = [];\n\n    for (const entity of entities) {\n      // Perform pattern matching similar to recognizePatterns but for a single pattern\n      try {\n        // Extract content and prepare for analysis\n        const entityContent = entity.raw_content || entity.content;\n\n        if (!entityContent) continue;\n\n        // Get token-based features\n        const tokenizedContent = TextTokenizerLogic.tokenize(entityContent);\n        const keywords = TextTokenizerLogic.extractKeywords(tokenizedContent);\n        const codeNgrams = TextTokenizerLogic.extractNGrams(\n          tokenizedContent,\n          3\n        );\n\n        // Get or generate structural features if needed for this pattern\n        let structuralFeatures = entity.custom_metadata?.structuralFeatures;\n\n        // Only parse the AST if the pattern has structural rules and we don't already have features\n        const needsStructuralAnalysis =\n          pattern.detection_rules.structural_rules && !structuralFeatures;\n\n        if (needsStructuralAnalysis) {\n          try {\n            const ast = await CodeStructureAnalyzerLogic.buildAST(\n              entityContent,\n              entity.language\n            );\n            structuralFeatures =\n              await CodeStructureAnalyzerLogic.extractStructuralFeatures(ast);\n          } catch (error) {\n            console.warn(\n              `Could not analyze structure for entity ${entity.id}:`,\n              error\n            );\n            structuralFeatures = {};\n          }\n        }\n\n        // Match this entity against the pattern\n        const matchResult = await matchPattern(\n          pattern,\n          entityContent,\n          structuralFeatures || {},\n          keywords,\n          codeNgrams,\n          entity.type\n        );\n\n        // If confidence is above threshold, add to results\n        if (matchResult.confidence > 0.3) {\n          // Using a slightly higher threshold than recognizePatterns\n          matchResults.push({\n            entity,\n            confidence: matchResult.confidence,\n          });\n        }\n      } catch (error) {\n        console.warn(\n          `Error matching entity ${entity.id} against pattern:`,\n          error\n        );\n      }\n    }\n\n    // 7. Sort by confidence and limit results\n    matchResults.sort((a, b) => b.confidence - a.confidence);\n\n    // 8. Return the entities, limited by the requested limit\n    return matchResults.slice(0, limit).map((result) => result.entity);\n  } catch (error) {\n    console.error(\"Error finding similar code by pattern:\", error);\n    return [];\n  }\n}\n\n/**\n * Generates a pattern definition from example code entities\n *\n * @param {CodeEntity[]} examples - Code entities that exemplify the pattern\n * @param {string} name - Name to give the generated pattern\n * @param {string} [patternType='derived_from_examples'] - Type of pattern to create\n * @returns {PatternDefinition} Generated pattern definition\n */\nexport function generatePatternFromExamples(\n  examples,\n  name,\n  patternType = \"derived_from_examples\"\n) {\n  if (!examples || examples.length === 0) {\n    throw new Error(\"At least one example is required to generate a pattern\");\n  }\n\n  // 1. Extract necessary information from examples\n  const language = identifyCommonLanguage(examples);\n  const entityType = identifyCommonEntityType(examples);\n\n  // 2. Extract textual features from all examples\n  const textualFeatures = extractTextualFeatures(examples);\n\n  // 3. Extract structural features if possible\n  const structuralFeatures = extractStructuralFeatures(examples);\n\n  // 4. Generate a description based on examples\n  const description = `Pattern derived from ${examples.length} examples related to ${name}`;\n\n  // 5. Create detection rules based on commonalities\n  const detectionRules = {\n    keywords: textualFeatures.commonKeywords,\n    text_patterns: textualFeatures.commonNgrams.map((ngram) => ngram.text),\n    structural_rules: structuralFeatures.rules,\n    applicable_types: [entityType],\n    weights: {\n      textual: 0.5,\n      structural: 0.4,\n      type: 0.1,\n    },\n  };\n\n  // 6. Create a representation based on the most representative example\n  // Choose the example with the highest number of common features\n  let bestExampleIndex = 0;\n  let bestMatchScore = -1;\n\n  examples.forEach((example, index) => {\n    const content = example.raw_content || example.content;\n    if (!content) return;\n\n    let matchScore = 0;\n\n    // Count how many common keywords and n-grams this example contains\n    const tokenizedContent = TextTokenizerLogic.tokenize(content);\n    const keywords = TextTokenizerLogic.extractKeywords(tokenizedContent);\n\n    textualFeatures.commonKeywords.forEach((keyword) => {\n      if (keywords.includes(keyword)) matchScore++;\n    });\n\n    textualFeatures.commonNgrams.forEach((ngram) => {\n      if (content.includes(ngram.text)) matchScore++;\n    });\n\n    if (matchScore > bestMatchScore) {\n      bestMatchScore = matchScore;\n      bestExampleIndex = index;\n    }\n  });\n\n  // Use the best example as the representation template\n  const representativeExample = examples[bestExampleIndex];\n  const representation = {\n    template:\n      representativeExample.raw_content || representativeExample.content,\n    variables: textualFeatures.variableTokens,\n    structure: structuralFeatures.commonPattern,\n  };\n\n  // 7. Return the pattern definition\n  return {\n    pattern_type: patternType,\n    name,\n    description,\n    language,\n    representation: JSON.stringify(representation),\n    detection_rules: detectionRules,\n    importance: 0.5, // Default moderate importance\n  };\n}\n\n/**\n * Identifies the common programming language from examples\n *\n * @param {CodeEntity[]} examples - Code entities to analyze\n * @returns {string} Common language or 'any' if mixed\n * @private\n */\nfunction identifyCommonLanguage(examples) {\n  const languages = examples.map((ex) => ex.language).filter(Boolean);\n\n  if (languages.length === 0) return \"any\";\n\n  // Check if all examples have the same language\n  const firstLanguage = languages[0];\n  const allSameLanguage = languages.every((lang) => lang === firstLanguage);\n\n  return allSameLanguage ? firstLanguage : \"any\";\n}\n\n/**\n * Identifies the common entity type from examples\n *\n * @param {CodeEntity[]} examples - Code entities to analyze\n * @returns {string} Common entity type\n * @private\n */\nfunction identifyCommonEntityType(examples) {\n  const types = examples.map((ex) => ex.type).filter(Boolean);\n\n  if (types.length === 0) return \"any\";\n\n  // Check if all examples have the same type\n  const firstType = types[0];\n  const allSameType = types.every((type) => type === firstType);\n\n  return allSameType ? firstType : \"any\";\n}\n\n/**\n * Extracts textual features from examples\n *\n * @param {CodeEntity[]} examples - Code entities to analyze\n * @returns {Object} Extracted textual features\n * @private\n */\nfunction extractTextualFeatures(examples) {\n  // 1. Extract tokens, keywords, and n-grams from each example\n  const allKeywords = [];\n  const allNgrams = [];\n  const allTokens = [];\n\n  examples.forEach((example) => {\n    const content = example.raw_content || example.content;\n    if (!content) return;\n\n    const tokenizedContent = TextTokenizerLogic.tokenize(content);\n    const keywords = TextTokenizerLogic.extractKeywords(tokenizedContent);\n    const ngrams = TextTokenizerLogic.extractNGrams(tokenizedContent, 3);\n\n    allKeywords.push(keywords);\n    allNgrams.push(ngrams);\n    allTokens.push(tokenizedContent);\n  });\n\n  // 2. Find common keywords across examples\n  let commonKeywords = [];\n  if (allKeywords.length > 0) {\n    // Start with first example's keywords\n    commonKeywords = [...allKeywords[0]];\n\n    // Intersect with all other examples\n    for (let i = 1; i < allKeywords.length; i++) {\n      commonKeywords = commonKeywords.filter((keyword) =>\n        allKeywords[i].includes(keyword)\n      );\n    }\n\n    // Limit to most significant keywords (top 10)\n    commonKeywords = commonKeywords.slice(0, 10);\n  }\n\n  // 3. Find common n-grams\n  let commonNgrams = [];\n  if (allNgrams.length > 0) {\n    // Create a frequency map of n-grams\n    const ngramFrequency = new Map();\n\n    allNgrams.forEach((exampleNgrams) => {\n      exampleNgrams.forEach((ngram) => {\n        const key = ngram.text;\n        ngramFrequency.set(key, (ngramFrequency.get(key) || 0) + 1);\n      });\n    });\n\n    // Find n-grams that appear in at least half of the examples\n    const threshold = Math.max(1, Math.floor(examples.length / 2));\n\n    commonNgrams = Array.from(ngramFrequency.entries())\n      .filter(([_, count]) => count >= threshold)\n      .map(([text, _]) => ({ text }))\n      .slice(0, 5); // Limit to top 5 common n-grams\n  }\n\n  // 4. Identify variable tokens (tokens that vary across examples)\n  const variableTokens = [];\n\n  // If we have more than one example, find tokens that vary in position\n  if (allTokens.length > 1) {\n    const firstTokens = allTokens[0];\n\n    // Simple approach: look for positions where token differs across examples\n    // For each token position in the first example:\n    for (let i = 0; i < Math.min(firstTokens.length, 30); i++) {\n      // Limit to first 30 tokens\n      if (i >= firstTokens.length) break;\n\n      const token = firstTokens[i];\n      let isVariable = false;\n\n      // Check if this position has different tokens in other examples\n      for (let j = 1; j < allTokens.length; j++) {\n        const otherTokens = allTokens[j];\n        if (i >= otherTokens.length || otherTokens[i] !== token) {\n          isVariable = true;\n          break;\n        }\n      }\n\n      if (isVariable) {\n        variableTokens.push({\n          position: i,\n          examples: examples\n            .map((ex) => {\n              const tokens = TextTokenizerLogic.tokenize(\n                ex.raw_content || ex.content || \"\"\n              );\n              return i < tokens.length ? tokens[i] : null;\n            })\n            .filter(Boolean),\n        });\n      }\n    }\n  }\n\n  return {\n    commonKeywords,\n    commonNgrams,\n    variableTokens,\n  };\n}\n\n/**\n * Extracts structural features from examples\n *\n * @param {CodeEntity[]} examples - Code entities to analyze\n * @returns {Object} Extracted structural features\n * @private\n */\nfunction extractStructuralFeatures(examples) {\n  // Default result with empty values\n  const defaultResult = {\n    rules: [],\n    commonPattern: null,\n  };\n\n  try {\n    // 1. Extract structural features from each example if possible\n    const allFeatures = [];\n\n    for (const example of examples) {\n      const content = example.raw_content || example.content;\n      if (!content) continue;\n\n      // Use existing structural features if available\n      if (example.custom_metadata?.structuralFeatures) {\n        allFeatures.push(example.custom_metadata.structuralFeatures);\n        continue;\n      }\n\n      // Otherwise try to extract features (synchronously)\n      try {\n        // Note: We're calling async functions synchronously here which is not ideal,\n        // but for simplicity in this example we'll assume they can work synchronously\n        const ast = CodeStructureAnalyzerLogic.buildAST(\n          content,\n          example.language\n        );\n        if (!ast) continue;\n\n        const features =\n          CodeStructureAnalyzerLogic.extractStructuralFeatures(ast);\n        if (features) {\n          allFeatures.push(features);\n        }\n      } catch (error) {\n        console.warn(\n          `Could not extract structural features for example: ${error.message}`\n        );\n      }\n    }\n\n    if (allFeatures.length === 0) {\n      return defaultResult;\n    }\n\n    // 2. Find common structural properties\n    const structuralRules = [];\n\n    // Start with the first example's features\n    const firstFeatures = allFeatures[0];\n\n    // For each property in the first example, check if it's common across all examples\n    for (const [feature, value] of Object.entries(firstFeatures)) {\n      // Skip if the value is complex or undefined\n      if (typeof value === \"undefined\" || typeof value === \"object\") continue;\n\n      // Check if this feature has the same value across all examples\n      const isCommon = allFeatures.every((features) => {\n        return features[feature] === value;\n      });\n\n      // If common, add a structural rule\n      if (isCommon) {\n        structuralRules.push({\n          feature,\n          condition: \"equals\",\n          value,\n        });\n      }\n      // If not exactly the same but similar (for numeric values)\n      else if (typeof value === \"number\") {\n        // Calculate range\n        const values = allFeatures\n          .map((f) => f[feature])\n          .filter((v) => typeof v === \"number\");\n        const min = Math.min(...values);\n        const max = Math.max(...values);\n\n        // If there's a reasonable range, add a range rule\n        if (max - min < max * 0.5) {\n          // Max is no more than 50% larger than min\n          structuralRules.push({\n            feature,\n            condition: \"greater_than\",\n            value: min * 0.9, // 10% below minimum observed\n          });\n\n          structuralRules.push({\n            feature,\n            condition: \"less_than\",\n            value: max * 1.1, // 10% above maximum observed\n          });\n        }\n      }\n    }\n\n    // 3. Identify common structural pattern\n    // For simplicity, we'll use the most important structural features\n    const commonPattern = {\n      nodeType: examples[0].type,\n      structuralRules: structuralRules.slice(0, 3), // Top 3 rules\n      complexity:\n        allFeatures.reduce((sum, f) => sum + (f.complexity || 0), 0) /\n        allFeatures.length,\n    };\n\n    return {\n      rules: structuralRules,\n      commonPattern,\n    };\n  } catch (error) {\n    console.error(\"Error extracting structural features:\", error);\n    return defaultResult;\n  }\n}\n\n/**\n * Detects design patterns in a set of code entities\n *\n * @param {CodeEntity[]} entities - Code entities to analyze\n * @returns {Array<{patternType: string, entities: string[], confidence: number}>} Detected design patterns\n */\nexport async function detectDesignPatterns(entities) {\n  if (!entities || entities.length === 0) {\n    return [];\n  }\n\n  // Results array\n  const detectedPatterns = [];\n\n  // Get entity IDs for relationship lookup\n  const entityIds = entities.map((entity) => entity.id);\n\n  // Get relationships between entities if available\n  let relationships = [];\n  try {\n    relationships = await RelationshipContextManagerLogic.getRelationships(\n      entityIds\n    );\n  } catch (error) {\n    console.warn(\"Error retrieving relationships between entities:\", error);\n    // Continue without relationships\n  }\n\n  // Define pattern detectors\n  const patternDetectors = [\n    detectSingletonPattern,\n    detectFactoryPattern,\n    detectObserverPattern,\n    // Add more pattern detectors here as needed\n  ];\n\n  // Apply each detector\n  for (const detector of patternDetectors) {\n    const result = await detector(entities, relationships);\n    if (result.length > 0) {\n      detectedPatterns.push(...result);\n    }\n  }\n\n  return detectedPatterns;\n}\n\n/**\n * Detects Singleton pattern\n *\n * @param {CodeEntity[]} entities - Code entities to analyze\n * @param {Array} relationships - Relationships between entities\n * @returns {Array<{patternType: string, entities: string[], confidence: number}>} Detected patterns\n * @private\n */\nasync function detectSingletonPattern(entities, relationships) {\n  const results = [];\n\n  // Find class entities\n  const classEntities = entities.filter(\n    (entity) => entity.type === \"class\" || entity.type === \"interface\"\n  );\n\n  for (const classEntity of classEntities) {\n    let confidence = 0;\n    let evidence = [];\n\n    const content = classEntity.raw_content || classEntity.content;\n    if (!content) continue;\n\n    // Look for private/protected constructor\n    const hasPrivateConstructor =\n      /private\\s+constructor|protected\\s+constructor/.test(content);\n    if (hasPrivateConstructor) {\n      confidence += 0.3;\n      evidence.push(\"private/protected constructor\");\n    }\n\n    // Look for static instance field\n    const hasStaticInstance =\n      /static\\s+(\\w+)\\s*:\\s*\\w+|static\\s+(\\w+)\\s*=/.test(content);\n    if (hasStaticInstance) {\n      confidence += 0.3;\n      evidence.push(\"static instance field\");\n    }\n\n    // Look for getInstance method\n    const hasGetInstanceMethod =\n      /static\\s+getInstance\\s*\\(|static\\s+instance\\s*\\(|static\\s+get\\s+instance\\s*\\(/.test(\n        content\n      );\n    if (hasGetInstanceMethod) {\n      confidence += 0.4;\n      evidence.push(\"getInstance method\");\n    }\n\n    // Look for self-assignment in constructor\n    const hasSelfAssignment =\n      /this\\._instance\\s*=\\s*this|instance\\s*=\\s*this/.test(content);\n    if (hasSelfAssignment) {\n      confidence += 0.2;\n      evidence.push(\"self-assignment in constructor\");\n    }\n\n    // Check if this class is being instantiated elsewhere\n    const isInstantiatedElsewhere = relationships.some(\n      (rel) =>\n        rel.relationship_type === \"instantiates\" &&\n        rel.target_entity_id === classEntity.id\n    );\n\n    // If instantiated in multiple places, it's less likely to be a Singleton\n    if (isInstantiatedElsewhere) {\n      confidence -= 0.2;\n      evidence.push(\"instantiated elsewhere (negative)\");\n    }\n\n    // If confidence is high enough, add to results\n    if (confidence >= 0.6) {\n      results.push({\n        patternType: \"Singleton\",\n        entities: [classEntity.id],\n        confidence,\n        evidence,\n      });\n    }\n  }\n\n  return results;\n}\n\n/**\n * Detects Factory pattern\n *\n * @param {CodeEntity[]} entities - Code entities to analyze\n * @param {Array} relationships - Relationships between entities\n * @returns {Array<{patternType: string, entities: string[], confidence: number}>} Detected patterns\n * @private\n */\nasync function detectFactoryPattern(entities, relationships) {\n  const results = [];\n\n  // Find class and function entities\n  const classEntities = entities.filter((entity) => entity.type === \"class\");\n  const functionEntities = entities.filter(\n    (entity) => entity.type === \"function\" || entity.type === \"method\"\n  );\n\n  // Look for factory classes\n  for (const classEntity of classEntities) {\n    let confidence = 0;\n    let evidence = [];\n    const involvedEntities = [classEntity.id];\n\n    const content = classEntity.raw_content || classEntity.content;\n    if (!content) continue;\n\n    // Class name suggests Factory\n    if (/Factory|Builder|Creator|Producer/i.test(classEntity.name)) {\n      confidence += 0.2;\n      evidence.push(\"name suggests factory\");\n    }\n\n    // Look for create/make/build methods in the class\n    const hasCreateMethods =\n      /\\b(create|make|build|produce|get)\\w*\\s*\\([^)]*\\)\\s*{/.test(content);\n    if (hasCreateMethods) {\n      confidence += 0.3;\n      evidence.push(\"has creation methods\");\n    }\n\n    // Check if this class has relationships that indicate creation of other objects\n    const creationRelationships = relationships.filter(\n      (rel) =>\n        rel.source_entity_id === classEntity.id &&\n        (rel.relationship_type === \"creates\" ||\n          rel.relationship_type === \"instantiates\")\n    );\n\n    if (creationRelationships.length > 0) {\n      confidence += 0.3;\n      evidence.push(`creates ${creationRelationships.length} other entities`);\n\n      // Add related entities\n      creationRelationships.forEach((rel) => {\n        if (!involvedEntities.includes(rel.target_entity_id)) {\n          involvedEntities.push(rel.target_entity_id);\n        }\n      });\n    }\n\n    // Look for method return types that match other known entities\n    const otherClassNames = classEntities\n      .filter((e) => e.id !== classEntity.id)\n      .map((e) => e.name);\n\n    let returnTypeMatches = 0;\n    for (const otherClass of otherClassNames) {\n      const returnTypeRegex = new RegExp(\n        `:\\\\s*${otherClass}\\\\b|return\\\\s+(new\\\\s+)?${otherClass}\\\\b`\n      );\n      if (returnTypeRegex.test(content)) {\n        returnTypeMatches++;\n      }\n    }\n\n    if (returnTypeMatches > 0) {\n      confidence += 0.2;\n      evidence.push(`returns known types (${returnTypeMatches})`);\n    }\n\n    // If confidence is high enough, add to results\n    if (confidence >= 0.5) {\n      results.push({\n        patternType: \"Factory\",\n        entities: involvedEntities,\n        confidence,\n        evidence,\n      });\n    }\n  }\n\n  // Look for standalone factory functions\n  for (const functionEntity of functionEntities) {\n    let confidence = 0;\n    let evidence = [];\n    const involvedEntities = [functionEntity.id];\n\n    const content = functionEntity.raw_content || functionEntity.content;\n    if (!content) continue;\n\n    // Function name suggests Factory\n    if (/create|make|build|produce|factory|new/i.test(functionEntity.name)) {\n      confidence += 0.3;\n      evidence.push(\"name suggests factory function\");\n    }\n\n    // Check if this function has relationships that indicate creation of objects\n    const creationRelationships = relationships.filter(\n      (rel) =>\n        rel.source_entity_id === functionEntity.id &&\n        (rel.relationship_type === \"creates\" ||\n          rel.relationship_type === \"instantiates\")\n    );\n\n    if (creationRelationships.length > 0) {\n      confidence += 0.3;\n      evidence.push(`creates ${creationRelationships.length} entities`);\n\n      // Add related entities\n      creationRelationships.forEach((rel) => {\n        if (!involvedEntities.includes(rel.target_entity_id)) {\n          involvedEntities.push(rel.target_entity_id);\n        }\n      });\n    }\n\n    // Look for 'new' keyword\n    if (/return\\s+new\\s+\\w+/.test(content)) {\n      confidence += 0.3;\n      evidence.push(\"returns new instance\");\n    }\n\n    // If confidence is high enough, add to results\n    if (confidence >= 0.5) {\n      results.push({\n        patternType: \"Factory\",\n        entities: involvedEntities,\n        confidence,\n        evidence,\n      });\n    }\n  }\n\n  return results;\n}\n\n/**\n * Detects Observer pattern\n *\n * @param {CodeEntity[]} entities - Code entities to analyze\n * @param {Array} relationships - Relationships between entities\n * @returns {Array<{patternType: string, entities: string[], confidence: number}>} Detected patterns\n * @private\n */\nasync function detectObserverPattern(entities, relationships) {\n  const results = [];\n\n  // Find class entities\n  const classEntities = entities.filter((entity) => entity.type === \"class\");\n\n  // Look for potential subject classes\n  for (const potentialSubject of classEntities) {\n    let confidence = 0;\n    let evidence = [];\n    const involvedEntities = [potentialSubject.id];\n\n    const content = potentialSubject.raw_content || potentialSubject.content;\n    if (!content) continue;\n\n    // Look for observer list/collection\n    const hasObserverCollection =\n      /(\\w+)?\\s*observers\\s*=|(\\w+)?\\s*listeners\\s*=/.test(content);\n    if (hasObserverCollection) {\n      confidence += 0.2;\n      evidence.push(\"has observer collection\");\n    }\n\n    // Look for add/remove/notify observer methods\n    const hasAddObserver =\n      /add(Observer|Listener|Subscriber|Handler)|subscribe/.test(content);\n    if (hasAddObserver) {\n      confidence += 0.2;\n      evidence.push(\"has add observer method\");\n    }\n\n    const hasRemoveObserver =\n      /remove(Observer|Listener|Subscriber|Handler)|unsubscribe/.test(content);\n    if (hasRemoveObserver) {\n      confidence += 0.2;\n      evidence.push(\"has remove observer method\");\n    }\n\n    const hasNotifyMethod =\n      /notify|notifyObservers|emit|trigger|dispatch|fire/.test(content);\n    if (hasNotifyMethod) {\n      confidence += 0.3;\n      evidence.push(\"has notify method\");\n    }\n\n    // Look for potential observers\n    let potentialObservers = [];\n\n    // Check relationships for \"observes\" relationship\n    const observerRelationships = relationships.filter(\n      (rel) =>\n        rel.target_entity_id === potentialSubject.id &&\n        rel.relationship_type === \"observes\"\n    );\n\n    if (observerRelationships.length > 0) {\n      confidence += 0.3;\n      evidence.push(`has ${observerRelationships.length} explicit observers`);\n\n      // Add observer entities\n      observerRelationships.forEach((rel) => {\n        const observerId = rel.source_entity_id;\n        if (!involvedEntities.includes(observerId)) {\n          involvedEntities.push(observerId);\n          potentialObservers.push(observerId);\n        }\n      });\n    }\n\n    // If no explicit observers found, look for classes with \"update\" or \"handle\" methods\n    if (potentialObservers.length === 0) {\n      for (const potentialObserver of classEntities) {\n        if (potentialObserver.id === potentialSubject.id) continue;\n\n        const observerContent =\n          potentialObserver.raw_content || potentialObserver.content;\n        if (!observerContent) continue;\n\n        const hasUpdateMethod =\n          /\\bupdate\\s*\\(|\\bhandle\\w+\\s*\\(|\\bon\\w+\\s*\\(/.test(observerContent);\n        if (hasUpdateMethod) {\n          potentialObservers.push(potentialObserver.id);\n          if (!involvedEntities.includes(potentialObserver.id)) {\n            involvedEntities.push(potentialObserver.id);\n          }\n\n          confidence += 0.1;\n          evidence.push(`found potential observer: ${potentialObserver.name}`);\n        }\n      }\n    }\n\n    // If confidence is high enough and we have potential observers, add to results\n    if (confidence >= 0.5 && potentialObservers.length > 0) {\n      results.push({\n        patternType: \"Observer\",\n        entities: involvedEntities,\n        confidence,\n        evidence,\n      });\n    }\n  }\n\n  return results;\n}\n", "/**\n * finalizeConversationContext.tool.js\n *\n * MCP tool implementation for finalizing a conversation context\n * This tool performs learning extraction, pattern promotion, and generates insights\n * when a conversation ends.\n */\n\nimport { z } from \"zod\";\nimport { executeQuery } from \"../db.js\";\nimport * as ConversationIntelligence from \"../logic/ConversationIntelligence.js\";\nimport * as TimelineManagerLogic from \"../logic/TimelineManagerLogic.js\";\nimport * as ActiveContextManager from \"../logic/ActiveContextManager.js\";\nimport * as LearningSystem from \"../logic/LearningSystem.js\";\nimport * as GlobalPatternRepository from \"../logic/GlobalPatternRepository.js\";\nimport * as SmartSearchServiceLogic from \"../logic/SmartSearchServiceLogic.js\";\nimport * as ContextCompressorLogic from \"../logic/ContextCompressorLogic.js\";\nimport * as TextTokenizerLogic from \"../logic/TextTokenizerLogic.js\";\nimport { logMessage } from \"../utils/logger.js\";\n\nimport {\n  finalizeConversationContextInputSchema,\n  finalizeConversationContextOutputSchema,\n} from \"../schemas/toolSchemas.js\";\n\n/**\n * Handler for finalize_conversation_context tool\n *\n * @param {object} input - Tool input parameters\n * @param {object} sdkContext - SDK context\n * @returns {Promise<object>} Tool output\n */\nasync function handler(input, sdkContext) {\n  try {\n    logMessage(\"INFO\", `finalize_conversation_context tool started`, {\n      conversationId: input.conversationId,\n      outcome: input.outcome || \"completed\",\n      clearActiveContext: input.clearActiveContext || false,\n    });\n\n    // 1. Extract input parameters\n    const {\n      conversationId,\n      clearActiveContext = false,\n      extractLearnings = true,\n      promotePatterns = true,\n      synthesizeRelatedTopics = true,\n      generateNextSteps = true,\n      outcome = \"completed\",\n    } = input;\n\n    // Validate conversation ID\n    if (!conversationId) {\n      const error = new Error(\"Conversation ID is required\");\n      error.code = \"MISSING_CONVERSATION_ID\";\n      throw error;\n    }\n\n    logMessage(\"DEBUG\", `Processing options`, {\n      extractLearnings,\n      promotePatterns,\n      synthesizeRelatedTopics,\n      generateNextSteps,\n    });\n\n    // 2. Fetch conversation history, purpose, and topics\n    let conversationHistory = [];\n    let conversationPurpose = null;\n    let conversationTopics = [];\n\n    try {\n      conversationHistory =\n        await ConversationIntelligence.getConversationHistory(conversationId);\n\n      if (!conversationHistory || conversationHistory.length === 0) {\n        const error = new Error(\n          `No conversation history found for ID: ${conversationId}`\n        );\n        error.code = \"CONVERSATION_NOT_FOUND\";\n        throw error;\n      }\n\n      logMessage(\"DEBUG\", `Retrieved conversation history`, {\n        messageCount: conversationHistory.length,\n      });\n    } catch (historyErr) {\n      logMessage(\"ERROR\", `Failed to retrieve conversation history`, {\n        error: historyErr.message,\n        conversationId,\n      });\n      throw historyErr; // This is critical, rethrow\n    }\n\n    // Get conversation purpose\n    try {\n      conversationPurpose =\n        await ConversationIntelligence.getConversationPurpose(conversationId);\n      logMessage(\n        \"DEBUG\",\n        `Retrieved conversation purpose: ${conversationPurpose || \"Unknown\"}`\n      );\n    } catch (purposeErr) {\n      logMessage(\"WARN\", `Failed to retrieve conversation purpose`, {\n        error: purposeErr.message,\n        conversationId,\n      });\n      // Continue without purpose\n    }\n\n    // Get conversation topics\n    try {\n      conversationTopics = await ConversationIntelligence.getConversationTopics(\n        conversationId\n      );\n      logMessage(\n        \"DEBUG\",\n        `Retrieved ${conversationTopics.length} conversation topics`\n      );\n    } catch (topicsErr) {\n      logMessage(\"WARN\", `Failed to retrieve conversation topics`, {\n        error: topicsErr.message,\n        conversationId,\n      });\n      // Continue with empty topics\n      conversationTopics = [];\n    }\n\n    // 3. Generate overall conversation summary\n    let summary = \"\";\n    try {\n      summary = await ConversationIntelligence.summarizeConversation(\n        conversationId\n      );\n      logMessage(\"INFO\", `Generated conversation summary`, {\n        summaryLength: summary.length,\n      });\n    } catch (summaryErr) {\n      logMessage(\"WARN\", `Failed to generate conversation summary`, {\n        error: summaryErr.message,\n        conversationId,\n      });\n      // Use a basic summary as fallback\n      summary = `Conversation ${conversationId} with ${conversationHistory.length} messages`;\n    }\n\n    // 4. Record conversation_end event in the timeline\n    try {\n      await TimelineManagerLogic.recordEvent(\n        \"conversation_end\",\n        {\n          summary,\n          purpose: conversationPurpose,\n          topics: conversationTopics.length,\n          outcome,\n        },\n        [], // No specific entities for conversation end\n        conversationId\n      );\n      logMessage(\"DEBUG\", `Recorded conversation_end event in timeline`);\n    } catch (timelineErr) {\n      logMessage(\"WARN\", `Failed to record conversation_end event`, {\n        error: timelineErr.message,\n        conversationId,\n      });\n      // Continue despite timeline error\n    }\n\n    // 5. Initialize result objects\n    let extractedLearnings = null;\n    let promotedPatterns = null;\n    let relatedConversations = null;\n    let nextSteps = null;\n\n    // 6. Extract learnings if requested\n    if (extractLearnings) {\n      try {\n        logMessage(\"INFO\", `Extracting learnings from conversation`);\n        extractedLearnings = await _extractConversationLearnings(\n          conversationId,\n          conversationHistory\n        );\n        logMessage(\n          \"INFO\",\n          `Extracted ${\n            extractedLearnings?.patterns?.length || 0\n          } patterns and ${\n            extractedLearnings?.bugPatterns?.length || 0\n          } bug patterns`\n        );\n      } catch (learningErr) {\n        logMessage(\"WARN\", `Failed to extract learnings`, {\n          error: learningErr.message,\n          conversationId,\n        });\n        // Continue without learnings\n        extractedLearnings = {\n          patterns: [],\n          bugPatterns: [],\n          conceptualInsights: [],\n          error: learningErr.message,\n        };\n      }\n    } else {\n      logMessage(\"DEBUG\", `Skipping learning extraction (not requested)`);\n    }\n\n    // 7. Promote patterns if requested\n    if (promotePatterns) {\n      try {\n        logMessage(\"INFO\", `Promoting patterns from conversation`);\n        promotedPatterns = await _promoteConversationPatterns(\n          conversationId,\n          outcome\n        );\n        logMessage(\"INFO\", `Promoted ${promotedPatterns?.count || 0} patterns`);\n      } catch (patternErr) {\n        logMessage(\"WARN\", `Failed to promote patterns`, {\n          error: patternErr.message,\n          conversationId,\n        });\n        // Continue without pattern promotion\n        promotedPatterns = {\n          count: 0,\n          patterns: [],\n          error: patternErr.message,\n        };\n      }\n    } else {\n      logMessage(\"DEBUG\", `Skipping pattern promotion (not requested)`);\n    }\n\n    // 8. Synthesize related topics if requested\n    if (synthesizeRelatedTopics) {\n      try {\n        logMessage(\"INFO\", `Finding and synthesizing related conversations`);\n        relatedConversations = await _findAndSynthesizeRelatedConversations(\n          conversationId,\n          conversationTopics,\n          conversationPurpose\n        );\n        logMessage(\n          \"INFO\",\n          `Found ${\n            relatedConversations?.conversations?.length || 0\n          } related conversations`\n        );\n      } catch (relatedErr) {\n        logMessage(\"WARN\", `Failed to synthesize related conversations`, {\n          error: relatedErr.message,\n          conversationId,\n        });\n        // Continue without related conversations\n        relatedConversations = {\n          conversations: [],\n          insights: [],\n          error: relatedErr.message,\n        };\n      }\n    } else {\n      logMessage(\"DEBUG\", `Skipping related topic synthesis (not requested)`);\n    }\n\n    // 9. Generate next step suggestions if requested\n    if (generateNextSteps) {\n      try {\n        logMessage(\"INFO\", `Generating next step suggestions`);\n        nextSteps = await _generateNextStepSuggestions(\n          conversationId,\n          conversationPurpose,\n          summary,\n          extractedLearnings\n        );\n        logMessage(\n          \"INFO\",\n          `Generated ${\n            nextSteps?.suggestions?.length || 0\n          } next step suggestions`\n        );\n      } catch (nextStepsErr) {\n        logMessage(\"WARN\", `Failed to generate next step suggestions`, {\n          error: nextStepsErr.message,\n          conversationId,\n        });\n        // Continue without next steps\n        nextSteps = {\n          suggestions: [],\n          error: nextStepsErr.message,\n        };\n      }\n    } else {\n      logMessage(\"DEBUG\", `Skipping next step generation (not requested)`);\n    }\n\n    // 10. Clear active context if requested\n    if (clearActiveContext) {\n      try {\n        await ActiveContextManager.clearActiveContext();\n        logMessage(\"INFO\", `Cleared active context`);\n      } catch (clearErr) {\n        logMessage(\"WARN\", `Failed to clear active context`, {\n          error: clearErr.message,\n        });\n        // Continue despite error\n      }\n    }\n\n    // 11. Return the finalized conversation data\n    logMessage(\n      \"INFO\",\n      `finalize_conversation_context tool completed successfully`\n    );\n\n    const responseData = {\n      message: `Conversation ${conversationId} finalized successfully with outcome: ${outcome}`,\n      status: \"success\",\n      summary,\n      purpose: conversationPurpose || \"Unknown purpose\",\n      extractedLearnings,\n      promotedPatterns,\n      relatedConversations,\n      nextSteps,\n    };\n\n    return {\n      content: [\n        {\n          type: \"text\",\n          text: JSON.stringify(responseData),\n        },\n      ],\n    };\n  } catch (error) {\n    // Log detailed error information\n    logMessage(\"ERROR\", `Error in finalize_conversation_context tool`, {\n      error: error.message,\n      stack: error.stack,\n      input: {\n        conversationId: input.conversationId,\n        outcome: input.outcome,\n      },\n    });\n\n    // Return error response\n    const errorResponse = {\n      error: true,\n      errorCode: error.code || \"FINALIZATION_FAILED\",\n      errorDetails: error.message,\n      summary: \"Failed to finalize conversation context\",\n      purpose: \"Unknown due to error\",\n      extractedLearnings: null,\n      promotedPatterns: null,\n      relatedConversations: null,\n      nextSteps: null,\n    };\n\n    return {\n      content: [\n        {\n          type: \"text\",\n          text: JSON.stringify(errorResponse),\n        },\n      ],\n    };\n  }\n}\n\n/**\n * Extracts learnings from a conversation using various analysis techniques\n *\n * @param {string} conversationId - The ID of the conversation\n * @param {Array} conversationHistory - The conversation message history\n * @returns {Promise<Object>} Extracted learnings with metadata\n * @private\n */\nasync function _extractConversationLearnings(\n  conversationId,\n  conversationHistory\n) {\n  try {\n    logMessage(\n      \"DEBUG\",\n      `Extracting learnings from conversation ${conversationId}`,\n      {\n        messageCount: conversationHistory.length,\n      }\n    );\n\n    // 1. Extract different types of learnings using LearningSystem\n    let patterns = [];\n    let bugPatterns = [];\n    let conceptualInsights = [];\n    let keyValuePairs = [];\n\n    // Extract patterns\n    try {\n      patterns = await LearningSystem.extractPatternsFromConversation(\n        conversationId\n      );\n      logMessage(\"DEBUG\", `Extracted ${patterns.length} patterns`);\n    } catch (patternErr) {\n      logMessage(\"WARN\", `Failed to extract patterns`, {\n        error: patternErr.message,\n      });\n      patterns = [];\n    }\n\n    // Extract bug patterns\n    try {\n      bugPatterns = await LearningSystem.extractBugPatterns(conversationId);\n      logMessage(\"DEBUG\", `Extracted ${bugPatterns.length} bug patterns`);\n    } catch (bugErr) {\n      logMessage(\"WARN\", `Failed to extract bug patterns`, {\n        error: bugErr.message,\n      });\n      bugPatterns = [];\n    }\n\n    // Extract key-value pairs\n    try {\n      keyValuePairs = await LearningSystem.extractKeyValuePairs(\n        conversationHistory\n      );\n      logMessage(\"DEBUG\", `Extracted ${keyValuePairs.length} key-value pairs`);\n    } catch (kvErr) {\n      logMessage(\"WARN\", `Failed to extract key-value pairs`, {\n        error: kvErr.message,\n      });\n      keyValuePairs = [];\n    }\n\n    // Extract conceptual insights using NLP analysis\n    try {\n      const userMessages = conversationHistory.filter(\n        (msg) => msg.role === \"user\"\n      );\n      const assistantMessages = conversationHistory.filter(\n        (msg) => msg.role === \"assistant\"\n      );\n\n      // Process only if there are messages\n      if (userMessages.length > 0 && assistantMessages.length > 0) {\n        // Extract key concepts\n        conceptualInsights = await _extractConcepts(\n          userMessages,\n          assistantMessages\n        );\n        logMessage(\n          \"DEBUG\",\n          `Extracted ${conceptualInsights.length} conceptual insights`\n        );\n      }\n    } catch (conceptErr) {\n      logMessage(\"WARN\", `Failed to extract conceptual insights`, {\n        error: conceptErr.message,\n      });\n      conceptualInsights = [];\n    }\n\n    // 2. Store extracted learnings in the database for future reference\n    try {\n      // Store patterns\n      for (const pattern of patterns) {\n        await LearningSystem.storePattern({\n          patternType: pattern.type,\n          patternContent: pattern.content,\n          context: pattern.context,\n          confidenceScore: pattern.confidence,\n          conversationId,\n          timestamp: Date.now(),\n        });\n      }\n\n      // Store bug patterns\n      for (const bug of bugPatterns) {\n        await LearningSystem.storeBugPattern({\n          symptom: bug.symptom,\n          cause: bug.cause,\n          solution: bug.solution,\n          context: bug.context,\n          confidenceScore: bug.confidence,\n          conversationId,\n          timestamp: Date.now(),\n        });\n      }\n\n      // Store key-value pairs\n      for (const kv of keyValuePairs) {\n        await LearningSystem.storeKeyValuePair({\n          key: kv.key,\n          value: kv.value,\n          context: kv.context,\n          confidenceScore: kv.confidence,\n          conversationId,\n          timestamp: Date.now(),\n        });\n      }\n\n      logMessage(\"INFO\", `Stored extracted learnings in database`);\n    } catch (storeErr) {\n      logMessage(\"WARN\", `Failed to store some extracted learnings`, {\n        error: storeErr.message,\n      });\n      // Continue despite storage issues\n    }\n\n    // 3. Record learning_extraction event in timeline\n    try {\n      await TimelineManagerLogic.recordEvent(\n        \"learning_extraction\",\n        {\n          patterns: patterns.length,\n          bugPatterns: bugPatterns.length,\n          keyValuePairs: keyValuePairs.length,\n          conceptualInsights: conceptualInsights.length,\n          timestamp: Date.now(),\n        },\n        [], // No specific entities\n        conversationId\n      );\n      logMessage(\"DEBUG\", `Recorded learning_extraction event in timeline`);\n    } catch (timelineErr) {\n      logMessage(\"WARN\", `Failed to record learning_extraction event`, {\n        error: timelineErr.message,\n      });\n      // Continue despite timeline error\n    }\n\n    // 4. Return the extracted learnings\n    return {\n      patterns,\n      bugPatterns,\n      keyValuePairs,\n      conceptualInsights,\n      extractionTime: Date.now(),\n    };\n  } catch (error) {\n    logMessage(\"ERROR\", `Error extracting conversation learnings`, {\n      error: error.message,\n      stack: error.stack,\n      conversationId,\n    });\n    throw error;\n  }\n}\n\n/**\n * Extract conceptual insights from message content\n *\n * @param {Array} userMessages - User messages from the conversation\n * @param {Array} assistantMessages - Assistant messages from the conversation\n * @returns {Promise<Array>} Extracted conceptual insights\n * @private\n */\nasync function _extractConcepts(userMessages, assistantMessages) {\n  try {\n    logMessage(\n      \"DEBUG\",\n      `Extracting concepts from ${userMessages.length} user messages and ${assistantMessages.length} assistant messages`\n    );\n\n    // Combine message content for processing\n    const userContent = userMessages.map((msg) => msg.content).join(\"\\n\");\n    const assistantContent = assistantMessages\n      .map((msg) => msg.content)\n      .join(\"\\n\");\n\n    // Tokenize content to extract key terms\n    const userTokens = TextTokenizerLogic.tokenize(userContent);\n    const assistantTokens = TextTokenizerLogic.tokenize(assistantContent);\n\n    // Get top terms by frequency\n    const userTerms = _getTopTermsByFrequency(userTokens, 20);\n    const assistantTerms = _getTopTermsByFrequency(assistantTokens, 20);\n\n    // Find common terms that appear in both user and assistant messages\n    const commonTerms = userTerms.filter((term) =>\n      assistantTerms.some((aterm) => aterm.term === term.term)\n    );\n\n    // Extract domain-specific insights\n    const domainInsights = commonTerms.map((term) => {\n      // Find relevant snippets containing this term\n      const snippets = _findRelevantSnippets(\n        [...userMessages, ...assistantMessages],\n        term.term\n      );\n\n      return {\n        concept: term.term,\n        frequency: term.frequency,\n        importance: term.frequency / userTokens.length, // Simple importance heuristic\n        relatedTerms: assistantTerms\n          .filter(\n            (aterm) =>\n              _areTermsRelated(term.term, aterm.term) &&\n              aterm.term !== term.term\n          )\n          .map((aterm) => aterm.term)\n          .slice(0, 5),\n        snippets: snippets.slice(0, 3), // Limit to 3 snippets\n      };\n    });\n\n    logMessage(\"DEBUG\", `Extracted ${domainInsights.length} domain insights`);\n    return domainInsights;\n  } catch (error) {\n    logMessage(\"ERROR\", `Error extracting concepts`, {\n      error: error.message,\n    });\n    throw error;\n  }\n}\n\n/**\n * Promotes patterns from a conversation to the global pattern repository\n *\n * @param {string} conversationId - The ID of the conversation\n * @param {string} outcome - The outcome of the conversation\n * @returns {Promise<Object>} Promoted patterns data\n * @private\n */\nasync function _promoteConversationPatterns(conversationId, outcome) {\n  try {\n    console.log(\n      `[_promoteConversationPatterns] Promoting patterns for conversation ${conversationId}`\n    );\n\n    // 1. Extract patterns from the conversation\n    const patterns = await LearningSystem.extractPatternsFromConversation(\n      conversationId\n    );\n\n    if (!patterns || patterns.length === 0) {\n      console.log(\n        `[_promoteConversationPatterns] No patterns found in conversation ${conversationId}`\n      );\n      return {\n        promoted: 0,\n        patterns: [],\n      };\n    }\n\n    console.log(\n      `[_promoteConversationPatterns] Found ${patterns.length} patterns to evaluate for promotion`\n    );\n\n    // 2. Prepare data for tracking promotion results\n    const promotedPatterns = {\n      promoted: 0,\n      patterns: [],\n    };\n\n    // Set minimum confidence threshold based on outcome\n    let minConfidence = 0.5; // Default threshold\n    if (outcome === \"completed\") minConfidence = 0.6;\n    if (outcome === \"abandoned\") minConfidence = 0.7; // Higher threshold for abandoned conversations\n\n    // 3. Process each pattern for potential promotion\n    for (const pattern of patterns) {\n      try {\n        // Skip patterns that are already global\n        if (pattern.is_global) {\n          promotedPatterns.patterns.push({\n            patternId: pattern.pattern_id,\n            name: pattern.name,\n            type: pattern.pattern_type,\n            promoted: false,\n            confidence: pattern.confidence_score,\n          });\n          continue;\n        }\n\n        // Skip patterns with confidence below threshold\n        if (pattern.confidence_score < minConfidence) {\n          promotedPatterns.patterns.push({\n            patternId: pattern.pattern_id,\n            name: pattern.name,\n            type: pattern.pattern_type,\n            promoted: false,\n            confidence: pattern.confidence_score,\n          });\n          continue;\n        }\n\n        // Promote pattern to global repository\n        await GlobalPatternRepository.promotePatternToGlobal(\n          pattern.pattern_id,\n          pattern.confidence_score\n        );\n\n        // Reinforce the pattern based on conversation outcome\n        const observationType =\n          outcome === \"completed\" || outcome === \"reference_only\"\n            ? \"confirmation\"\n            : \"usage\";\n\n        await GlobalPatternRepository.reinforcePattern(\n          pattern.pattern_id,\n          observationType,\n          { conversationId }\n        );\n\n        // Record successful promotion\n        promotedPatterns.promoted++;\n        promotedPatterns.patterns.push({\n          patternId: pattern.pattern_id,\n          name: pattern.name,\n          type: pattern.pattern_type,\n          promoted: true,\n          confidence: pattern.confidence_score,\n        });\n\n        console.log(\n          `[_promoteConversationPatterns] Successfully promoted pattern ${pattern.pattern_id}`\n        );\n      } catch (error) {\n        console.warn(\n          `[_promoteConversationPatterns] Error processing pattern ${pattern.pattern_id}:`,\n          error\n        );\n        // Continue with next pattern\n      }\n    }\n\n    console.log(\n      `[_promoteConversationPatterns] Promoted ${promotedPatterns.promoted} patterns to global repository`\n    );\n    return promotedPatterns;\n  } catch (error) {\n    console.error(\n      `[_promoteConversationPatterns] Error promoting patterns:`,\n      error\n    );\n    return {\n      promoted: 0,\n      patterns: [],\n      error: error.message,\n    };\n  }\n}\n\n/**\n * Finds and synthesizes insights from related conversations\n *\n * @param {string} conversationId - The ID of the current conversation\n * @param {Array} conversationTopics - Topics from the current conversation\n * @param {string} conversationPurpose - Purpose of the current conversation\n * @returns {Promise<Object>} Related conversations data with synthesized insights\n * @private\n */\nasync function _findAndSynthesizeRelatedConversations(\n  conversationId,\n  conversationTopics,\n  conversationPurpose\n) {\n  try {\n    console.log(\n      `[_findAndSynthesizeRelatedConversations] Finding related conversations for ${conversationId}`\n    );\n\n    // 1. Extract keywords from conversation topics\n    const topicKeywords = new Set();\n    conversationTopics.forEach((topic) => {\n      if (topic.keywords && Array.isArray(topic.keywords)) {\n        topic.keywords.forEach((kw) => topicKeywords.add(kw));\n      }\n    });\n\n    const keywordArray = Array.from(topicKeywords);\n\n    // 2. Get recent conversation events from timeline (excluding current conversation)\n    const recentConversationEvents = await TimelineManagerLogic.getEvents({\n      types: [\"conversation_end\", \"conversation_completed\"],\n      limit: 10,\n      excludeConversationId: conversationId,\n    });\n\n    if (!recentConversationEvents || recentConversationEvents.length === 0) {\n      console.log(\n        `[_findAndSynthesizeRelatedConversations] No recent conversations found to compare`\n      );\n      return {\n        relatedCount: 0,\n        conversations: [],\n        synthesizedInsights: [],\n      };\n    }\n\n    // 3. Score conversations by relevance\n    const scoredConversations = [];\n\n    for (const event of recentConversationEvents) {\n      try {\n        if (!event.data || !event.conversation_id) continue;\n\n        // Get conversation topics for comparison\n        const eventTopics =\n          await ConversationIntelligence.getConversationTopics(\n            event.conversation_id\n          );\n\n        // Extract keywords from event topics\n        const eventKeywords = new Set();\n        eventTopics.forEach((topic) => {\n          if (topic.keywords && Array.isArray(topic.keywords)) {\n            topic.keywords.forEach((kw) => eventKeywords.add(kw));\n          }\n        });\n\n        // Calculate keyword overlap (Jaccard similarity)\n        const overlapCount = keywordArray.filter((kw) =>\n          eventKeywords.has(kw)\n        ).length;\n        const totalUniqueKeywords = new Set([...keywordArray, ...eventKeywords])\n          .size;\n\n        const similarityScore =\n          totalUniqueKeywords > 0 ? overlapCount / totalUniqueKeywords : 0;\n\n        // Find common topics by name\n        const commonTopics = [];\n        eventTopics.forEach((eventTopic) => {\n          conversationTopics.forEach((currentTopic) => {\n            if (\n              eventTopic.topic_name &&\n              currentTopic.topic_name &&\n              eventTopic.topic_name.toLowerCase() ===\n                currentTopic.topic_name.toLowerCase()\n            ) {\n              commonTopics.push(eventTopic.topic_name);\n            }\n          });\n        });\n\n        // Only consider conversations with some similarity\n        if (similarityScore > 0.2 || commonTopics.length > 0) {\n          scoredConversations.push({\n            conversationId: event.conversation_id,\n            summary: event.data.summary || \"No summary available\",\n            timestamp: event.timestamp,\n            similarityScore,\n            commonTopics,\n          });\n        }\n      } catch (error) {\n        console.warn(\n          `[_findAndSynthesizeRelatedConversations] Error processing event ${event.event_id}:`,\n          error\n        );\n        // Continue with next event\n      }\n    }\n\n    // Sort by similarity score descending\n    scoredConversations.sort((a, b) => b.similarityScore - a.similarityScore);\n\n    // Limit to top 5 most similar\n    const relatedConversations = scoredConversations.slice(0, 5);\n\n    console.log(\n      `[_findAndSynthesizeRelatedConversations] Found ${relatedConversations.length} related conversations`\n    );\n\n    // 4. Synthesize insights from related conversations\n    const synthesizedInsights =\n      await _synthesizeInsightsFromRelatedConversations(\n        relatedConversations,\n        conversationPurpose\n      );\n\n    return {\n      relatedCount: relatedConversations.length,\n      conversations: relatedConversations,\n      synthesizedInsights,\n    };\n  } catch (error) {\n    console.error(\n      `[_findAndSynthesizeRelatedConversations] Error finding related conversations:`,\n      error\n    );\n    return {\n      relatedCount: 0,\n      conversations: [],\n      synthesizedInsights: [],\n      error: error.message,\n    };\n  }\n}\n\n/**\n * Synthesizes insights from related conversations\n *\n * @param {Array} relatedConversations - Array of related conversation data\n * @param {string} currentPurpose - Purpose of the current conversation\n * @returns {Promise<Array>} Array of synthesized insights by topic\n * @private\n */\nasync function _synthesizeInsightsFromRelatedConversations(\n  relatedConversations,\n  currentPurpose\n) {\n  try {\n    // If no related conversations, return empty insights\n    if (!relatedConversations || relatedConversations.length === 0) {\n      return [];\n    }\n\n    // Group conversations by common topics\n    const conversationsByTopic = {};\n\n    // First, identify common topics across conversations\n    relatedConversations.forEach((conversation) => {\n      if (conversation.commonTopics && conversation.commonTopics.length > 0) {\n        conversation.commonTopics.forEach((topic) => {\n          if (!conversationsByTopic[topic]) {\n            conversationsByTopic[topic] = [];\n          }\n          conversationsByTopic[topic].push(conversation);\n        });\n      }\n    });\n\n    // If there are no common topics, create a synthetic topic based on purpose\n    if (Object.keys(conversationsByTopic).length === 0 && currentPurpose) {\n      const syntheticTopic = `Conversations about ${currentPurpose}`;\n      conversationsByTopic[syntheticTopic] = relatedConversations;\n    }\n\n    // Generate insights for each topic group\n    const insights = [];\n\n    for (const [topic, conversations] of Object.entries(conversationsByTopic)) {\n      // Only synthesize if we have enough conversations on this topic\n      if (conversations.length >= 2) {\n        // Combine summaries for synthesis\n        const combinedSummaries = conversations\n          .map((c) => c.summary)\n          .join(\" | \");\n\n        // Generate synthesized insight using ContextCompressorLogic\n        const insight = await ContextCompressorLogic.summarizeText(\n          combinedSummaries,\n          {\n            targetLength: 150,\n            preserveKeyPoints: true,\n          }\n        );\n\n        insights.push({\n          topic,\n          insight,\n          conversationCount: conversations.length,\n          sourceSummaries: conversations.map((c) => ({\n            conversationId: c.conversationId,\n            summary: c.summary,\n          })),\n        });\n      }\n    }\n\n    return insights;\n  } catch (error) {\n    console.error(\n      `[_synthesizeInsightsFromRelatedConversations] Error synthesizing insights:`,\n      error\n    );\n    return [];\n  }\n}\n\n/**\n * Generates next step suggestions based on conversation analysis\n *\n * @param {string} conversationId - The ID of the conversation\n * @param {string} purpose - The purpose of the conversation\n * @param {string} summary - The conversation summary\n * @param {Object} extractedLearnings - The extracted learnings from the conversation\n * @returns {Promise<Object>} Next steps recommendations\n * @private\n */\nasync function _generateNextStepSuggestions(\n  conversationId,\n  purpose,\n  summary,\n  extractedLearnings\n) {\n  try {\n    console.log(\n      `[_generateNextStepSuggestions] Generating next steps for conversation ${conversationId}`\n    );\n\n    // Initialize results\n    const result = {\n      suggestedNextSteps: [],\n      followUpTopics: [],\n      referenceMaterials: [],\n    };\n\n    // 1. Extract key terms from summary for searching reference materials\n    const tokens = TextTokenizerLogic.tokenize(summary);\n    const keywords = TextTokenizerLogic.extractKeywords(tokens, 10);\n\n    // Use purpose to determine likely next steps\n    let nextSteps = [];\n    let followUpTopics = [];\n\n    if (purpose) {\n      // Different next steps based on conversation purpose\n      switch (purpose.toLowerCase()) {\n        case \"debugging\":\n        case \"bug_fixing\":\n          nextSteps.push({\n            action: \"Create a test case that verifies the bug fix\",\n            priority: \"high\",\n            rationale: \"Ensure the bug doesn't reoccur in the future\",\n          });\n          nextSteps.push({\n            action: \"Document the root cause and solution\",\n            priority: \"medium\",\n            rationale: \"Help prevent similar issues in the future\",\n          });\n          break;\n\n        case \"feature_planning\":\n        case \"design_discussion\":\n          nextSteps.push({\n            action: \"Create tickets/tasks for implementation work\",\n            priority: \"high\",\n            rationale: \"Break down the feature into manageable pieces\",\n          });\n          nextSteps.push({\n            action: \"Draft initial implementation plan with milestones\",\n            priority: \"medium\",\n            rationale: \"Establish a timeline and checkpoints\",\n          });\n          break;\n\n        case \"code_review\":\n          nextSteps.push({\n            action: \"Address feedback points and resubmit for review\",\n            priority: \"high\",\n            rationale: \"Incorporate the suggested improvements\",\n          });\n          nextSteps.push({\n            action: \"Update documentation to reflect changes\",\n            priority: \"medium\",\n            rationale: \"Keep documentation in sync with code\",\n          });\n          break;\n\n        case \"onboarding\":\n        case \"knowledge_sharing\":\n          nextSteps.push({\n            action: \"Create summary documentation of discussed topics\",\n            priority: \"high\",\n            rationale: \"Solidify knowledge transfer\",\n          });\n          nextSteps.push({\n            action: \"Schedule follow-up session for additional questions\",\n            priority: \"medium\",\n            rationale: \"Address remaining questions after initial processing\",\n          });\n          break;\n\n        default:\n          // Generic next steps\n          nextSteps.push({\n            action: \"Document key decisions from the conversation\",\n            priority: \"medium\",\n            rationale: \"Preserve important context for future reference\",\n          });\n      }\n    }\n\n    // 2. Add follow-up topics based on extracted learnings\n    if (extractedLearnings && extractedLearnings.learnings) {\n      // Find design decisions that may need follow-up\n      const designDecisions = extractedLearnings.learnings.filter(\n        (l) => l.type === \"design_decision\"\n      );\n\n      if (designDecisions.length > 0) {\n        const highConfidenceDecisions = designDecisions\n          .filter((d) => d.confidence >= 0.7)\n          .slice(0, 2);\n\n        highConfidenceDecisions.forEach((decision) => {\n          followUpTopics.push({\n            topic: `Implementation details for: ${decision.content}`,\n            priority: \"high\",\n            rationale: \"Turn design decision into concrete implementation\",\n          });\n        });\n      }\n\n      // Find bug patterns that may need follow-up\n      const bugPatterns = extractedLearnings.learnings.filter(\n        (l) => l.type === \"bug_pattern\"\n      );\n\n      if (bugPatterns.length > 0) {\n        const criticalBugs = bugPatterns\n          .filter((b) => b.confidence >= 0.8)\n          .slice(0, 2);\n\n        criticalBugs.forEach((bug) => {\n          followUpTopics.push({\n            topic: `Root cause analysis for: ${bug.content}`,\n            priority: \"medium\",\n            rationale: \"Prevent similar bugs in the future\",\n          });\n        });\n      }\n    }\n\n    // 3. Search for reference materials based on keywords\n    const referenceResults = await SmartSearchServiceLogic.searchByKeywords(\n      keywords,\n      {\n        fileTypes: [\"md\", \"txt\", \"rst\", \"pdf\", \"doc\"],\n        maxResults: 5,\n        searchDocumentation: true,\n      }\n    );\n\n    const referenceMaterials = referenceResults.map((result) => ({\n      title: result.name || result.file_path || \"Unnamed reference\",\n      path: result.file_path,\n      type: result.entity_type || \"document\",\n      relevance: result.score || 0.5,\n    }));\n\n    // 4. Combine all results\n    result.suggestedNextSteps = nextSteps;\n    result.followUpTopics = followUpTopics;\n    result.referenceMaterials = referenceMaterials;\n\n    console.log(\n      `[_generateNextStepSuggestions] Generated ${nextSteps.length} next steps and ${followUpTopics.length} follow-up topics`\n    );\n\n    return result;\n  } catch (error) {\n    console.error(\n      `[_generateNextStepSuggestions] Error generating next steps:`,\n      error\n    );\n    return {\n      suggestedNextSteps: [],\n      followUpTopics: [],\n      referenceMaterials: [],\n      error: error.message,\n    };\n  }\n}\n\n// Export the tool definition for server registration\nexport default {\n  name: \"finalize_conversation_context\",\n  description:\n    \"Finalizes a conversation context, extracting learnings, promoting patterns, and generating insights\",\n  inputSchema: finalizeConversationContextInputSchema,\n  outputSchema: finalizeConversationContextOutputSchema,\n  handler,\n};\n", "/**\n * tools/index.js\n *\n * Aggregates and exports all MCP tool definitions for registration with the MCP server.\n */\n\nimport initializeConversationContextTool from \"./initializeConversationContext.tool.js\";\nimport updateConversationContextTool from \"./updateConversationContext.tool.js\";\nimport retrieveRelevantContextTool from \"./retrieveRelevantContext.tool.js\";\nimport recordMilestoneContextTool from \"./recordMilestoneContext.tool.js\";\nimport finalizeConversationContextTool from \"./finalizeConversationContext.tool.js\";\n\nconst allTools = [\n  initializeConversationContextTool,\n  updateConversationContextTool,\n  retrieveRelevantContextTool,\n  recordMilestoneContextTool,\n  finalizeConversationContextTool,\n];\n\nexport default allTools;\n", "/**\n * mcpDevContextTools.js\n *\n * Provides wrapper functions for the MCP DevContext tools\n * to ensure proper callback handling and compatibility with MCP SDK.\n */\n\nimport { logMessage } from \"../utils/logger.js\";\n\n// Store conversation ID globally for this session\nif (typeof global.lastConversationId === \"undefined\") {\n  global.lastConversationId = null;\n}\n\n/**\n * Creates a wrapped handler for DevContext MCP tools that follows the MCP SDK pattern\n *\n * @param {Function} handler - The original tool handler function\n * @param {string} toolName - The name of the tool for logging purposes\n * @returns {Function} A wrapped handler function compatible with MCP SDK\n */\nexport function createToolHandler(handler, toolName) {\n  return async (params, context) => {\n    try {\n      logMessage(\"DEBUG\", `${toolName} tool handler invoked`, {\n        paramsKeys: Object.keys(params),\n      });\n\n      // Handle weird parameter structure with signal property\n      let actualParams = params;\n      if (\n        params &&\n        typeof params === \"object\" &&\n        Object.keys(params).length === 1 &&\n        params.signal &&\n        Object.keys(params.signal).length === 0\n      ) {\n        // If we're just getting a signal object with no real params, use defaults\n        actualParams = {};\n        logMessage(\n          \"WARN\",\n          `${toolName} received only signal object, using defaults`,\n          { params }\n        );\n      } else if (params && params.signal && Object.keys(params).length > 1) {\n        // If params contains signal plus other properties, extract just the other properties\n        const { signal, ...otherParams } = params;\n        actualParams = otherParams;\n        logMessage(\n          \"DEBUG\",\n          `${toolName} extracted parameters from signal object`,\n          {\n            extractedParams: Object.keys(actualParams),\n          }\n        );\n      }\n\n      // Extract additional parameters from any special formats\n      const extractedParams = extractParamsFromInput(actualParams);\n\n      // Generate default parameters for this specific tool\n      const defaultParams = createDefaultParamsForTool(toolName);\n\n      // Merge extracted parameters with defaults, prioritizing user-provided values\n      const mergedParams = { ...defaultParams, ...extractedParams };\n\n      // If conversation ID was provided, store it for future use\n      if (mergedParams.conversationId) {\n        global.lastConversationId = mergedParams.conversationId;\n      }\n\n      // Now call the handler with the properly merged parameters\n      const result = await handler(mergedParams, context);\n\n      return {\n        content: [\n          {\n            type: \"text\",\n            text: typeof result === \"string\" ? result : JSON.stringify(result),\n          },\n        ],\n      };\n    } catch (error) {\n      logMessage(\"ERROR\", `Error in ${toolName} tool handler`, {\n        error: error.message,\n        stack: error.stack,\n      });\n\n      return {\n        content: [\n          {\n            type: \"text\",\n            text: JSON.stringify({\n              error: true,\n              message: error.message,\n              details: error.stack,\n            }),\n          },\n        ],\n      };\n    }\n  };\n}\n\n/**\n * Extracts parameters from various input formats\n *\n * @param {any} input - The input parameters (could be object, string, etc.)\n * @returns {Object} Extracted parameters\n */\nfunction extractParamsFromInput(input) {\n  const extractedParams = {};\n\n  try {\n    // Case 1: Input is already an object\n    if (input && typeof input === \"object\") {\n      // Copy all properties except signal and requestId\n      Object.keys(input).forEach((key) => {\n        if (key !== \"signal\" && key !== \"requestId\") {\n          extractedParams[key] = input[key];\n        }\n      });\n\n      // Special case: If there's a random_string property, try to parse it\n      if (input.random_string) {\n        try {\n          // Try to parse as JSON\n          const parsedJson = JSON.parse(input.random_string);\n          Object.assign(extractedParams, parsedJson);\n        } catch (e) {\n          // If not JSON, use as-is if it looks like a conversationId\n          if (\n            typeof input.random_string === \"string\" &&\n            input.random_string.length > 30 &&\n            input.random_string.includes(\"-\")\n          ) {\n            extractedParams.conversationId = input.random_string;\n          }\n        }\n      }\n\n      // Special case: Direct parameter formats for specific tools\n      if (input.initialQuery) {\n        extractedParams.initialQuery = input.initialQuery;\n      }\n\n      if (input.contextDepth) {\n        extractedParams.contextDepth = input.contextDepth;\n      }\n\n      if (input.query) {\n        extractedParams.query = input.query;\n      }\n\n      if (input.name) {\n        extractedParams.name = input.name;\n      }\n    }\n    // Case 2: Input is a string\n    else if (typeof input === \"string\") {\n      try {\n        // Try to parse as JSON\n        const parsedJson = JSON.parse(input);\n        Object.assign(extractedParams, parsedJson);\n      } catch (e) {\n        // If not JSON, use as conversationId if it looks like one\n        if (input.length > 30 && input.includes(\"-\")) {\n          extractedParams.conversationId = input;\n        }\n      }\n    }\n  } catch (e) {\n    logMessage(\"ERROR\", `Error extracting params: ${e.message}`);\n  }\n\n  return extractedParams;\n}\n\n/**\n * Creates default parameters for each tool type\n *\n * @param {string} toolName - The name of the tool\n * @returns {Object} Default parameters for the tool\n */\nfunction createDefaultParamsForTool(toolName) {\n  switch (toolName) {\n    case \"initialize_conversation_context\":\n      return {\n        initialQuery: \"Starting a new conversation with DevContext\",\n        includeArchitecture: true,\n        includeRecentConversations: true,\n        maxCodeContextItems: 5,\n        maxRecentChanges: 5,\n        contextDepth: \"standard\",\n      };\n    case \"update_conversation_context\":\n      return {\n        conversationId: global.lastConversationId,\n        newMessages: [\n          {\n            role: \"user\",\n            content: \"Working with DevContext tools\",\n          },\n        ],\n        preserveContextOnTopicShift: true,\n        contextIntegrationLevel: \"balanced\",\n        trackIntentTransitions: true,\n      };\n    case \"retrieve_relevant_context\":\n      return {\n        conversationId: global.lastConversationId,\n        query: \"DevContext tools and functionality\",\n        constraints: {\n          includeConversation: true,\n          crossTopicSearch: false,\n        },\n        contextFilters: {\n          minRelevanceScore: 0.3,\n        },\n        weightingStrategy: \"balanced\",\n        balanceStrategy: \"proportional\",\n        contextBalance: \"auto\",\n      };\n    case \"record_milestone_context\":\n      return {\n        conversationId: global.lastConversationId,\n        name: \"DevContext Tool Milestone\",\n        description: \"Milestone recorded during DevContext tools testing\",\n        milestoneCategory: \"uncategorized\",\n        assessImpact: true,\n      };\n    case \"finalize_conversation_context\":\n      return {\n        conversationId: global.lastConversationId,\n        clearActiveContext: false,\n        extractLearnings: true,\n        promotePatterns: true,\n        synthesizeRelatedTopics: true,\n        generateNextSteps: true,\n        outcome: \"completed\",\n      };\n    default:\n      return {};\n  }\n}\n\n/**\n * Creates a specialized wrapped handler for initialize_conversation_context\n *\n * @param {Function} handler - The original handler function\n * @returns {Function} A wrapped handler function\n */\nexport function createInitializeContextHandler(handler) {\n  return createToolHandler(handler, \"initialize_conversation_context\");\n}\n\n/**\n * Creates a specialized wrapped handler for finalize_conversation_context\n *\n * @param {Function} handler - The original handler function\n * @returns {Function} A wrapped handler function\n */\nexport function createFinalizeContextHandler(handler) {\n  return createToolHandler(handler, \"finalize_conversation_context\");\n}\n"],
  "mappings": ";;;;;;;;AAIA,OAAO,YAAY;AAJnB,IAUa,oBACA,kBAGA,WACA,oBAGA,sBAIA,oBAGA;AAzBb;AAAA;AAOA,WAAO,OAAO;AAGP,IAAM,qBAAqB,QAAQ,IAAI;AACvC,IAAM,mBAAmB,QAAQ,IAAI;AAGrC,IAAM,YAAY,QAAQ,IAAI,aAAa;AAC3C,IAAM,qBAAqB,QAAQ,IAAI,uBAAuB;AAG9D,IAAM,uBAAuB;AAAA,MAClC,QAAQ,IAAI,wBAAwB;AAAA,MACpC;AAAA,IACF;AACO,IAAM,qBAAqB;AAAA,MAChC,QAAQ,IAAI,sBAAsB;AAAA,IACpC;AACO,IAAM,iBAAiB;AAAA,MAC5B,QAAQ,IAAI,kBAAkB;AAAA,MAC9B;AAAA,IACF;AAAA;AAAA;;;AC5BA,IAQM,YAaO;AArBb;AAAA;AAKA;AAGA,IAAM,aAAa;AAAA,MACjB,OAAO;AAAA,MACP,MAAM;AAAA,MACN,MAAM;AAAA,MACN,OAAO;AAAA,IACT;AAQO,IAAM,aAAa,CAAC,OAAO,SAAS,OAAO,SAAS;AAEzD,YAAM,aAAa,MAAM,YAAY;AAGrC,UACE,CAAC,WAAW,eAAe,UAAU,KACrC,WAAW,UAAU,IAAI,WAAW,SAAS,GAC7C;AACA;AAAA,MACF;AAGA,YAAM,aAAY,oBAAI,KAAK,GAAE,YAAY;AAGzC,UAAI,YAAY,IAAI,SAAS,MAAM,UAAU,MAAM,OAAO;AAC1D,UAAI,MAAM;AACR,cAAM,aAAa,OAAO,SAAS,WAAW,OAAO,KAAK,UAAU,IAAI;AACxE,qBAAa,MAAM,UAAU;AAAA,MAC/B;AAGA,UAAI,eAAe,WAAW,eAAe,QAAQ;AACnD,gBAAQ,IAAI,SAAS;AAAA,MACvB,OAAO;AACL,gBAAQ,MAAM,SAAS;AAAA,MACzB;AAAA,IAKF;AAAA;AAAA;;;AChDA,SAAS,oBAAoB;AAyG7B,eAAe,aAAa,WAAW,YAAY;AACjD,MAAI;AACF,QAAI,CAAC,aAAa,CAAC,YAAY;AAC7B,iBAAW,SAAS,uCAAuC;AAC3D,aAAO;AAAA,IACT;AAEA,UAAM,SAAS,YAAY;AAC3B,UAAM,SAAS,MAAM,OAAO,QAAQ;AAAA,MAClC,KAAK,qBAAqB,SAAS;AAAA,IACrC,CAAC;AAGD,QAAI,CAAC,UAAU,CAAC,OAAO,QAAQ,OAAO,KAAK,WAAW,GAAG;AACvD,iBAAW,QAAQ,2BAA2B,SAAS,EAAE;AACzD,aAAO;AAAA,IACT;AAGA,eAAW,OAAO,OAAO,MAAM;AAC7B,UAAI,OAAO,IAAI,SAAS,YAAY;AAClC,eAAO;AAAA,MACT;AAAA,IACF;AAEA,eAAW,SAAS,UAAU,UAAU,iBAAiB,SAAS,EAAE;AACpE,WAAO;AAAA,EACT,SAAS,OAAO;AACd,eAAW,SAAS,oCAAoC,MAAM,OAAO,EAAE;AACvE,WAAO;AAAA,EACT;AACF;AAMA,eAAe,8BAA8B;AAC3C,MAAI;AAEF,UAAM,mBAAmB,MAAM,aAAa;AAAA;AAAA;AAAA,KAG3C;AAED,UAAM,cACJ,oBACA,iBAAiB,QACjB,iBAAiB,KAAK,SAAS;AAEjC,QAAI,CAAC,aAAa;AAChB;AAAA,QACE;AAAA,QACA;AAAA,MACF;AACA;AAAA,IACF;AAGA,UAAM,oBAAoB,MAAM;AAAA,MAC9B;AAAA,MACA;AAAA,IACF;AAEA,QAAI,CAAC,mBAAmB;AACtB,iBAAW,QAAQ,kDAAkD;AAErE,UAAI;AAEF,cAAM;AAAA,UACJ;AAAA,QACF;AAEA;AAAA,UACE;AAAA,UACA;AAAA,QACF;AAAA,MACF,SAAS,YAAY;AAEnB,YAAI,WAAW,QAAQ,SAAS,kBAAkB,GAAG;AACnD,qBAAW,QAAQ,0CAA0C;AAAA,QAC/D,OAAO;AAEL,gBAAM;AAAA,QACR;AAAA,MACF;AAGA,UAAI;AACF,cAAM;AAAA,UACJ;AAAA,QACF;AACA,mBAAW,QAAQ,mCAAmC;AAAA,MACxD,SAAS,YAAY;AACnB,mBAAW,QAAQ,yBAAyB,WAAW,OAAO,EAAE;AAAA,MAClE;AAAA,IACF,OAAO;AACL;AAAA,QACE;AAAA,QACA;AAAA,MACF;AAAA,IACF;AAAA,EACF,SAAS,OAAO;AACd,UAAM,IAAI,MAAM,qBAAqB,MAAM,OAAO,EAAE;AAAA,EACtD;AACF;AAvNA,IAUI,UAOS,aA6BA,kBAiBA,cAgKA;AA/Nb;AAAA;AAMA;AACA;AAGA,IAAI,WAAW;AAOR,IAAM,cAAc,MAAM;AAC/B,UAAI,UAAU;AACZ,eAAO;AAAA,MACT;AAEA,UAAI,CAAC,oBAAoB;AACvB,cAAM,IAAI;AAAA,UACR;AAAA,QACF;AAAA,MACF;AAEA,UAAI,CAAC,kBAAkB;AACrB,cAAM,IAAI,MAAM,0DAA0D;AAAA,MAC5E;AAEA,iBAAW,aAAa;AAAA,QACtB,KAAK;AAAA,QACL,WAAW;AAAA,MACb,CAAC;AAED,aAAO;AAAA,IACT;AAQO,IAAM,mBAAmB,OAAO,SAAS,SAAS;AACvD,UAAI;AACF,cAAMA,YAAW,UAAU,YAAY;AACvC,cAAMA,UAAS,QAAQ,UAAU;AACjC,eAAO;AAAA,MACT,SAAS,OAAO;AACd,cAAM,IAAI,MAAM,oCAAoC,MAAM,OAAO,EAAE;AAAA,MACrE;AAAA,IACF;AASO,IAAM,eAAe,OAAO,UAAU,OAAO,CAAC,MAAM;AACzD,UAAI;AAEF,gBAAQ,IAAI,yBAAyB;AAAA,UACnC,KAAK,SAAS,UAAU,GAAG,GAAG,KAAK,SAAS,SAAS,MAAM,QAAQ;AAAA,UACnE,MACE,KAAK,SAAS,IACV,KAAK,UAAU,KAAK,MAAM,GAAG,CAAC,CAAC,KAAK,KAAK,SAAS,IAAI,QAAQ,MAC9D;AAAA,QACR,CAAC;AAED,cAAM,SAAS,YAAY;AAC3B,cAAM,SAAS,MAAM,OAAO,QAAQ;AAAA,UAClC,KAAK;AAAA,UACL;AAAA,QACF,CAAC;AAGD,gBAAQ,IAAI,sBAAsB;AAAA,UAChC,UAAU,OAAO,MAAM,UAAU;AAAA,UACjC,aACE,OAAO,MAAM,SAAS,IAClB,KAAK,UAAU,OAAO,KAAK,CAAC,CAAC,EAAE,UAAU,GAAG,GAAG,IAAI,QACnD;AAAA,UACN,cAAc,OAAO;AAAA,QACvB,CAAC;AAED,eAAO;AAAA,MACT,SAAS,OAAO;AACd,gBAAQ,MAAM,qBAAqB;AAAA,UACjC,SAAS,MAAM;AAAA,UACf,OAAO,SAAS,UAAU,GAAG,GAAG;AAAA,UAChC,MAAM,KAAK,SAAS,IAAI,KAAK,UAAU,KAAK,MAAM,GAAG,CAAC,CAAC,IAAI;AAAA,QAC7D,CAAC;AAED,cAAM,IAAI;AAAA,UACR,2BAA2B,MAAM,OAAO;AAAA,SAAY,QAAQ;AAAA,QAC9D;AAAA,MACF;AAAA,IACF;AAyHO,IAAM,2BAA2B,YAAY;AAClD,UAAI;AACF,cAAM,SAAS,YAAY;AAC3B,YAAI,UAAU;AAGd,YAAI;AACF,gBAAM,4BAA4B;AAAA,QACpC,SAAS,gBAAgB;AACvB,qBAAW,QAAQ,sBAAsB,eAAe,OAAO,EAAE;AAAA,QAEnE;AAGA,cAAM,mBAAmB;AAAA;AAAA,UAEvB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,UAoBA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA;AAAA,UAGA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,UAWA;AAAA,UACA;AAAA,UACA;AAAA;AAAA,UAGA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,UAOA;AAAA;AAAA;AAAA;AAAA,UAKA;AAAA;AAAA;AAAA,UAIA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,UAQA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,UAaA;AAAA,UACA;AAAA,UACA;AAAA;AAAA,UAGA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,UAeA;AAAA,UACA;AAAA;AAAA,UAGA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,UAkBA;AAAA,UACA;AAAA;AAAA,UAGA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,UAWA;AAAA,UACA;AAAA;AAAA,UAGA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,UAUA;AAAA,UACA;AAAA;AAAA,UAGA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,UAWA;AAAA;AAAA,UAGA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,UAYA;AAAA;AAAA,UAGA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,UAoBA;AAAA,UACA;AAAA,UACA;AAAA;AAAA,UAGA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,UAYA;AAAA,UACA;AAAA;AAAA,UAGA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,UASA;AAAA,QACF;AAGA,mBAAW,aAAa,kBAAkB;AACxC,cAAI;AACF,kBAAM,OAAO,QAAQ,SAAS;AAAA,UAChC,SAAS,OAAO;AAEd;AAAA,cACE;AAAA,cACA,qCAAqC,MAAM,OAAO;AAAA,YACpD;AACA;AAAA,cACE;AAAA,cACA,qBAAqB,UAAU,UAAU,GAAG,GAAG,CAAC;AAAA,YAClD;AACA,sBAAU;AAAA,UACZ;AAAA,QACF;AAEA,eAAO;AAAA,MACT,SAAS,OAAO;AACd,cAAM,IAAI,MAAM,0CAA0C,MAAM,OAAO,EAAE;AAAA,MAC3E;AAAA,IACF;AAAA;AAAA;;;AC5eA,SAAS,MAAM,cAAc;AAa7B,eAAsB,gBACpB,gBACA,gBACA,kBACA,SAAS,GACT,WAAW,CAAC,GACZ;AAEA,MAAI,CAAC,kBAAkB,CAAC,kBAAkB,CAAC,kBAAkB;AAC3D,UAAM,IAAI;AAAA,MACR;AAAA,IACF;AAAA,EACF;AAGA,QAAM,iBAAiB,OAAO;AAG9B,QAAM,eAAe,KAAK,UAAU,QAAQ;AAE5C,MAAI;AAEF,UAAM,QAAQ;AAAA;AAAA;AAAA;AAAA;AAMd,UAAM,aAAa,OAAO;AAAA,MACxB;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAAA,EACH,SAAS,OAAO;AAEd,QAAI,MAAM,WAAW,MAAM,QAAQ,SAAS,0BAA0B,GAAG;AAEvE,YAAM,cAAc;AAAA;AAAA;AAAA;AAAA;AAMpB,YAAM,aAAa,aAAa;AAAA,QAC9B;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MACF,CAAC;AAAA,IACH,OAAO;AAEL,cAAQ;AAAA,QACN,qCAAqC,cAAc,QAAQ,cAAc;AAAA,QACzE;AAAA,MACF;AACA,YAAM;AAAA,IACR;AAAA,EACF;AACF;AAqBA,eAAsB,iBACpB,UACA,YAAY,YACZ,QAAQ,CAAC,GACT;AAEA,MAAI,CAAC,UAAU;AACb,UAAM,IAAI,MAAM,uBAAuB;AAAA,EACzC;AAGA,MAAI,CAAC,CAAC,YAAY,YAAY,MAAM,EAAE,SAAS,SAAS,GAAG;AACzD,UAAM,IAAI,MAAM,qDAAqD;AAAA,EACvE;AAGA,MAAI,QAAQ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAYZ,QAAM,cAAc,CAAC;AAGrB,MAAI,cAAc,YAAY;AAC5B,aAAS;AACT,gBAAY,KAAK,QAAQ;AAAA,EAC3B,WAAW,cAAc,YAAY;AACnC,aAAS;AACT,gBAAY,KAAK,QAAQ;AAAA,EAC3B,OAAO;AAEL,aAAS;AACT,gBAAY,KAAK,UAAU,QAAQ;AAAA,EACrC;AAGA,MAAI,MAAM,SAAS,GAAG;AAEpB,UAAM,mBAAmB,MAAM,IAAI,MAAM,GAAG,EAAE,KAAK,IAAI;AACvD,aAAS,8BAA8B,gBAAgB;AACvD,gBAAY,KAAK,GAAG,KAAK;AAAA,EAC3B;AAEA,MAAI;AAEF,UAAM,gBAAgB,MAAM,aAAa,OAAO,WAAW;AAG3D,WAAO,cAAc,IAAI,CAAC,kBAAkB;AAAA,MAC1C,GAAG;AAAA;AAAA,MAEH,UAAU,aAAa,WAAW,KAAK,MAAM,aAAa,QAAQ,IAAI,CAAC;AAAA,IACzE,EAAE;AAAA,EACJ,SAAS,OAAO;AACd,YAAQ,MAAM,0CAA0C,QAAQ,KAAK,KAAK;AAC1E,UAAM;AAAA,EACR;AACF;AAvKA;AAAA;AAOA;AAAA;AAAA;;;ACIA;AACA;AAKA;AARA,SAAS,iBAAiB;AAC1B,SAAS,4BAA4B;;;ACDrC;AAFA,SAAS,KAAAC,UAAS;AAClB,SAAS,MAAMC,eAAc;;;ACD7B;AACA,SAAS,MAAMC,eAAc;;;ACMtB,SAAS,SAAS,MAAM,WAAW,aAAa;AAErD,QAAM,iBAAiB,KAAK,YAAY;AAGxC,UAAQ,UAAU;AAAA,IAChB,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AACH,aAAO,mBAAmB,cAAc;AAAA,IAC1C,KAAK;AACH,aAAO,eAAe,cAAc;AAAA,IACtC,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AACH,aAAO,iBAAiB,cAAc;AAAA,IACxC,KAAK;AACH,aAAO,aAAa,cAAc;AAAA,IACpC,KAAK;AACH,aAAO,WAAW,cAAc;AAAA,IAClC,KAAK;AAAA,IACL;AACE,aAAO,gBAAgB,cAAc;AAAA,EACzC;AACF;AASO,SAAS,eAAe,QAAQ,GAAG;AAExC,MAAI,CAAC,UAAU,OAAO,WAAW;AAAG,WAAO,CAAC;AAC5C,MAAI,KAAK;AAAG,WAAO,CAAC;AACpB,MAAI,OAAO,SAAS;AAAG,WAAO,CAAC,OAAO,KAAK,GAAG,CAAC;AAE/C,QAAM,SAAS,CAAC;AAIhB,QAAM,qBAAqB,oBAAI,IAAI;AAGnC,WAAS,IAAI,GAAG,IAAI,OAAO,QAAQ,KAAK;AACtC,UAAM,QAAQ,OAAO,CAAC;AAGtB,QAAI,MAAM,WAAW,IAAI,KAAK,MAAM,SAAS,IAAI,GAAG;AAClD,yBAAmB,IAAI,CAAC;AACxB,yBAAmB,IAAI,IAAI,CAAC;AAAA,IAC9B;AAGA,QAAI,CAAC,KAAK,KAAK,KAAK,KAAK,KAAK,KAAK,KAAK,GAAG,EAAE,SAAS,KAAK,GAAG;AAC5D,yBAAmB,IAAI,CAAC;AACxB,yBAAmB,IAAI,IAAI,CAAC;AAAA,IAC9B;AAAA,EACF;AAIA,WAAS,IAAI,GAAG,KAAK,OAAO,SAAS,GAAG,KAAK;AAE3C,QAAI,cAAc;AAClB,aAAS,IAAI,GAAG,IAAI,IAAI,IAAI,GAAG,KAAK;AAClC,UAAI,mBAAmB,IAAI,IAAI,CAAC,GAAG;AACjC,sBAAc;AACd;AAAA,MACF;AAAA,IACF;AAGA,QAAI,CAAC,aAAa;AAChB,YAAM,QAAQ,OAAO,MAAM,GAAG,IAAI,CAAC,EAAE,KAAK,GAAG;AAC7C,aAAO,KAAK,KAAK;AAAA,IACnB;AAAA,EACF;AAEA,SAAO;AACT;AASO,SAAS,cAAc,QAAQ,GAAG;AACvC,SAAO,eAAe,QAAQ,CAAC;AACjC;AASO,SAAS,+BAA+B,MAAM,UAAU;AAE7D,MAAI,CAAC;AAAM,WAAO,CAAC;AAEnB,QAAM,SAAS,CAAC;AAGhB,QAAM,qBAAqB,SAAS,YAAY;AAGhD,UAAQ,oBAAoB;AAAA,IAC1B,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AACH,+BAAyB,MAAM,MAAM;AACrC;AAAA,IACF,KAAK;AACH,2BAAqB,MAAM,MAAM;AACjC;AAAA,IACF,KAAK;AAAA,IACL,KAAK;AACH,2BAAqB,MAAM,MAAM;AACjC;AAAA,EAEJ;AAEA,SAAO;AACT;AASA,SAAS,yBAAyB,MAAM,QAAQ;AAE9C,QAAM,oBACJ;AACF,MAAI;AAEJ,UAAQ,QAAQ,kBAAkB,KAAK,IAAI,OAAO,MAAM;AACtD,WAAO,KAAK;AAAA,MACV,OAAO,MAAM,CAAC;AAAA,MACd,MAAM;AAAA,MACN,UAAU;AAAA,QACR,OAAO,MAAM;AAAA,QACb,KAAK,MAAM,QAAQ,MAAM,CAAC,EAAE;AAAA,MAC9B;AAAA,IACF,CAAC;AAAA,EACH;AAGA,QAAM,kBACJ;AAEF,UAAQ,QAAQ,gBAAgB,KAAK,IAAI,OAAO,MAAM;AACpD,WAAO,KAAK;AAAA,MACV,OAAO,MAAM,CAAC;AAAA,MACd,MAAM;AAAA,MACN,UAAU;AAAA,QACR,OAAO,MAAM;AAAA,QACb,KAAK,MAAM,QAAQ,MAAM,CAAC,EAAE;AAAA,MAC9B;AAAA,IACF,CAAC;AAAA,EACH;AAGA,QAAM,qBACJ;AAEF,UAAQ,QAAQ,mBAAmB,KAAK,IAAI,OAAO,MAAM;AAEvD,UAAM,cAAc,OAAO;AAAA,MACzB,CAAC,UACC,MAAM,SAAS,sBACf,MAAM,SAAS,MAAM,SAAS,SAC9B,MAAM,QAAQ,MAAM,CAAC,EAAE,UAAU,MAAM,SAAS;AAAA,IACpD;AAEA,QAAI,CAAC,aAAa;AAChB,aAAO,KAAK;AAAA,QACV,OAAO,MAAM,CAAC;AAAA,QACd,MAAM;AAAA,QACN,UAAU;AAAA,UACR,OAAO,MAAM;AAAA,UACb,KAAK,MAAM,QAAQ,MAAM,CAAC,EAAE;AAAA,QAC9B;AAAA,MACF,CAAC;AAAA,IACH;AAAA,EACF;AACF;AASA,SAAS,qBAAqB,MAAM,QAAQ;AAE1C,QAAM,yBACJ;AACF,MAAI;AAEJ,UAAQ,QAAQ,uBAAuB,KAAK,IAAI,OAAO,MAAM;AAC3D,WAAO,KAAK;AAAA,MACV,OAAO,MAAM,CAAC;AAAA,MACd,MAAM;AAAA,MACN,UAAU;AAAA,QACR,OAAO,MAAM;AAAA,QACb,KAAK,MAAM,QAAQ,MAAM,CAAC,EAAE;AAAA,MAC9B;AAAA,IACF,CAAC;AAAA,EACH;AAGA,QAAM,yBACJ;AAEF,UAAQ,QAAQ,uBAAuB,KAAK,IAAI,OAAO,MAAM;AAC3D,WAAO,KAAK;AAAA,MACV,OAAO,MAAM,CAAC;AAAA,MACd,MAAM;AAAA,MACN,UAAU;AAAA,QACR,OAAO,MAAM;AAAA,QACb,KAAK,MAAM,QAAQ,MAAM,CAAC,EAAE;AAAA,MAC9B;AAAA,IACF,CAAC;AAAA,EACH;AAGA,QAAM,cAAc;AAEpB,UAAQ,QAAQ,YAAY,KAAK,IAAI,OAAO,MAAM;AAChD,WAAO,KAAK;AAAA,MACV,OAAO,MAAM,CAAC;AAAA,MACd,MAAM;AAAA,MACN,UAAU;AAAA,QACR,OAAO,MAAM;AAAA,QACb,KAAK,MAAM,QAAQ,MAAM,CAAC,EAAE;AAAA,MAC9B;AAAA,IACF,CAAC;AAAA,EACH;AAGA,QAAM,iBACJ;AAEF,UAAQ,QAAQ,eAAe,KAAK,IAAI,OAAO,MAAM;AACnD,WAAO,KAAK;AAAA,MACV,OAAO,MAAM,CAAC;AAAA,MACd,MAAM;AAAA,MACN,UAAU;AAAA,QACR,OAAO,MAAM;AAAA,QACb,KAAK,MAAM,QAAQ,MAAM,CAAC,EAAE;AAAA,MAC9B;AAAA,IACF,CAAC;AAAA,EACH;AACF;AASA,SAAS,qBAAqB,MAAM,QAAQ;AAE1C,QAAM,kBACJ;AACF,MAAI;AAEJ,UAAQ,QAAQ,gBAAgB,KAAK,IAAI,OAAO,MAAM;AACpD,WAAO,KAAK;AAAA,MACV,OAAO,MAAM,CAAC;AAAA,MACd,MAAM;AAAA,MACN,UAAU;AAAA,QACR,OAAO,MAAM;AAAA,QACb,KAAK,MAAM,QAAQ,MAAM,CAAC,EAAE;AAAA,MAC9B;AAAA,IACF,CAAC;AAAA,EACH;AAGA,QAAM,iBACJ;AAEF,UAAQ,QAAQ,eAAe,KAAK,IAAI,OAAO,MAAM;AACnD,WAAO,KAAK;AAAA,MACV,OAAO,MAAM,CAAC;AAAA,MACd,MAAM;AAAA,MACN,UAAU;AAAA,QACR,OAAO,MAAM;AAAA,QACb,KAAK,MAAM,QAAQ,MAAM,CAAC,EAAE;AAAA,MAC9B;AAAA,IACF,CAAC;AAAA,EACH;AAGA,QAAM,kBACJ;AAEF,UAAQ,QAAQ,gBAAgB,KAAK,IAAI,OAAO,MAAM;AACpD,WAAO,KAAK;AAAA,MACV,OAAO,MAAM,CAAC;AAAA,MACd,MAAM;AAAA,MACN,UAAU;AAAA,QACR,OAAO,MAAM;AAAA,QACb,KAAK,MAAM,QAAQ,MAAM,CAAC,EAAE;AAAA,MAC9B;AAAA,IACF,CAAC;AAAA,EACH;AAGA,QAAM,cAAc;AAEpB,UAAQ,QAAQ,YAAY,KAAK,IAAI,OAAO,MAAM;AAEhD,UAAM,cAAc,OAAO;AAAA,MACzB,CAAC,WACE,MAAM,SAAS,wBACd,MAAM,SAAS,wBACjB,MAAM,SAAS,MAAM,SAAS,SAC9B,MAAM,QAAQ,MAAM,CAAC,EAAE,UAAU,MAAM,SAAS;AAAA,IACpD;AAEA,QAAI,CAAC,aAAa;AAChB,aAAO,KAAK;AAAA,QACV,OAAO,MAAM,CAAC;AAAA,QACd,MAAM;AAAA,QACN,UAAU;AAAA,UACR,OAAO,MAAM;AAAA,UACb,KAAK,MAAM,QAAQ,MAAM,CAAC,EAAE;AAAA,QAC9B;AAAA,MACF,CAAC;AAAA,IACH;AAAA,EACF;AACF;AAUO,SAAS,gBAAgB,QAAQ,OAAO,IAAI,WAAW,aAAa;AAEzE,QAAM,YAAY,aAAa,QAAQ;AAGvC,QAAM,kBAAkB,CAAC;AACzB,aAAW,SAAS,QAAQ;AAC1B,QAAI,CAAC,gBAAgB,KAAK,GAAG;AAC3B,sBAAgB,KAAK,IAAI;AAAA,IAC3B;AACA,oBAAgB,KAAK;AAAA,EACvB;AAGA,QAAM,iBAAiB,CAAC;AAExB,aAAW,CAAC,OAAO,SAAS,KAAK,OAAO,QAAQ,eAAe,GAAG;AAGhE,QAAI,UAAU,IAAI,KAAK,KAAK,MAAM,SAAS,KAAK,CAAC,WAAW,KAAK,KAAK,GAAG;AACvE;AAAA,IACF;AAGA,QAAI,QAAQ;AAGZ,QAAI,sBAAsB,OAAO,QAAQ,GAAG;AAC1C,eAAS;AAAA,IACX;AAGA,QAAI,MAAM,SAAS,GAAG;AACpB,eAAS;AAAA,IACX;AAGA,QAAI,OAAO,KAAK,KAAK,GAAG;AACtB,eAAS;AAAA,IACX;AAGA,QAAI,MAAM,SAAS,KAAK,CAAC,WAAW,KAAK,KAAK,GAAG;AAC/C,eAAS;AAAA,IACX;AAGA,YAAQ,4BAA4B,OAAO,OAAO,QAAQ;AAE1D,mBAAe,KAAK;AAAA,MAClB,SAAS;AAAA,MACT;AAAA,IACF,CAAC;AAAA,EACH;AAGA,SAAO,eAAe,KAAK,CAAC,GAAG,MAAM,EAAE,QAAQ,EAAE,KAAK,EAAE,MAAM,GAAG,IAAI;AACvE;AASA,SAAS,sBAAsB,OAAO,UAAU;AAI9C,MAAI,aAAa,KAAK,KAAK,KAAK,cAAc,KAAK,KAAK,GAAG;AACzD,WAAO;AAAA,EACT;AAGA,MAAI,MAAM,SAAS,GAAG,KAAK,MAAM,SAAS,GAAG;AAC3C,WAAO;AAAA,EACT;AAGA,MAAI,mDAAmD,KAAK,KAAK,GAAG;AAClE,WAAO;AAAA,EACT;AAGA,MAAI,aAAa,KAAK,KAAK,GAAG;AAC5B,WAAO;AAAA,EACT;AAGA,OACG,aAAa,gBAAgB,aAAa,kBAC1C,KAAK,KAAK,KAAK;AAAA,EACd,YAAY,KAAK,KAAK,IACxB;AAEA,WAAO;AAAA,EACT;AAGA,MACE,aAAa,aACZ,WAAW,KAAK,KAAK;AAAA,EACpB,UAAU,KAAK,KAAK,IACtB;AAEA,WAAO;AAAA,EACT;AAEA,SAAO;AACT;AAUA,SAAS,4BAA4B,OAAO,OAAO,UAAU;AAC3D,UAAQ,UAAU;AAAA,IAChB,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAEH,UACE,2DAA2D,KAAK,KAAK,GACrE;AACA,iBAAS;AAAA,MACX;AAEA,UAAI,yBAAyB,KAAK,KAAK,GAAG;AACxC,iBAAS;AAAA,MACX;AACA;AAAA,IAEF,KAAK;AAEH,UAAI,4CAA4C,KAAK,KAAK,GAAG;AAC3D,iBAAS;AAAA,MACX;AAEA,UAAI,KAAK,KAAK,KAAK,GAAG;AACpB,iBAAS;AAAA,MACX;AACA;AAAA,IAEF,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAEH,UACE,qEAAqE;AAAA,QACnE;AAAA,MACF,GACA;AACA,iBAAS;AAAA,MACX;AAEA,UAAI,wCAAwC,KAAK,KAAK,GAAG;AACvD,iBAAS;AAAA,MACX;AACA;AAAA,IAEF,KAAK;AAEH,UAAI,mDAAmD,KAAK,KAAK,GAAG;AAClE,iBAAS;AAAA,MACX;AAEA,UAAI,KAAK,KAAK,KAAK,GAAG;AACpB,iBAAS;AAAA,MACX;AACA;AAAA,IAEF,KAAK;AAEH,UAAI,wDAAwD,KAAK,KAAK,GAAG;AACvE,iBAAS;AAAA,MACX;AACA;AAAA,EACJ;AAEA,SAAO;AACT;AAQA,SAAS,aAAa,UAAU;AAE9B,QAAM,kBAAkB,oBAAI,IAAI;AAAA,IAC9B;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF,CAAC;AAGD,QAAM,6BAA6B,oBAAI,IAAI;AAAA,IACzC;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF,CAAC;AAGD,QAAM,YAAY,oBAAI,IAAI;AAAA,IACxB,GAAG;AAAA,IACH,GAAG;AAAA,EACL,CAAC;AAGD,UAAQ,UAAU;AAAA,IAChB,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAEH;AAAA,QACE;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MACF,EAAE,QAAQ,CAAC,SAAS,UAAU,IAAI,IAAI,CAAC;AACvC;AAAA,IAEF,KAAK;AAEH;AAAA,QACE;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MACF,EAAE,QAAQ,CAAC,SAAS,UAAU,IAAI,IAAI,CAAC;AACvC;AAAA,IAEF,KAAK;AAEH;AAAA,QACE;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MACF,EAAE,QAAQ,CAAC,SAAS,UAAU,IAAI,IAAI,CAAC;AACvC;AAAA,IAEF,KAAK;AAAA,IACL,KAAK;AAEH;AAAA,QACE;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MACF,EAAE,QAAQ,CAAC,SAAS,UAAU,IAAI,IAAI,CAAC;AACvC;AAAA,IAEF,KAAK;AAEH;AAAA,QACE;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MACF,EAAE,QAAQ,CAAC,SAAS,UAAU,IAAI,IAAI,CAAC;AACvC;AAAA,IAEF,KAAK;AAEH;AAAA,QACE;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MACF,EAAE,QAAQ,CAAC,SAAS,UAAU,IAAI,IAAI,CAAC;AACvC;AAAA,EACJ;AAEA,SAAO;AACT;AAQA,SAAS,gBAAgB,MAAM;AAG7B,QAAM,aAAa,KAEhB,QAAQ,6BAA6B,OAAY,EAEjD,QAAQ,sBAAsB,MAAM,EAEpC,QAAQ,WAAW,GAAG;AAGzB,MAAI,SAAS,WAAW,MAAM,KAAK,EAAE,OAAO,CAAC,UAAU,MAAM,SAAS,CAAC;AAEvE,SAAO;AACT;AASA,SAAS,mBAAmB,MAAM;AAChC,MAAI,SAAS,CAAC;AAGd,QAAM,sBAAsB,CAAC;AAC7B,MAAI,iBAAiB;AAGrB,QAAM,uBAAuB,KAAK,QAAQ,qBAAqB,CAAC,UAAU;AACxE,UAAM,cAAc,mBAAmB,gBAAgB;AACvD,wBAAoB,WAAW,IAAI;AACnC,WAAO;AAAA,EACT,CAAC;AAGD,QAAM,kBAAkB,qBAAqB;AAAA,IAC3C;AAAA,IACA,CAAC,UAAU;AACT,YAAM,cAAc,kBAAkB,gBAAgB;AACtD,0BAAoB,WAAW,IAAI;AACnC,aAAO;AAAA,IACT;AAAA,EACF;AAGA,QAAM,qBAAqB,CAAC;AAC5B,MAAI,gBAAgB;AAIpB,QAAM,eAAe,gBAAgB;AAAA,IACnC;AAAA,IACA,CAAC,UAAU;AACT,YAAM,cAAc,WAAW,eAAe;AAC9C,yBAAmB,WAAW,IAAI;AAClC,aAAO;AAAA,IACT;AAAA,EACF;AAIA,QAAM,0BAA0B,aAAa;AAAA,IAC3C;AAAA,IACA,CAAC,UAAU;AACT,YAAM,cAAc,cAAc,eAAe;AACjD,yBAAmB,WAAW,IAAI;AAGlC,YAAM,cAAc,CAAC;AACrB,UAAI,aAAa,MAAM,MAAM,cAAc;AAC3C,UAAI,YAAY;AACd,mBAAW,QAAQ,CAAC,QAAQ;AAC1B,sBAAY,KAAK,IAAI,MAAM,GAAG,EAAE,CAAC;AAAA,QACnC,CAAC;AAGD,oBAAY,QAAQ,CAAC,QAAQ;AAC3B,gBAAM,YAAY,mBAAmB,GAAG;AACxC,iBAAO,KAAK,GAAG,SAAS;AAAA,QAC1B,CAAC;AAAA,MACH;AAEA,aAAO;AAAA,IACT;AAAA,EACF;AAGA,QAAM,iBAAiB,wBAAwB;AAAA,IAC7C;AAAA,IACA,CAAC,UAAU;AACT,YAAM,cAAc,YAAY,eAAe;AAC/C,yBAAmB,WAAW,IAAI;AAClC,aAAO;AAAA,IACT;AAAA,EACF;AAIA,QAAM,aAAa,eAAe;AAAA,IAChC;AAAA,IACA,CAAC,OAAO,SAAS,eAAe;AAC9B,YAAM,cAAc,aAAa,eAAe;AAChD,yBAAmB,WAAW,IAAI;AAGlC,aAAO,KAAK,OAAO;AAGnB,UAAI,YAAY;AACd,cAAM,cAAc,WAAW,MAAM,qBAAqB;AAC1D,YAAI,aAAa;AACf,iBAAO,KAAK,GAAG,WAAW;AAAA,QAC5B;AAAA,MACF;AAEA,aAAO;AAAA,IACT;AAAA,EACF;AAGA,QAAM,oBAAoB,WAAW;AAAA,IACnC;AAAA,IACA,CAAC,OAAO,YAAY;AAClB,YAAM,cAAc,iBAAiB,eAAe;AACpD,yBAAmB,WAAW,IAAI;AAClC,aAAO,KAAK,OAAO;AACnB,aAAO;AAAA,IACT;AAAA,EACF;AAGA,QAAM,oBAAoB,kBAAkB;AAAA,IAC1C;AAAA,IACA,CAAC,OAAO,kBAAkB;AACxB,YAAM,cAAc,eAAe,eAAe;AAClD,yBAAmB,WAAW,IAAI;AAGlC,aAAO,KAAK,aAAa;AAGzB,YAAM,aAAa,MAAM,MAAM,UAAU;AACzC,UAAI,cAAc,WAAW,CAAC,GAAG;AAC/B,cAAM,cAAc,gBAAgB,WAAW,CAAC,CAAC;AACjD,eAAO,KAAK,GAAG,WAAW;AAAA,MAC5B;AAEA,aAAO;AAAA,IACT;AAAA,EACF;AAGA,QAAM,gBAAgB,kBAAkB,QAAQ,OAAO,CAAC,UAAU;AAChE,WAAO,KAAK,gBAAgB;AAC5B,WAAO;AAAA,EACT,CAAC;AAGD,QAAM,iBAAiB,cACpB,QAAQ,SAAS,CAAC,UAAU;AAC3B,WAAO,KAAK,mBAAmB;AAC/B,WAAO;AAAA,EACT,CAAC,EACA,QAAQ,SAAS,CAAC,UAAU;AAC3B,WAAO,KAAK,oBAAoB;AAChC,WAAO;AAAA,EACT,CAAC;AAGH,QAAM,iBAAiB,eAAe;AAAA,IACpC;AAAA,IACA,CAAC,UAAU;AAET,aAAO,KAAK,QAAQ;AAGpB,YAAM,cAAc,MAAM,MAAM,yBAAyB;AACzD,UAAI,eAAe,YAAY,CAAC,GAAG;AACjC,eAAO,KAAK,YAAY,CAAC,CAAC;AAAA,MAC5B;AAGA,YAAM,gBAAgB,MAAM;AAAA,QAC1B;AAAA,MACF;AACA,UAAI,iBAAiB,cAAc,CAAC,GAAG;AACrC,cAAM,gBAAgB,cAAc,CAAC;AAErC,YAAI,cAAc,WAAW,GAAG,GAAG;AAEjC,gBAAM,eAAe,cAClB,QAAQ,SAAS,EAAE,EACnB,MAAM,GAAG,EACT,IAAI,CAAC,SAAS,KAAK,KAAK,CAAC,EACzB,OAAO,CAAC,SAAS,KAAK,SAAS,CAAC;AAEnC,iBAAO,KAAK,GAAG,YAAY;AAAA,QAC7B,WAAW,cAAc,SAAS,MAAM,GAAG;AAEzC,gBAAM,UAAU,cAAc;AAAA,YAC5B;AAAA,UACF;AACA,cAAI,WAAW,QAAQ,CAAC,GAAG;AACzB,mBAAO,KAAK,QAAQ,CAAC,CAAC;AAAA,UACxB;AAAA,QACF,OAAO;AAEL,iBAAO,KAAK,cAAc,KAAK,CAAC;AAAA,QAClC;AAAA,MACF;AAEA,aAAO;AAAA,IACT;AAAA,EACF;AAGA,MAAI,aAAa,gBAAgB,cAAc;AAG/C,QAAM,kBAAkB,CAAC;AACzB,aAAW,SAAS,YAAY;AAE9B,QAAI,MAAM,WAAW,IAAI,KAAK,MAAM,SAAS,IAAI,GAAG;AAClD,sBAAgB,KAAK,KAAK;AAC1B;AAAA,IACF;AAGA,QAAI,CAAC,MAAM,MAAM,IAAI,EAAE,SAAS,KAAK,GAAG;AACtC,sBAAgB,KAAK,KAAK;AAC1B;AAAA,IACF;AAGA,UAAM,cAAc,MACjB,QAAQ,mBAAmB,OAAO,EAClC,YAAY,EACZ,MAAM,GAAG;AAGZ,oBAAgB,KAAK,KAAK;AAC1B,QAAI,YAAY,SAAS,GAAG;AAC1B,sBAAgB,KAAK,GAAG,WAAW;AAAA,IACrC;AAAA,EACF;AAGA,QAAM,cAAc,CAAC;AACrB,aAAW,SAAS,iBAAiB;AACnC,QAAI,mBAAmB,KAAK,GAAG;AAE7B,UAAI,MAAM,WAAW,UAAU,GAAG;AAChC,oBAAY,KAAK,eAAe;AAAA,MAClC,WAAW,MAAM,WAAW,QAAQ,GAAG;AACrC,oBAAY,KAAK,aAAa;AAAA,MAChC,WAAW,MAAM,WAAW,cAAc,GAAG;AAC3C,oBAAY,KAAK,WAAW;AAAA,MAC9B,OAAO;AACL,oBAAY,KAAK,KAAK;AAAA,MACxB;AAGA,UAAI,MAAM,WAAW,WAAW,KAAK,MAAM,WAAW,aAAa,GAAG;AAEpE,cAAM,UAAU,mBAAmB,KAAK;AAExC,cAAM,aAAa,QAAQ,QAAQ,qBAAqB,IAAI;AAC5D,cAAM,gBAAgB,gBAAgB,UAAU;AAChD,oBAAY,KAAK,GAAG,aAAa;AAAA,MACnC;AAAA,IACF,WAAW,oBAAoB,KAAK,GAAG;AAGrC,kBAAY,KAAK,cAAc;AAG/B,YAAM,iBAAiB,oBAAoB,KAAK,EAC7C,QAAQ,gBAAgB,EAAE,EAC1B,QAAQ,UAAU,EAAE;AAGvB,YAAM,gBAAgB,eACnB,MAAM,KAAK,EACX,OAAO,CAAC,SAAS,mBAAmB,KAAK,IAAI,CAAC,EAC9C,IAAI,CAAC,SAAS,KAAK,YAAY,CAAC;AAEnC,kBAAY,KAAK,GAAG,aAAa;AAAA,IACnC,OAAO;AACL,kBAAY,KAAK,KAAK;AAAA,IACxB;AAAA,EACF;AAEA,SAAO,CAAC,GAAG,IAAI,IAAI,WAAW,CAAC;AACjC;AASA,SAAS,eAAe,MAAM;AAC5B,MAAI,SAAS,CAAC;AAGd,QAAM,sBAAsB,CAAC;AAC7B,MAAI,iBAAiB;AAGrB,QAAM,oBAAoB,KAAK;AAAA,IAC7B;AAAA,IACA,CAAC,UAAU;AACT,YAAM,cAAc,qBAAqB,gBAAgB;AACzD,0BAAoB,WAAW,IAAI;AACnC,aAAO;AAAA,IACT;AAAA,EACF;AAGA,QAAM,kBAAkB,kBAAkB,QAAQ,YAAY,CAAC,UAAU;AACvE,UAAM,cAAc,oBAAoB,gBAAgB;AACxD,wBAAoB,WAAW,IAAI;AACnC,WAAO;AAAA,EACT,CAAC;AAGD,QAAM,qBAAqB,CAAC;AAC5B,MAAI,qBAAqB;AAGzB,QAAM,kBAAkB,gBAAgB;AAAA,IACtC;AAAA,IACA,CAAC,UAAU;AACT,YAAM,cAAc,eAAe,oBAAoB;AACvD,yBAAmB,WAAW,IAAI;AAGlC,YAAM,cAAc,CAAC;AAErB,UAAI,aAAa,MAAM,MAAM,oBAAoB;AACjD,UAAI,YAAY;AACd,mBAAW,QAAQ,CAAC,QAAQ;AAC1B,sBAAY,KAAK,IAAI,MAAM,GAAG,EAAE,CAAC;AAAA,QACnC,CAAC;AAGD,oBAAY,QAAQ,CAAC,QAAQ;AAC3B,gBAAM,YAAY,eAAe,GAAG;AACpC,iBAAO,KAAK,GAAG,SAAS;AAAA,QAC1B,CAAC;AAAA,MACH;AAEA,aAAO;AAAA,IACT;AAAA,EACF;AAGA,QAAM,wBAAwB,gBAAgB;AAAA,IAC5C;AAAA,IACA,CAAC,UAAU;AACT,YAAM,cAAc,cAAc,oBAAoB;AACtD,yBAAmB,WAAW,IAAI;AAClC,aAAO;AAAA,IACT;AAAA,EACF;AAGA,QAAM,oBAAoB,sBAAsB;AAAA,IAC9C;AAAA,IACA,CAAC,OAAO,kBAAkB;AACxB,YAAM,cAAc,iBAAiB,oBAAoB;AACzD,yBAAmB,WAAW,IAAI;AAGlC,aAAO,KAAK,aAAa;AAGzB,YAAM,aAAa,MAAM,MAAM,UAAU;AACzC,UAAI,cAAc,WAAW,CAAC,GAAG;AAC/B,cAAM,cAAc,gBAAgB,WAAW,CAAC,CAAC;AACjD,eAAO,KAAK,GAAG,WAAW;AAAA,MAC5B;AAEA,aAAO;AAAA,IACT;AAAA,EACF;AAGA,QAAM,iBAAiB,kBAEpB,QAAQ,OAAO,CAAC,UAAU;AACzB,WAAO,KAAK,iBAAiB;AAC7B,WAAO;AAAA,EACT,CAAC,EAEA,QAAQ,cAAc,CAAC,UAAU;AAChC,WAAO,KAAK,iBAAiB;AAE7B,UAAM,eAAe,MAAM,MAAM,GAAG,EAAE;AACtC,UAAM,aAAa,aAAa,MAAM,GAAG;AACzC,eAAW,QAAQ,CAAC,SAAS;AAC3B,UAAI,KAAK,KAAK,GAAG;AACf,cAAM,aAAa,gBAAgB,KAAK,KAAK,CAAC;AAC9C,eAAO,KAAK,GAAG,UAAU;AAAA,MAC3B;AAAA,IACF,CAAC;AACD,WAAO;AAAA,EACT,CAAC;AAGH,QAAM,QAAQ,eAAe,MAAM,IAAI;AAGvC,MAAI,sBAAsB;AAE1B,aAAW,QAAQ,OAAO;AAExB,QAAI,KAAK,KAAK,MAAM;AAAI;AAGxB,UAAM,cAAc,KAAK,MAAM,QAAQ;AACvC,UAAM,gBAAgB,cAAc,YAAY,CAAC,EAAE,SAAS;AAE5D,QAAI,kBAAkB,qBAAqB;AACzC,UAAI,gBAAgB,qBAAqB;AAEvC,eAAO,KAAK,QAAQ;AAAA,MACtB,OAAO;AAGL,cAAM,eAAe,KAAK;AAAA,WACvB,sBAAsB,iBAAiB;AAAA,QAC1C;AACA,iBAAS,IAAI,GAAG,IAAI,cAAc,KAAK;AACrC,iBAAO,KAAK,QAAQ;AAAA,QACtB;AAAA,MACF;AACA,4BAAsB;AAAA,IACxB;AAGA,UAAM,cAAc,KAAK,KAAK;AAC9B,QAAI,aAAa;AAEf,YAAM,iBAAiB;AAAA,QACrB;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MACF;AAGA,iBAAW,WAAW,gBAAgB;AACpC,YAAI,YAAY,SAAS,OAAO,GAAG;AACjC,gBAAM,eAAe,IAAI,OAAO,MAAM,OAAO,OAAO,GAAG;AACvD,cAAI,aAAa,KAAK,WAAW,GAAG;AAClC,mBAAO,KAAK,OAAO;AAAA,UACrB;AAAA,QACF;AAAA,MACF;AAGA,YAAM,aAAa,gBAAgB,WAAW;AAC9C,aAAO,KAAK,GAAG,UAAU;AAAA,IAC3B;AAAA,EACF;AAGA,MACE,eAAe,SAAS,SAAS,KACjC,eAAe,SAAS,UAAU,GAClC;AACA,WAAO,KAAK,gBAAgB;AAAA,EAC9B;AAGA,MACE,eAAe,SAAS,OAAO,KAC/B,eAAe,SAAS,UAAU,KAClC,eAAe,SAAS,SAAS,KACjC,eAAe,SAAS,WAAW,GACnC;AACA,WAAO,KAAK,gBAAgB;AAAA,EAC9B;AAGA,QAAM,kBAAkB,CAAC;AACzB,aAAW,SAAS,QAAQ;AAE1B,QAAI,MAAM,WAAW,IAAI,KAAK,MAAM,SAAS,IAAI,GAAG;AAClD,sBAAgB,KAAK,KAAK;AAC1B;AAAA,IACF;AAGA,QAAI,MAAM,SAAS,GAAG,GAAG;AACvB,YAAM,QAAQ,MAAM,MAAM,GAAG,EAAE,OAAO,CAAC,SAAS,KAAK,SAAS,CAAC;AAC/D,sBAAgB,KAAK,KAAK;AAC1B,sBAAgB,KAAK,GAAG,KAAK;AAAA,IAC/B,OAAO;AACL,sBAAgB,KAAK,KAAK;AAAA,IAC5B;AAAA,EACF;AAGA,QAAM,cAAc,CAAC;AACrB,aAAW,SAAS,iBAAiB;AACnC,QAAI,mBAAmB,KAAK,GAAG;AAC7B,UAAI,MAAM,WAAW,cAAc,GAAG;AACpC,oBAAY,KAAK,UAAU;AAAA,MAC7B,WAAW,MAAM,WAAW,aAAa,GAAG;AAC1C,oBAAY,KAAK,gBAAgB;AAAA,MACnC,WAAW,MAAM,WAAW,gBAAgB,GAAG;AAC7C,oBAAY,KAAK,WAAW;AAAA,MAC9B,OAAO;AACL,oBAAY,KAAK,KAAK;AAAA,MACxB;AAGA,UAAI,MAAM,WAAW,aAAa,KAAK,MAAM,WAAW,cAAc,GAAG;AACvE,cAAM,UAAU,mBAAmB,KAAK;AAExC,YAAI,aAAa;AAGjB,YACE,WAAW,WAAW,GAAG,KACzB,WAAW,WAAW,GAAG,KACzB,WAAW,WAAW,IAAI,KAC1B,WAAW,WAAW,IAAI,KAC1B,WAAW,WAAW,GAAG,KACzB,WAAW,WAAW,IAAI,KAC1B,WAAW,WAAW,IAAI,GAC1B;AACA,gBAAM,eAAe,UAAU,KAAK,UAAU,EAAE,CAAC,EAAE;AACnD,uBAAa,WAAW,UAAU,YAAY;AAAA,QAChD;AAGA,qBAAa,WAAW,QAAQ,gBAAgB,EAAE;AAClD,qBAAa,WAAW,QAAQ,cAAc,EAAE;AAChD,qBAAa,WAAW,QAAQ,cAAc,EAAE;AAGhD,qBAAa,WAAW,QAAQ,aAAa,GAAG;AAGhD,cAAM,gBAAgB,gBAAgB,UAAU;AAChD,oBAAY,KAAK,GAAG,aAAa;AAAA,MACnC;AAAA,IACF,WAAW,oBAAoB,KAAK,GAAG;AAErC,kBAAY,KAAK,cAAc;AAG/B,YAAM,iBAAiB,oBAAoB,KAAK,EAC7C,QAAQ,SAAS,EAAE,EACnB,QAAQ,cAAc,EAAE,EACxB,QAAQ,cAAc,EAAE;AAG3B,YAAM,gBAAgB,eACnB,MAAM,KAAK,EACX,OAAO,CAAC,SAAS,mBAAmB,KAAK,IAAI,CAAC,EAC9C,IAAI,CAAC,SAAS,KAAK,YAAY,CAAC;AAEnC,kBAAY,KAAK,GAAG,aAAa;AAAA,IACnC,OAAO;AACL,kBAAY,KAAK,KAAK;AAAA,IACxB;AAAA,EACF;AAEA,SAAO,CAAC,GAAG,IAAI,IAAI,WAAW,CAAC;AACjC;AASA,SAAS,iBAAiB,MAAM;AAC9B,MAAI,SAAS,CAAC;AAGd,QAAM,sBAAsB,CAAC;AAC7B,MAAI,iBAAiB;AAGrB,QAAM,uBAAuB,KAAK,QAAQ,qBAAqB,CAAC,UAAU;AACxE,UAAM,cAAc,wBAAwB,gBAAgB;AAC5D,wBAAoB,WAAW,IAAI;AACnC,WAAO;AAAA,EACT,CAAC;AAGD,QAAM,kBAAkB,qBAAqB;AAAA,IAC3C;AAAA,IACA,CAAC,UAAU;AACT,YAAM,cAAc,uBAAuB,gBAAgB;AAC3D,0BAAoB,WAAW,IAAI;AACnC,aAAO;AAAA,IACT;AAAA,EACF;AAGA,QAAM,qBAAqB,CAAC;AAC5B,MAAI,qBAAqB;AAGzB,QAAM,iBAAiB,gBAAgB;AAAA,IACrC;AAAA,IACA,CAAC,UAAU;AACT,YAAM,cAAc,gBAAgB,oBAAoB;AACxD,yBAAmB,WAAW,IAAI;AAClC,aAAO;AAAA,IACT;AAAA,EACF;AAGA,QAAM,qBAAqB,eAAe;AAAA,IACxC;AAAA,IACA,CAAC,OAAO,mBAAmB;AACzB,YAAM,cAAc,gBAAgB,oBAAoB;AACxD,yBAAmB,WAAW,IAAI;AAGlC,aAAO,KAAK,YAAY;AACxB,aAAO,KAAK,eAAe,YAAY,CAAC;AAGxC,YAAM,aAAa,MAAM,MAAM,UAAU;AACzC,UAAI,cAAc,WAAW,CAAC,GAAG;AAC/B,cAAM,SAAS,WAAW,CAAC;AAG3B,cAAM,gBAAgB,OAAO,MAAM,GAAG;AACtC,mBAAW,QAAQ,eAAe;AAChC,gBAAM,QAAQ,KAAK,MAAM,GAAG;AAC5B,cAAI,MAAM,WAAW,GAAG;AAEtB,mBAAO,KAAK,MAAM,CAAC,EAAE,KAAK,CAAC;AAAA,UAC7B;AAEA,gBAAM,cAAc,gBAAgB,IAAI;AACxC,iBAAO,KAAK,GAAG,WAAW;AAAA,QAC5B;AAAA,MACF;AAEA,aAAO;AAAA,IACT;AAAA,EACF;AAIA,QAAM,kBAAkB,mBAAmB;AAAA,IACzC;AAAA,IACA,CAAC,UAAU;AACT,YAAM,cAAc,aAAa,oBAAoB;AACrD,yBAAmB,WAAW,IAAI;AAGlC,aAAO,KAAK,cAAc;AAI1B,YAAM,UAAU,MAAM,MAAM,GAAG,EAAE;AAGjC,YAAM,aAAa,QAAQ,MAAM,cAAc;AAG/C,iBAAW,SAAS,YAAY;AAC9B,cAAM,cAAc,gBAAgB,MAAM,KAAK,CAAC;AAChD,eAAO,KAAK,GAAG,WAAW;AAAA,MAC5B;AAEA,aAAO;AAAA,IACT;AAAA,EACF;AAGA,QAAM,iBAAiB,gBAAgB;AAAA,IACrC;AAAA,IACA,CAAC,UAAU;AACT,YAAM,cAAc,YAAY,oBAAoB;AACpD,yBAAmB,WAAW,IAAI;AAGlC,aAAO,KAAK,mBAAmB;AAG/B,YAAM,eAAe,gBAAgB,KAAK;AAC1C,aAAO,KAAK,GAAG,YAAY;AAE3B,aAAO;AAAA,IACT;AAAA,EACF;AAGA,QAAM,kBAAkB;AAAA,IACtB;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF;AAEA,MAAI,sBAAsB;AAC1B,aAAW,YAAY,iBAAiB;AAEtC,UAAM,QAAQ,IAAI,OAAO,MAAM,QAAQ,OAAO,IAAI;AAClD,0BAAsB,oBAAoB,QAAQ,OAAO,CAAC,UAAU;AAClE,aAAO,KAAK,MAAM,YAAY,CAAC;AAC/B,aAAO,KAAK,iBAAiB;AAC7B,aAAO;AAAA,IACT,CAAC;AAAA,EACH;AAGA,wBAAsB,oBAAoB;AAAA,IACxC;AAAA,IACA,CAAC,OAAO,gBAAgB;AACtB,aAAO,KAAK,qBAAqB;AAGjC,YAAM,eAAe,YAAY,MAAM,GAAG;AAC1C,aAAO,KAAK,WAAW;AACvB,aAAO,KAAK,GAAG,YAAY;AAE3B,aAAO;AAAA,IACT;AAAA,EACF;AAGA,wBAAsB,oBAAoB;AAAA,IACxC;AAAA,IACA,CAAC,OAAO,eAAe;AACrB,aAAO,KAAK,kBAAkB;AAG9B,YAAM,cAAc,WAAW,MAAM,GAAG;AACxC,aAAO,KAAK,UAAU;AAEtB,UACE,YAAY,SAAS,KACrB,YAAY,YAAY,SAAS,CAAC,MAAM,KACxC;AACA,oBAAY,IAAI;AAChB,eAAO,KAAK,iBAAiB;AAAA,MAC/B;AACA,aAAO,KAAK,GAAG,WAAW;AAE1B,aAAO;AAAA,IACT;AAAA,EACF;AAGA,MAAI,gCAAgC,KAAK,mBAAmB,GAAG;AAC7D,WAAO,KAAK,iBAAiB;AAG7B,UAAM,eAAe;AAAA,MACnB;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAEA,eAAW,WAAW,cAAc;AAClC,YAAM,QAAQ,IAAI,OAAO,MAAM,OAAO,OAAO,IAAI;AACjD,UAAI,MAAM,KAAK,mBAAmB,GAAG;AACnC,eAAO,KAAK,QAAQ,OAAO,EAAE;AAAA,MAC/B;AAAA,IACF;AAAA,EACF;AAGA,QAAM,aAAa,gBAAgB,mBAAmB;AACtD,SAAO,KAAK,GAAG,UAAU;AAGzB,QAAM,kBAAkB,CAAC;AACzB,aAAW,SAAS,QAAQ;AAE1B,QAAI,MAAM,WAAW,IAAI,KAAK,MAAM,SAAS,IAAI,GAAG;AAClD,sBAAgB,KAAK,KAAK;AAC1B;AAAA,IACF;AAGA,QAAI,MAAM,SAAS,GAAG,GAAG;AACvB,YAAM,QAAQ,MAAM,MAAM,GAAG;AAC7B,sBAAgB,KAAK,KAAK;AAC1B,sBAAgB,KAAK,GAAG,KAAK;AAC7B;AAAA,IACF;AAIA,oBAAgB,KAAK,KAAK;AAG1B,QAAI,aAAa,KAAK,KAAK,GAAG;AAC5B,YAAM,QAAQ,MACX,QAAQ,mBAAmB,OAAO,EAClC,YAAY,EACZ,MAAM,GAAG;AAEZ,UAAI,MAAM,SAAS,GAAG;AACpB,wBAAgB,KAAK,GAAG,KAAK;AAAA,MAC/B;AAAA,IACF;AAAA,EACF;AAGA,QAAM,cAAc,CAAC;AACrB,aAAW,SAAS,iBAAiB;AACnC,QAAI,mBAAmB,KAAK,GAAG;AAE7B,UAAI,MAAM,WAAW,eAAe,GAAG;AACrC,oBAAY,KAAK,gBAAgB;AAAA,MACnC,WAAW,MAAM,WAAW,eAAe,GAAG;AAC5C,oBAAY,KAAK,YAAY;AAAA,MAC/B,WAAW,MAAM,WAAW,YAAY,GAAG;AACzC,oBAAY,KAAK,SAAS;AAAA,MAC5B,WAAW,MAAM,WAAW,WAAW,GAAG;AACxC,oBAAY,KAAK,QAAQ;AAAA,MAC3B,OAAO;AACL,oBAAY,KAAK,KAAK;AAAA,MACxB;AAGA,UAAI,MAAM,WAAW,eAAe,GAAG;AACrC,cAAM,UAAU,mBAAmB,KAAK;AAExC,cAAM,aAAa,QAAQ,QAAQ,UAAU,EAAE,EAAE,QAAQ,UAAU,EAAE;AAGrE,YAAI,WAAW,KAAK,EAAE,SAAS,GAAG;AAChC,gBAAM,gBAAgB,gBAAgB,UAAU;AAChD,sBAAY,KAAK,GAAG,aAAa;AAAA,QACnC;AAAA,MACF;AAAA,IACF,WAAW,oBAAoB,KAAK,GAAG;AAErC,kBAAY,KAAK,cAAc;AAG/B,YAAM,iBAAiB,oBAAoB,KAAK,EAC7C,QAAQ,gBAAgB,EAAE,EAC1B,QAAQ,UAAU,EAAE;AAGvB,YAAM,gBAAgB,eACnB,MAAM,KAAK,EACX,OAAO,CAAC,SAAS,mBAAmB,KAAK,IAAI,CAAC,EAC9C,IAAI,CAAC,SAAS,KAAK,YAAY,CAAC;AAEnC,kBAAY,KAAK,GAAG,aAAa;AAAA,IACnC,OAAO;AACL,kBAAY,KAAK,KAAK;AAAA,IACxB;AAAA,EACF;AAEA,SAAO,CAAC,GAAG,IAAI,IAAI,WAAW,CAAC;AACjC;AASA,SAAS,aAAa,MAAM;AAC1B,MAAI,SAAS,CAAC;AAGd,QAAM,sBAAsB,CAAC;AAC7B,MAAI,iBAAiB;AAGrB,QAAM,uBAAuB,KAAK,QAAQ,uBAAuB,CAAC,UAAU;AAC1E,UAAM,cAAc,wBAAwB,gBAAgB;AAC5D,wBAAoB,WAAW,IAAI;AACnC,WAAO;AAAA,EACT,CAAC;AAGD,QAAM,kBAAkB,qBAAqB,QAAQ,YAAY,CAAC,UAAU;AAC1E,UAAM,cAAc,uBAAuB,gBAAgB;AAC3D,wBAAoB,WAAW,IAAI;AACnC,WAAO;AAAA,EACT,CAAC;AAGD,QAAM,qBAAqB,CAAC;AAC5B,MAAI,qBAAqB;AAIzB,QAAM,uBAAuB,gBAAgB;AAAA,IAC3C;AAAA,IACA,CAAC,UAAU;AACT,YAAM,cAAc,8BAA8B,oBAAoB;AACtE,yBAAmB,WAAW,IAAI;AAGlC,YAAM,cAAc,CAAC;AACrB,UAAI,aAAa,MAAM,MAAM,eAAe;AAC5C,UAAI,YAAY;AACd,mBAAW,QAAQ,CAAC,QAAQ;AAC1B,sBAAY,KAAK,IAAI,MAAM,GAAG,EAAE,CAAC;AAAA,QACnC,CAAC;AAGD,oBAAY,QAAQ,CAAC,QAAQ;AAC3B,gBAAM,YAAY,aAAa,GAAG;AAClC,iBAAO,KAAK,GAAG,SAAS;AAAA,QAC1B,CAAC;AAAA,MACH;AAEA,aAAO;AAAA,IACT;AAAA,EACF;AAGA,QAAM,iBAAiB,qBAAqB;AAAA,IAC1C;AAAA,IACA,CAAC,UAAU;AACT,YAAM,cAAc,iBAAiB,oBAAoB;AACzD,yBAAmB,WAAW,IAAI;AAClC,aAAO;AAAA,IACT;AAAA,EACF;AAGA,QAAM,iBAAiB,eAAe;AAAA,IACpC;AAAA,IACA,CAAC,UAAU;AACT,YAAM,cAAc,iBAAiB,oBAAoB;AACzD,yBAAmB,WAAW,IAAI;AAClC,aAAO,KAAK,QAAQ;AACpB,aAAO;AAAA,IACT;AAAA,EACF;AAGA,QAAM,iBAAiB,eAAe;AAAA,IACpC;AAAA,IACA,CAAC,UAAU;AACT,YAAM,cAAc,iBAAiB,oBAAoB;AACzD,yBAAmB,WAAW,IAAI;AAGlC,YAAM,aAAa,MAAM,UAAU,CAAC;AACpC,aAAO,KAAK,QAAQ;AACpB,aAAO,KAAK,UAAU,UAAU,EAAE;AAGlC,UAAI,WAAW,SAAS,GAAG,KAAK,WAAW,SAAS,GAAG,GAAG;AACxD,eAAO,KAAK,UAAU,WAAW,MAAM,GAAG,EAAE,CAAC,EAAE;AAAA,MACjD;AAEA,aAAO;AAAA,IACT;AAAA,EACF;AAGA,MAAI,gBAAgB;AAGpB,kBAAgB,cAAc;AAAA,IAC5B;AAAA,IACA,CAAC,UAAU;AACT,YAAM,cAAc,mBAAmB,oBAAoB;AAC3D,yBAAmB,WAAW,IAAI;AAElC,aAAO,KAAK,cAAc;AAG1B,YAAM,aAAa,MAAM,MAAM,mBAAmB;AAClD,UAAI,cAAc,WAAW,CAAC,GAAG;AAC/B,cAAM,SAAS,WAAW,CAAC,EAAE,MAAM,GAAG;AACtC,eAAO,QAAQ,CAAC,UAAU;AACxB,iBAAO,KAAK,MAAM,KAAK,CAAC;AAAA,QAC1B,CAAC;AAAA,MACH;AAGA,YAAM,eAAe,MAClB,QAAQ,yBAAyB,EAAE,EACnC,QAAQ,YAAY,EAAE;AAEzB,YAAM,gBAAgB,gBAAgB,YAAY;AAClD,aAAO,KAAK,GAAG,aAAa;AAE5B,aAAO;AAAA,IACT;AAAA,EACF;AAGA,kBAAgB,cAAc;AAAA,IAC5B;AAAA,IACA,CAAC,UAAU;AAET,UAAI,UAAU,KAAK,KAAK,KAAK,aAAa,KAAK,KAAK,GAAG;AACrD,eAAO;AAAA,MACT;AAEA,YAAM,cAAc,sBAAsB,oBAAoB;AAC9D,yBAAmB,WAAW,IAAI;AAElC,aAAO,KAAK,aAAa;AAGzB,YAAM,aAAa,MAAM,MAAM,mBAAmB;AAClD,UAAI,cAAc,WAAW,CAAC,GAAG;AAC/B,cAAM,SAAS,WAAW,CAAC,EAAE,MAAM,GAAG;AACtC,eAAO,QAAQ,CAAC,UAAU;AACxB,iBAAO,KAAK,MAAM,KAAK,CAAC;AAAA,QAC1B,CAAC;AAAA,MACH;AAGA,UAAI,eAAe,MAAM,MAAM,GAAG,EAAE;AACpC,UAAI,YAAY;AACd,uBAAe,aAAa,QAAQ,mBAAmB,EAAE;AAAA,MAC3D;AAEA,YAAM,gBAAgB,gBAAgB,YAAY;AAClD,aAAO,KAAK,GAAG,aAAa;AAE5B,aAAO;AAAA,IACT;AAAA,EACF;AAGA,MAAI,eAAe,cAAc,QAAQ,cAAc,CAAC,UAAU;AAChE,WAAO;AAAA,MACL,UAAU,OAAO,6BAA6B;AAAA,IAChD;AACA,WAAO,MAAM,QAAQ;AAAA,EACvB,CAAC;AAGD,iBAAe,aAAa;AAAA,IAC1B;AAAA,IACA,CAAC,OAAO,eAAe;AACrB,aAAO,KAAK,mBAAmB;AAC/B,aAAO,KAAK,UAAU;AAGtB,UACE,WAAW,SAAS,GAAG,KACvB,WAAW,SAAS,GAAG,KACvB,WAAW,SAAS,GAAG,GACvB;AACA,eAAO,KAAK,WAAW,MAAM,GAAG,EAAE,CAAC;AAAA,MACrC;AAEA,aAAO;AAAA,IACT;AAAA,EACF;AAGA,iBAAe,aAAa;AAAA,IAC1B;AAAA,IACA,CAAC,OAAO,cAAc;AACpB,aAAO;AAAA,QACL,MAAM,WAAW,OAAO,IAAI,qBAAqB;AAAA,MACnD;AAGA,aAAO,KAAK,SAAS;AAGrB,UAAI,UAAU,SAAS,IAAI,GAAG;AAC5B,cAAM,QAAQ,UAAU,MAAM,IAAI;AAClC,eAAO,KAAK,GAAG,KAAK;AAAA,MACtB;AAEA,aAAO;AAAA,IACT;AAAA,EACF;AAGA,QAAM,gBAAgB,gBAAgB,YAAY;AAGlD,QAAM,eAAe;AAAA,IACnB;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF;AAEA,aAAW,WAAW,cAAc;AAClC,UAAM,QAAQ,IAAI,OAAO,MAAM,OAAO,OAAO,GAAG;AAChD,QAAI,MAAM,KAAK,YAAY,GAAG;AAC5B,aAAO,KAAK,OAAO;AAAA,IACrB;AAAA,EACF;AAEA,SAAO,KAAK,GAAG,aAAa;AAG5B,QAAM,kBAAkB,CAAC;AACzB,aAAW,SAAS,QAAQ;AAE1B,QAAI,MAAM,WAAW,SAAS,GAAG;AAC/B,sBAAgB,KAAK,KAAK;AAC1B;AAAA,IACF;AAEA,oBAAgB,KAAK,KAAK;AAG1B,QAAI,MAAM,SAAS,GAAG,KAAK,MAAM,SAAS,GAAG,GAAG;AAC9C,sBAAgB,KAAK,MAAM,MAAM,GAAG,EAAE,CAAC;AAAA,IACzC;AAGA,QACE,MAAM,SAAS,GAAG,KAClB,CAAC,CAAC,MAAM,MAAM,MAAM,MAAM,IAAI,EAAE,SAAS,KAAK,GAC9C;AACA,sBAAgB,KAAK,MAAM,MAAM,GAAG,EAAE,CAAC;AAAA,IACzC;AAAA,EACF;AAGA,QAAM,cAAc,CAAC;AACrB,aAAW,SAAS,iBAAiB;AACnC,QAAI,mBAAmB,KAAK,GAAG;AAE7B,UACE,MAAM,WAAW,gBAAgB,KACjC,MAAM,WAAW,6BAA6B,GAC9C;AACA,oBAAY,KAAK,gBAAgB;AAGjC,cAAM,UAAU,mBAAmB,KAAK;AACxC,YAAI,aAAa;AAGjB,YAAI,WAAW,WAAW,GAAG,KAAK,WAAW,SAAS,GAAG,GAAG;AAC1D,uBAAa,WAAW,MAAM,GAAG,EAAE;AAAA,QACrC,WAAW,WAAW,WAAW,GAAG,KAAK,WAAW,SAAS,GAAG,GAAG;AACjE,uBAAa,WAAW,MAAM,GAAG,EAAE;AAAA,QACrC,WAAW,WAAW,WAAW,IAAI,KAAK,WAAW,WAAW,IAAI,GAAG;AAErE,uBAAa,WAAW,MAAM,GAAG,EAAE;AAAA,QACrC;AAGA,qBAAa,WAAW,QAAQ,eAAe,GAAG;AAGlD,YAAI,WAAW,KAAK,GAAG;AACrB,gBAAM,gBAAgB,gBAAgB,UAAU;AAChD,sBAAY,KAAK,GAAG,aAAa;AAAA,QACnC;AAAA,MACF,WAAW,MAAM,WAAW,gBAAgB,GAAG;AAC7C,oBAAY,KAAK,QAAQ;AAAA,MAC3B,WAAW,MAAM,WAAW,eAAe,GAAG;AAC5C,oBAAY,KAAK,OAAO;AAAA,MAC1B,WAAW,MAAM,WAAW,gBAAgB,GAAG;AAC7C,oBAAY,KAAK,QAAQ;AAAA,MAC3B,OAAO;AACL,oBAAY,KAAK,KAAK;AAAA,MACxB;AAAA,IACF,WAAW,oBAAoB,KAAK,GAAG;AAErC,kBAAY,KAAK,cAAc;AAG/B,YAAM,iBAAiB,oBAAoB,KAAK,EAC7C,QAAQ,MAAM,EAAE,EAChB,QAAQ,wBAAwB,EAAE;AAGrC,YAAM,gBAAgB,eACnB,MAAM,KAAK,EACX,OAAO,CAAC,SAAS,mBAAmB,KAAK,IAAI,CAAC,EAC9C,IAAI,CAAC,SAAS,KAAK,YAAY,CAAC;AAEnC,kBAAY,KAAK,GAAG,aAAa;AAAA,IACnC,OAAO;AACL,kBAAY,KAAK,KAAK;AAAA,IACxB;AAAA,EACF;AAEA,SAAO,CAAC,GAAG,IAAI,IAAI,WAAW,CAAC;AACjC;AASA,SAAS,WAAW,MAAM;AACxB,MAAI,SAAS,CAAC;AAGd,QAAM,sBAAsB,CAAC;AAC7B,MAAI,iBAAiB;AAGrB,QAAM,uBAAuB,KAAK,QAAQ,qBAAqB,CAAC,UAAU;AACxE,UAAM,cAAc,sBAAsB,gBAAgB;AAC1D,wBAAoB,WAAW,IAAI;AACnC,WAAO;AAAA,EACT,CAAC;AAGD,QAAM,kBAAkB,qBAAqB;AAAA,IAC3C;AAAA,IACA,CAAC,UAAU;AACT,YAAM,cAAc,qBAAqB,gBAAgB;AACzD,0BAAoB,WAAW,IAAI;AACnC,aAAO;AAAA,IACT;AAAA,EACF;AAGA,QAAM,qBAAqB,CAAC;AAC5B,MAAI,qBAAqB;AAGzB,QAAM,oBAAoB,gBAAgB,QAAQ,YAAY,CAAC,UAAU;AACvE,UAAM,cAAc,mBAAmB,oBAAoB;AAC3D,uBAAmB,WAAW,IAAI;AAClC,WAAO;AAAA,EACT,CAAC;AAGD,QAAM,iBAAiB,kBAAkB;AAAA,IACvC;AAAA,IACA,CAAC,UAAU;AACT,YAAM,cAAc,eAAe,oBAAoB;AACvD,yBAAmB,WAAW,IAAI;AAClC,aAAO;AAAA,IACT;AAAA,EACF;AAGA,QAAM,eAAe,eAAe,QAAQ,sBAAsB,CAAC,UAAU;AAC3E,UAAM,cAAc,aAAa,oBAAoB;AACrD,uBAAmB,WAAW,IAAI;AAClC,WAAO;AAAA,EACT,CAAC;AAID,QAAM,oBAAoB,aAAa;AAAA,IACrC;AAAA,IACA,CAAC,UAAU;AACT,YAAM,cAAc,mBAAmB,oBAAoB;AAC3D,yBAAmB,WAAW,IAAI;AAGlC,aAAO,KAAK,YAAY;AAGxB,YAAM,WAAW,MAAM,MAAM,GAAG,EAAE,EAAE,MAAM,GAAG;AAC7C,iBAAW,QAAQ,UAAU;AAC3B,YAAI,CAAC,KAAK,KAAK;AAAG;AAGlB,cAAM,CAAC,KAAK,WAAW,IAAI,KAAK,MAAM,GAAG;AACzC,YAAI,OAAO,aAAa;AACtB,iBAAO,KAAK,OAAO,GAAG,EAAE;AAGxB,gBAAM,QAAQ,YAAY,QAAQ,UAAU,EAAE;AAC9C,cAAI,OAAO;AAET,gBAAI,MAAM,SAAS,GAAG,GAAG;AACvB,oBAAM,aAAa,MAAM,MAAM,GAAG;AAClC,qBAAO,KAAK,GAAG,UAAU;AAAA,YAC3B,OAAO;AACL,qBAAO,KAAK,KAAK;AAAA,YACnB;AAAA,UACF;AAAA,QACF;AAAA,MACF;AAEA,aAAO;AAAA,IACT;AAAA,EACF;AAGA,QAAM,oBAAoB,kBAAkB,QAAQ,OAAO,CAAC,UAAU;AACpE,WAAO,KAAK,mBAAmB;AAC/B,WAAO;AAAA,EACT,CAAC;AAGD,QAAM,oBAAoB,kBAAkB;AAAA,IAC1C;AAAA,IACA,CAAC,UAAU;AACT,aAAO,KAAK,WAAW;AAGvB,YAAM,gBAAgB,MAAM,MAAM,+BAA+B;AACjE,UAAI,iBAAiB,cAAc,CAAC,GAAG;AACrC,eAAO,KAAK,cAAc,CAAC,CAAC;AAAA,MAC9B;AAEA,aAAO;AAAA,IACT;AAAA,EACF;AAGA,QAAM,gBAAgB,kBAAkB;AAAA,IACtC;AAAA,IACA,CAAC,UAAU;AACT,aAAO,KAAK,kBAAkB;AAG9B,YAAM,QAAQ,MAAM,MAAM,gBAAgB;AAC1C,UAAI,OAAO;AACT,mBAAW,WAAW,OAAO;AAE3B,gBAAM,cAAc,QAAQ,MAAM,GAAG,EAAE,EAAE,KAAK;AAC9C,gBAAM,aAAa,gBAAgB,WAAW;AAC9C,iBAAO,KAAK,GAAG,UAAU;AAAA,QAC3B;AAAA,MACF;AAEA,aAAO;AAAA,IACT;AAAA,EACF;AAGA,QAAM,eAAe,cAAc;AAAA,IACjC;AAAA,IACA,CAAC,UAAU;AACT,aAAO,KAAK,OAAO;AAGnB,YAAM,YAAY,MAAM,MAAM,kCAAkC;AAChE,UAAI,aAAa,UAAU,CAAC,GAAG;AAC7B,eAAO,KAAK,UAAU,CAAC,CAAC;AAAA,MAC1B;AAEA,aAAO;AAAA,IACT;AAAA,EACF;AAGA,QAAM,mBAAmB,aAAa;AAAA,IACpC;AAAA,IACA,CAAC,OAAO,aAAa;AACnB,aAAO,KAAK,kBAAkB;AAC9B,aAAO,KAAK,QAAQ;AAGpB,UAAI,MAAM,SAAS,QAAQ,GAAG;AAC5B,eAAO,KAAK,aAAa;AAGzB,cAAM,eAAe,MAAM;AAAA,UACzB;AAAA,QACF;AACA,YAAI,cAAc;AAChB,qBAAW,cAAc,cAAc;AACrC,kBAAM,QAAQ,WAAW,KAAK,EAAE,MAAM,KAAK;AAC3C,gBAAI,MAAM,UAAU,GAAG;AACrB,qBAAO,KAAK,MAAM,CAAC,CAAC;AACpB,qBAAO,KAAK,MAAM,CAAC,CAAC;AAAA,YACtB;AAAA,UACF;AAAA,QACF;AAAA,MACF,WAAW,MAAM,SAAS,WAAW,GAAG;AACtC,eAAO,KAAK,gBAAgB;AAG5B,cAAM,gBAAgB,MAAM;AAAA,UAC1B;AAAA,QACF;AACA,YAAI,eAAe;AACjB,qBAAW,eAAe,eAAe;AACvC,kBAAM,aAAa,YAAY,MAAM,0BAA0B;AAC/D,gBAAI,cAAc,WAAW,CAAC,GAAG;AAC/B,qBAAO,KAAK,WAAW,CAAC,CAAC;AAAA,YAC3B;AAAA,UACF;AAAA,QACF;AAAA,MACF;AAEA,aAAO;AAAA,IACT;AAAA,EACF;AAGA,MAAI,eAAe;AACnB,QAAM,aAAa;AAAA,IACjB;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF;AAEA,aAAW,WAAW,YAAY;AAChC,UAAM,QAAQ,IAAI,OAAO,MAAM,OAAO,WAAW,GAAG;AACpD,mBAAe,aAAa,QAAQ,OAAO,CAAC,UAAU;AACpD,aAAO,KAAK,WAAW,OAAO,EAAE;AAChC,aAAO;AAAA,IACT,CAAC;AAAA,EACH;AAGA,QAAM,aAAa;AAAA,IACjB;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF;AAEA,aAAW,WAAW,YAAY;AAChC,UAAM,QAAQ,IAAI,OAAO,MAAM,OAAO,OAAO,GAAG;AAChD,QAAI,MAAM,KAAK,YAAY,GAAG;AAC5B,aAAO,KAAK,OAAO;AAAA,IACrB;AAAA,EACF;AAGA,QAAM,gBAAgB,gBAAgB,YAAY;AAClD,SAAO,KAAK,GAAG,aAAa;AAG5B,QAAM,kBAAkB,CAAC;AACzB,aAAW,SAAS,QAAQ;AAE1B,QAAI,MAAM,WAAW,OAAO,GAAG;AAC7B,sBAAgB,KAAK,KAAK;AAC1B;AAAA,IACF;AAEA,oBAAgB,KAAK,KAAK;AAG1B,QAAI,aAAa,KAAK,KAAK,GAAG;AAC5B,YAAM,QAAQ,MACX,QAAQ,mBAAmB,OAAO,EAClC,YAAY,EACZ,MAAM,GAAG;AACZ,UAAI,MAAM,SAAS,GAAG;AACpB,wBAAgB,KAAK,GAAG,KAAK;AAAA,MAC/B;AAAA,IACF;AAAA,EACF;AAGA,QAAM,cAAc,CAAC;AACrB,aAAW,SAAS,iBAAiB;AACnC,QAAI,mBAAmB,KAAK,GAAG;AAC7B,UAAI,MAAM,WAAW,cAAc,GAAG;AACpC,oBAAY,KAAK,gBAAgB;AAGjC,cAAM,UAAU,mBAAmB,KAAK;AAExC,cAAM,aAAa,QAAQ,MAAM,GAAG,EAAE;AAGtC,YAAI,WAAW,KAAK,EAAE,SAAS,GAAG;AAChC,gBAAM,gBAAgB,gBAAgB,UAAU;AAChD,sBAAY,KAAK,GAAG,aAAa;AAAA,QACnC;AAAA,MACF,WAAW,MAAM,WAAW,kBAAkB,GAAG;AAC/C,oBAAY,KAAK,oBAAoB;AAGrC,cAAM,UAAU,mBAAmB,KAAK;AAExC,cAAM,aAAa,QAAQ,MAAM,GAAG,EAAE;AAGtC,YAAI,WAAW,SAAS,IAAI,GAAG;AAE7B,gBAAM,QAAQ,WAAW,MAAM,IAAI;AACnC,qBAAW,QAAQ,OAAO;AACxB,gBAAI,KAAK,KAAK,GAAG;AACf,oBAAM,aAAa,gBAAgB,KAAK,KAAK,CAAC;AAC9C,0BAAY,KAAK,GAAG,UAAU;AAAA,YAChC;AAAA,UACF;AAAA,QACF,WAAW,WAAW,KAAK,GAAG;AAC5B,gBAAM,gBAAgB,gBAAgB,UAAU;AAChD,sBAAY,KAAK,GAAG,aAAa;AAAA,QACnC;AAAA,MACF,WAAW,MAAM,WAAW,kBAAkB,GAAG;AAC/C,oBAAY,KAAK,YAAY;AAAA,MAC/B,WAAW,MAAM,WAAW,YAAY,GAAG;AACzC,oBAAY,KAAK,cAAc;AAAA,MACjC,OAAO;AACL,oBAAY,KAAK,KAAK;AAAA,MACxB;AAAA,IACF,WAAW,oBAAoB,KAAK,GAAG;AAErC,kBAAY,KAAK,cAAc;AAG/B,YAAM,iBAAiB,oBAAoB,KAAK,EAC7C,QAAQ,gBAAgB,EAAE,EAC1B,QAAQ,UAAU,EAAE;AAGvB,YAAM,gBAAgB,eACnB,MAAM,KAAK,EACX,OAAO,CAAC,SAAS,mBAAmB,KAAK,IAAI,CAAC,EAC9C,IAAI,CAAC,SAAS,KAAK,YAAY,CAAC;AAEnC,kBAAY,KAAK,GAAG,aAAa;AAAA,IACnC,OAAO;AACL,kBAAY,KAAK,KAAK;AAAA,IACxB;AAAA,EACF;AAEA,SAAO,CAAC,GAAG,IAAI,IAAI,WAAW,CAAC;AACjC;AAQO,SAAS,KAAK,MAAM;AAEzB,MAAI,OAAO,SAAS;AAAU,WAAO;AACrC,QAAM,YAAY,KAAK,YAAY;AAGnC,MAAI,UAAU,UAAU;AAAG,WAAO;AAGlC,MAAI,UAAU,SAAS,KAAK,GAAG;AAE7B,UAAM,UAAU,UAAU,MAAM,GAAG,EAAE;AACrC,QAAI,QAAQ,SAAS;AAAG,aAAO;AAC/B,WAAO;AAAA,EACT;AAEA,MAAI,UAAU,SAAS,IAAI,GAAG;AAE5B,UAAM,UAAU,UAAU,MAAM,GAAG,EAAE;AACrC,QAAI,QAAQ,SAAS;AAAG,aAAO;AAC/B,WAAO;AAAA,EACT;AAEA,MAAI,UAAU,SAAS,GAAG,KAAK,CAAC,UAAU,SAAS,IAAI,GAAG;AAExD,WAAO,UAAU,MAAM,GAAG,EAAE;AAAA,EAC9B;AAEA,MAAI,UAAU,SAAS,IAAI,GAAG;AAE5B,WAAO,UAAU,MAAM,GAAG,EAAE;AAAA,EAC9B;AAEA,MAAI,UAAU,SAAS,IAAI,GAAG;AAE5B,WAAO,UAAU,MAAM,GAAG,EAAE;AAAA,EAC9B;AAEA,MAAI,UAAU,SAAS,IAAI,GAAG;AAE5B,WAAO,UAAU,MAAM,GAAG,EAAE;AAAA,EAC9B;AAGA,SAAO;AACT;;;ACjiFA;AAJA,SAAS,MAAMC,eAAc;AAC7B,OAAO,YAAY;AACnB,OAAO,UAAU;AACjB,YAAYC,YAAW;AAGvB;;;ACLA;AADA,YAAY,WAAW;AAUvB,eAAsB,SAAS,SAAS,UAAU;AAEhD,MAAI,CAAC,WAAW,QAAQ,KAAK,MAAM,IAAI;AACrC,YAAQ,KAAK,yCAAyC;AACtD,WAAO;AAAA,EACT;AAGA,QAAM,qBAAqB,SAAS,YAAY;AAGhD,MACE,CAAC,cAAc,cAAc,MAAM,MAAM,OAAO,KAAK,EAAE;AAAA,IACrD;AAAA,EACF,GACA;AACA,QAAI;AAEF,YAAM,UAAU;AAAA,QACd,aAAa;AAAA,QACb,YAAY;AAAA,QACZ,WAAW;AAAA,QACX,QAAQ;AAAA;AAAA,QAER,2BAA2B;AAAA,QAC3B,6BAA6B;AAAA,QAC7B,eAAe;AAAA,QACf,4BAA4B;AAAA,QAC5B,yBAAyB;AAAA,MAC3B;AAGA,YAAM,MAAY,YAAM,SAAS,OAAO;AACxC,aAAO;AAAA,IACT,SAAS,OAAO;AACd,cAAQ,MAAM,iBAAiB,kBAAkB,UAAU,MAAM,OAAO;AAExE,aAAO;AAAA,QACL,OAAO;AAAA,QACP,SAAS,MAAM;AAAA,QACf,UAAU,MAAM;AAAA,QAChB,MAAM;AAAA,MACR;AAAA,IACF;AAAA,EACF,OAAO;AAEL,YAAQ;AAAA,MACN,2CAA2C,kBAAkB;AAAA,IAC/D;AACA,WAAO;AAAA,EACT;AACF;AAQO,SAAS,0BAA0B,KAAK;AAE7C,MAAI,CAAC,OAAO,IAAI,OAAO;AACrB,WAAO,EAAE,UAAU,CAAC,GAAG,YAAY,EAAE;AAAA,EACvC;AAGA,QAAM,SAAS;AAAA,IACb,UAAU,CAAC;AAAA,IACX,YAAY;AAAA;AAAA,EACd;AAGA,MAAI,kBAAkB;AACtB,MAAI,sBAAsB;AAG1B,QAAM,eAAe,oBAAI,QAAQ;AAGjC,WAAS,MAAM,MAAM,aAAa,MAAM,eAAe,UAAU;AAE/D,QAAI,CAAC,QAAQ,aAAa,IAAI,IAAI,GAAG;AACnC;AAAA,IACF;AAEA,iBAAa,IAAI,IAAI;AAGrB,QAAI,OAAO,SAAS,UAAU;AAC5B;AAAA,IACF;AAGA,UAAM,OAAO,KAAK,KAAK,OAAO;AAG9B,YAAQ,KAAK,MAAM;AAAA,MAEjB,KAAK;AACH,eAAO,SAAS,KAAK;AAAA,UACnB,MAAM;AAAA,UACN,WAAW;AAAA,UACX;AAAA,UACA,SAAS;AAAA,QACX,CAAC;AACD,eAAO;AACP;AAAA,MAEF,KAAK;AAAA,MACL,KAAK;AAAA,MACL,KAAK;AACH,eAAO,SAAS,KAAK;AAAA,UACnB,MAAM;AAAA,UACN,WAAW;AAAA,UACX;AAAA,UACA,SAAS;AAAA,QACX,CAAC;AACD,eAAO;AACP;AAAA,MAEF,KAAK;AAAA,MACL,KAAK;AACH,eAAO,SAAS,KAAK;AAAA,UACnB,MAAM;AAAA,UACN,WAAW;AAAA,UACX;AAAA,UACA,SAAS;AAAA,QACX,CAAC;AACD,eAAO;AACP;AAAA,MAEF,KAAK;AACH,eAAO,SAAS,KAAK;AAAA,UACnB,MAAM;AAAA,UACN,WAAW;AAAA,UACX;AAAA,UACA,SAAS;AAAA,QACX,CAAC;AAED,cAAM,YAAY,KAAK,OAAO,UAAU;AACxC,eAAO,cAAc,YAAY,IAAI,YAAY,IAAI;AACrD;AAAA,MAEF,KAAK;AACH,eAAO,SAAS,KAAK;AAAA,UACnB,MAAM;AAAA,UACN,WAAW;AAAA,UACX;AAAA,UACA,SAAS;AAAA,QACX,CAAC;AAED;AAAA,MAEF,KAAK;AACH,eAAO,SAAS,KAAK;AAAA,UACnB,MAAM;AAAA,UACN,WAAW;AAAA,UACX;AAAA,UACA,SAAS;AAAA,QACX,CAAC;AACD,eAAO;AACP;AAAA,MAEF,KAAK;AACH,YAAI,KAAK,aAAa,QAAQ,KAAK,aAAa,MAAM;AACpD,iBAAO,SAAS,KAAK;AAAA,YACnB,MAAM;AAAA,YACN,WAAW;AAAA,YACX,UAAU,KAAK;AAAA,YACf;AAAA,YACA,SAAS;AAAA,UACX,CAAC;AACD,iBAAO;AAAA,QACT;AACA;AAAA,MAGF,KAAK;AACH,eAAO,SAAS,KAAK;AAAA,UACnB,MAAM;AAAA,UACN,MAAM,KAAK,IAAI,QAAQ;AAAA,UACvB,QAAQ,KAAK,QAAQ,UAAU;AAAA,UAC/B;AAAA,UACA,OAAO,KAAK,SAAS;AAAA,UACrB,WAAW,KAAK,aAAa;AAAA,QAC/B,CAAC;AAED,uBAAe,KAAK,IAAI,QAAQ;AAChC;AAAA,MAEF,KAAK;AAAA,MACL,KAAK;AACH,eAAO,SAAS,KAAK;AAAA,UACnB,MAAM;AAAA,UACN,MAAM,KAAK,IAAI,QAAQ;AAAA,UACvB,QAAQ,KAAK,QAAQ,UAAU;AAAA,UAC/B;AAAA,UACA,OAAO,KAAK,SAAS;AAAA,UACrB,WACE,KAAK,SAAS,uBACV,KAAK,aAAa,QAClB;AAAA,UACN,OAAO,KAAK,SAAS;AAAA,QACvB,CAAC;AAED,uBAAe,KAAK,IAAI,QAAQ;AAChC;AAAA,MAGF,KAAK;AACH,YAAI,WAAW;AAGf,YAAI,KAAK,OAAO,SAAS,cAAc;AACrC,qBAAW,KAAK,OAAO;AAAA,QACzB,WAAW,KAAK,OAAO,SAAS,oBAAoB;AAElD,cACE,KAAK,OAAO,YACZ,KAAK,OAAO,SAAS,SAAS,cAC9B;AACA,uBAAW,KAAK,OAAO,SAAS;AAEhC,gBACE,KAAK,OAAO,UACZ,KAAK,OAAO,OAAO,SAAS,cAC5B;AACA,yBAAW,GAAG,KAAK,OAAO,OAAO,IAAI,IAAI,QAAQ;AAAA,YACnD;AAAA,UACF;AAAA,QACF;AAEA,eAAO,SAAS,KAAK;AAAA,UACnB,MAAM;AAAA,UACN,MAAM;AAAA,UACN,WAAW,KAAK,WAAW,UAAU;AAAA,UACrC;AAAA,QACF,CAAC;AACD;AAAA,MAGF,KAAK;AAEH,aAAK,aAAa,QAAQ,CAAC,eAAe;AACxC,cAAI,WAAW,MAAM,WAAW,GAAG,SAAS,cAAc;AACxD,mBAAO,SAAS,KAAK;AAAA,cACnB,MAAM;AAAA,cACN,MAAM,WAAW,GAAG;AAAA,cACpB,MAAM,KAAK;AAAA;AAAA,cACX,OAAO;AAAA,cACP;AAAA,cACA,aAAa,WAAW,SAAS;AAAA,YACnC,CAAC;AAAA,UACH;AAAA,QACF,CAAC;AACD;AAAA,MAGF,KAAK;AACH,eAAO,SAAS,KAAK;AAAA,UACnB,MAAM;AAAA,UACN,MAAM,KAAK,IAAI,QAAQ;AAAA,UACvB,SAAS,KAAK,aAAa,KAAK,WAAW,QAAQ,YAAY;AAAA,UAC/D;AAAA,QACF,CAAC;AACD;AAAA,MAGF,KAAK;AACH,eAAO,SAAS,KAAK;AAAA,UACnB,MAAM;AAAA,UACN,QAAQ,KAAK,QAAQ;AAAA,UACrB;AAAA,QACF,CAAC;AACD;AAAA,MAEF,KAAK;AAAA,MACL,KAAK;AACH,eAAO,SAAS,KAAK;AAAA,UACnB,MAAM;AAAA,UACN,SAAS,KAAK,SAAS;AAAA,UACvB;AAAA,QACF,CAAC;AACD;AAAA,IACJ;AAGA,QAAI,KAAK,SAAS,kBAAkB;AAClC;AACA,wBAAkB,KAAK,IAAI,iBAAiB,mBAAmB;AAAA,IACjE;AAGA,eAAW,OAAO,MAAM;AACtB,YAAM,QAAQ,KAAK,GAAG;AAGtB,UACE,QAAQ,UACR,QAAQ,SACR,QAAQ,WACR,QAAQ,YACR,QAAQ,qBACR,QAAQ,oBACR;AACA;AAAA,MACF;AAEA,UAAI,MAAM,QAAQ,KAAK,GAAG;AAExB,mBAAW,QAAQ,OAAO;AACxB,gBAAM,MAAM,MAAM,YAAY;AAAA,QAChC;AAAA,MACF,WAAW,SAAS,OAAO,UAAU,UAAU;AAE7C,cAAM,OAAO,MAAM,YAAY;AAAA,MACjC;AAAA,IACF;AAGA,QAAI,KAAK,SAAS,kBAAkB;AAClC;AAAA,IACF;AAAA,EACF;AAGA,QAAM,GAAG;AAGT,SAAO,SAAS,KAAK;AAAA,IACnB,MAAM;AAAA,IACN,MAAM;AAAA,IACN,OAAO;AAAA,EACT,CAAC;AAED,SAAO;AACT;;;AD3UA,SAAS,qBAAqB,SAAS;AACrC,SAAO,OAAO,WAAW,QAAQ,EAAE,OAAO,OAAO,EAAE,OAAO,KAAK;AACjE;AAQA,SAAS,gBAAgB,UAAU;AACjC,SAAO,KAAK,SAAS,QAAQ;AAC/B;AASA,SAAS,eAAe,UAAU,cAAc;AAC9C,MAAI,cAAc;AAChB,WAAO,aAAa,YAAY;AAAA,EAClC;AAEA,QAAM,YAAY,KAAK,QAAQ,QAAQ,EAAE,YAAY;AAErD,QAAM,eAAe;AAAA,IACnB,OAAO;AAAA,IACP,QAAQ;AAAA,IACR,OAAO;AAAA,IACP,QAAQ;AAAA,IACR,OAAO;AAAA,IACP,OAAO;AAAA,IACP,SAAS;AAAA,IACT,OAAO;AAAA,IACP,OAAO;AAAA,IACP,QAAQ;AAAA,IACR,MAAM;AAAA,IACN,QAAQ;AAAA,IACR,MAAM;AAAA,IACN,QAAQ;AAAA,IACR,OAAO;AAAA,IACP,UAAU;AAAA,IACV,OAAO;AAAA,IACP,SAAS;AAAA,IACT,QAAQ;AAAA,IACR,SAAS;AAAA,IACT,SAAS;AAAA,IACT,OAAO;AAAA,IACP,QAAQ;AAAA,IACR,SAAS;AAAA,IACT,QAAQ;AAAA,EACV;AAEA,SAAO,aAAa,SAAS,KAAK;AACpC;AASA,SAAS,oBAAoB,SAAS,UAAU;AAC9C,QAAM,QAAQ,QAAQ,UAAU,GAAG,QAAQ,EAAE,MAAM,IAAI;AACvD,SAAO,MAAM;AACf;AASA,SAAS,yBAAyB,SAAS,UAAU;AACnD,QAAM,WAAW,CAAC;AAGlB,QAAM,WAAW;AAAA;AAAA,IAEf,UAAU;AAAA,MACR,QAAQ;AAAA,MACR,MAAM;AAAA,MACN,MAAM;AAAA,MACN,IAAI;AAAA,MACJ,KAAK;AAAA,MACL,SAAS;AAAA,IACX;AAAA;AAAA,IAEA,OAAO;AAAA,MACL,QAAQ;AAAA,MACR,MAAM;AAAA,MACN,MAAM;AAAA,MACN,IAAI;AAAA,MACJ,KAAK;AAAA,MACL,SACE;AAAA,IACJ;AAAA;AAAA,IAEA,UAAU;AAAA,MACR,QAAQ;AAAA,MACR,MAAM;AAAA,MACN,MAAM;AAAA,MACN,IAAI;AAAA,MACJ,KAAK;AAAA,MACL,SAAS;AAAA,IACX;AAAA,EACF;AAGA,QAAM,kBACJ,SAAS,SAAS,QAAQ,KAAK,SAAS,SAAS;AACnD,MAAI;AACJ,UAAQ,QAAQ,gBAAgB,KAAK,OAAO,OAAO,MAAM;AACvD,UAAM,OAAO,MAAM,CAAC,KAAK,MAAM,CAAC;AAChC,UAAM,gBAAgB,MAAM;AAI5B,UAAM,YAAY,oBAAoB,SAAS,aAAa;AAC5D,QAAI,UAAU,YAAY;AAE1B,aAAS,KAAK;AAAA,MACZ,MAAM;AAAA,MACN;AAAA,MACA,gBAAgB;AAAA,MAChB,YAAY;AAAA,MACZ,UAAU;AAAA;AAAA,MACV,aAAa,QAAQ;AAAA,QACnB;AAAA,QACA,gBAAgB,MAAM,CAAC,EAAE,SAAS;AAAA,MACpC;AAAA;AAAA,IACF,CAAC;AAAA,EACH;AAGA,QAAM,eAAe,SAAS,MAAM,QAAQ,KAAK,SAAS,MAAM;AAChE,UAAQ,QAAQ,aAAa,KAAK,OAAO,OAAO,MAAM;AACpD,UAAM,OAAO,MAAM,CAAC;AACpB,UAAM,gBAAgB,MAAM;AAC5B,UAAM,YAAY,oBAAoB,SAAS,aAAa;AAC5D,QAAI,UAAU,YAAY;AAE1B,aAAS,KAAK;AAAA,MACZ,MAAM;AAAA,MACN;AAAA,MACA,gBAAgB;AAAA,MAChB,YAAY;AAAA,MACZ,UAAU;AAAA;AAAA,MACV,aAAa,QAAQ;AAAA,QACnB;AAAA,QACA,gBAAgB,MAAM,CAAC,EAAE,SAAS;AAAA,MACpC;AAAA;AAAA,IACF,CAAC;AAAA,EACH;AAIA,SAAO;AACT;AASA,SAAS,uBAAuB,KAAK,SAAS;AAC5C,QAAM,WAAW,CAAC;AAClB,QAAM,gBAAgB,CAAC;AACvB,QAAM,QAAQ,oBAAI,IAAI;AAGtB,QAAM,eAAe,oBAAI,QAAQ;AAgBjC,WAAS,aACP,MACA,MACA,eACA,aACA,WACA,SACA,YACA,eAAe,MACf,iBAAiB,CAAC,GAClB;AACA,UAAM,SAAS;AAAA,MACb;AAAA,MACA;AAAA,MACA,gBAAgB;AAAA,MAChB,cAAc;AAAA,MACd,YAAY;AAAA,MACZ,UAAU;AAAA,MACV,aAAa;AAAA,MACb,iBAAiB;AAAA,IACnB;AAEA,aAAS,KAAK,MAAM;AAGpB,QAAI,cAAc;AAChB,oBAAc,KAAK;AAAA,QACjB,QAAQ;AAAA,QACR,QAAQ;AAAA,QACR,MAAM;AAAA,MACR,CAAC;AAAA,IACH;AAEA,WAAO;AAAA,EACT;AAKA,WAAS,MAAM,MAAM,aAAa,MAAM,eAAe,MAAM,QAAQ,MAAM;AACzE,QAAI,CAAC,QAAQ,OAAO,SAAS,YAAY,aAAa,IAAI,IAAI,GAAG;AAC/D;AAAA,IACF;AAEA,iBAAa,IAAI,IAAI;AAGrB,QAAI,CAAC,KAAK,KAAK;AACb;AAAA,IACF;AAGA,UAAM,YAAY,KAAK,KAAK,OAAO;AACnC,UAAM,UAAU,KAAK,KAAK,KAAK;AAC/B,UAAM,gBAAgB,KAAK;AAC3B,UAAM,cAAc,KAAK;AACzB,UAAM,aAAa,QAAQ,UAAU,eAAe,WAAW;AAG/D,QAAI,gBAAgB;AAGpB,YAAQ,KAAK,MAAM;AAAA,MACjB,KAAK,uBAAuB;AAC1B,cAAM,OAAO,KAAK,IAAI,QAAQ;AAC9B,cAAM,SAAS,KAAK,QAAQ;AAAA,UAAI,CAAC,MAC/B,EAAE,SAAS,eAAe,EAAE,OAAO;AAAA,QACrC;AAEA,wBAAgB;AAAA,UACd;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,YACE,eAAe,KAAK;AAAA,YACpB,QAAQ,UAAU,CAAC;AAAA,YACnB,UAAU,KAAK,SAAS;AAAA,YACxB,cAAc,KAAK,aAAa;AAAA,UAClC;AAAA,QACF;AAGA,cAAM,IAAI,MAAM,aAAa;AAG7B,YAAI,KAAK,MAAM;AACb,gBAAM,KAAK,MAAM,MAAM,eAAe,IAAI;AAAA,QAC5C;AACA;AAAA,MACF;AAAA,MAEA,KAAK;AAAA,MACL,KAAK,2BAA2B;AAE9B,YAAI,OAAO;AACX,YAAI,eAAe;AAEnB,YACE,cACA,WAAW,SAAS,wBACpB,WAAW,IACX;AACA,iBAAO,WAAW,GAAG;AACrB,yBAAe;AAAA,QACjB,WACE,cACA,WAAW,SAAS,0BACpB,WAAW,MACX;AACA,cAAI,WAAW,KAAK,SAAS,cAAc;AACzC,mBAAO,WAAW,KAAK;AACvB,2BAAe;AAAA,UACjB,WACE,WAAW,KAAK,SAAS,sBACzB,WAAW,KAAK,UAChB;AACA,mBAAO,WAAW,KAAK,SAAS;AAChC,2BAAe;AAAA,UACjB;AAAA,QACF,WACE,cACA,WAAW,SAAS,cACpB,WAAW,KACX;AACA,iBAAO,WAAW,IAAI,QAAQ,WAAW,IAAI,SAAS;AACtD,yBAAe;AAAA,QACjB,WACE,cACA,WAAW,SAAS,sBACpB,WAAW,KACX;AACA,iBAAO,WAAW,IAAI,QAAQ;AAC9B,yBAAe;AAAA,QACjB;AAEA,cAAM,SAAS,KAAK,QAAQ;AAAA,UAAI,CAAC,MAC/B,EAAE,SAAS,eAAe,EAAE,OAAO;AAAA,QACrC;AAEA,wBAAgB;AAAA,UACd;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,YACE,eAAe,KAAK;AAAA,YACpB,QAAQ,UAAU,CAAC;AAAA,YACnB,UAAU,KAAK,SAAS;AAAA,YACxB,cAAc,KAAK,aAAa;AAAA,YAChC,UAAU,KAAK,SAAS;AAAA,UAC1B;AAAA,QACF;AAEA,cAAM,IAAI,MAAM,aAAa;AAG7B,YAAI,KAAK,MAAM;AACb,gBAAM,KAAK,MAAM,MAAM,eAAe,IAAI;AAAA,QAC5C;AACA;AAAA,MACF;AAAA,MAEA,KAAK,oBAAoB;AACvB,cAAM,OAAO,KAAK,IAAI,QAAQ;AAE9B,wBAAgB;AAAA,UACd;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,YACE,eAAe,KAAK;AAAA,UACtB;AAAA,QACF;AAEA,cAAM,IAAI,MAAM,aAAa;AAG7B,YAAI,KAAK,YAAY;AACnB,cAAI,KAAK,WAAW,SAAS,cAAc;AACzC,0BAAc,KAAK;AAAA,cACjB,QAAQ;AAAA,cACR,QAAQ,EAAE,MAAM,KAAK,WAAW,MAAM,MAAM,QAAQ;AAAA,cACpD,MAAM;AAAA,YACR,CAAC;AAAA,UACH;AAAA,QACF;AAGA,YAAI,KAAK,MAAM;AACb,gBAAM,KAAK,MAAM,MAAM,eAAe,IAAI;AAAA,QAC5C;AACA;AAAA,MACF;AAAA,MAEA,KAAK,mBAAmB;AAEtB,YAAI,OAAO,KAAK,IAAI,QAAQ;AAC5B,YACE,cACA,WAAW,SAAS,wBACpB,WAAW,IACX;AACA,iBAAO,WAAW,GAAG;AAAA,QACvB;AAEA,wBAAgB;AAAA,UACd;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,YACE,eAAe,KAAK;AAAA,UACtB;AAAA,QACF;AAEA,cAAM,IAAI,MAAM,aAAa;AAG7B,YAAI,KAAK,YAAY;AACnB,cAAI,KAAK,WAAW,SAAS,cAAc;AACzC,0BAAc,KAAK;AAAA,cACjB,QAAQ;AAAA,cACR,QAAQ,EAAE,MAAM,KAAK,WAAW,MAAM,MAAM,QAAQ;AAAA,cACpD,MAAM;AAAA,YACR,CAAC;AAAA,UACH;AAAA,QACF;AAGA,YAAI,KAAK,MAAM;AACb,gBAAM,KAAK,MAAM,MAAM,eAAe,IAAI;AAAA,QAC5C;AACA;AAAA,MACF;AAAA,MAEA,KAAK,oBAAoB;AACvB,cAAM,OAAO,KAAK,KAAK,QAAQ,KAAK,KAAK,SAAS;AAClD,cAAM,OAAO,KAAK,QAAQ;AAE1B,wBAAgB;AAAA,UACd,SAAS,gBAAgB,gBAAgB;AAAA,UACzC;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,YACE,eAAe,KAAK;AAAA,YACpB;AAAA,YACA,WAAW,CAAC,CAAC,KAAK;AAAA,YAClB,UAAU,KAAK,OAAO,SAAS;AAAA,YAC/B,cAAc,KAAK,OAAO,aAAa;AAAA,UACzC;AAAA,QACF;AAEA,cAAM,IAAI,MAAM,aAAa;AAG7B,YAAI,KAAK,OAAO;AACd,gBAAM,KAAK,OAAO,MAAM,eAAe,IAAI;AAAA,QAC7C;AACA;AAAA,MACF;AAAA,MAEA,KAAK,uBAAuB;AAE1B,aAAK,aAAa,QAAQ,CAAC,eAAe;AACxC,gBAAM,YAAY,MAAM,cAAc,KAAK;AAAA,QAC7C,CAAC;AACD;AAAA,MACF;AAAA,MAEA,KAAK,sBAAsB;AACzB,YAAI,KAAK,MAAM,KAAK,GAAG,SAAS,cAAc;AAC5C,gBAAM,OAAO,KAAK,GAAG;AAIrB,cAAI,qBAAqB;AACzB,cAAI,aAAa;AAEjB,cAAI,CAAC,KAAK,MAAM;AACd,iCAAqB;AAAA,UACvB,WACE;AAAA,YACE;AAAA,YACA;AAAA,YACA;AAAA,YACA;AAAA,YACA;AAAA,UACF,EAAE,SAAS,KAAK,KAAK,IAAI,GACzB;AACA,iCAAqB;AACrB,gBAAI,KAAK,KAAK,SAAS,oBAAoB;AACzC,2BAAa;AAAA,YACf;AAAA,UACF,WACE,KAAK,KAAK,SAAS,aACnB,OAAO,KAAK,KAAK,UAAU,UAC3B;AACA,iCAAqB;AACrB,yBAAa;AAAA,UACf,WAAW,gBAAgB,aAAa,SAAS,YAAY;AAE3D,iCAAqB;AAAA,UACvB;AAEA,cAAI,oBAAoB;AACtB,4BAAgB;AAAA,cACd;AAAA,cACA;AAAA,cACA;AAAA,cACA;AAAA,cACA;AAAA,cACA;AAAA,cACA;AAAA,cACA;AAAA,cACA;AAAA,gBACE,eAAe,KAAK;AAAA,gBACpB,eAAe,YAAY,QAAQ;AAAA;AAAA,cACrC;AAAA,YACF;AAEA,kBAAM,IAAI,MAAM,aAAa;AAAA,UAC/B;AAAA,QACF;AAGA,YAAI,KAAK,MAAM;AACb,gBAAM,KAAK,MAAM,MAAM,gBAAgB,eAAe,KAAK;AAAA,QAC7D;AACA;AAAA,MACF;AAAA,MAEA,KAAK,qBAAqB;AAExB,cAAM,SAAS,KAAK,OAAO;AAC3B,cAAM,aAAa,KAAK,WAAW,IAAI,CAAC,cAAc;AACpD,cAAI,UAAU,SAAS,0BAA0B;AAC/C,mBAAO,EAAE,MAAM,WAAW,MAAM,UAAU,MAAM,KAAK;AAAA,UACvD,WAAW,UAAU,SAAS,4BAA4B;AACxD,mBAAO,EAAE,MAAM,aAAa,MAAM,UAAU,MAAM,KAAK;AAAA,UACzD,OAAO;AACL,mBAAO;AAAA,cACL,MAAM;AAAA,cACN,MAAM,UAAU,MAAM;AAAA,cACtB,UAAU,UAAU,UAAU,QAAQ,UAAU,MAAM;AAAA,YACxD;AAAA,UACF;AAAA,QACF,CAAC;AAED,wBAAgB;AAAA,UACd;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,YACE,eAAe,KAAK;AAAA,YACpB;AAAA,UACF;AAAA,QACF;AAEA,cAAM,IAAI,MAAM,aAAa;AAG7B,mBAAW,QAAQ,CAAC,SAAS;AAC3B,wBAAc,KAAK;AAAA,YACjB,QAAQ;AAAA,YACR,QAAQ,EAAE,MAAM,KAAK,MAAM,MAAM,WAAW;AAAA,YAC5C,MAAM;AAAA,YACN,UAAU;AAAA,cACR,eAAe;AAAA,cACf,aAAa,KAAK;AAAA,cAClB,eAAe,KAAK;AAAA,YACtB;AAAA,UACF,CAAC;AAAA,QACH,CAAC;AAED;AAAA,MACF;AAAA,MAEA,KAAK,0BAA0B;AAE7B,YAAI,OAAO;AACX,YAAI,KAAK,aAAa;AACpB,cACE,KAAK,YAAY,SAAS,yBAC1B,KAAK,YAAY,SAAS,oBAC1B;AACA,mBAAO,KAAK,YAAY,IAAI,QAAQ;AAAA,UACtC,WACE,KAAK,YAAY,SAAS,yBAC1B,KAAK,YAAY,aAAa,SAAS,GACvC;AACA,mBAAO,KAAK,YAAY,aAAa,CAAC,EAAE,IAAI,QAAQ;AAAA,UACtD;AAAA,QACF,WAAW,KAAK,cAAc,KAAK,WAAW,SAAS,GAAG;AACxD,iBAAO,KAAK,WACT,IAAI,CAAC,MAAM,EAAE,UAAU,QAAQ,EAAE,OAAO,QAAQ,WAAW,EAC3D,KAAK,GAAG;AAAA,QACb;AAEA,wBAAgB;AAAA,UACd;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,YACE,eAAe,KAAK;AAAA,YACpB,QAAQ,KAAK,QAAQ;AAAA,UACvB;AAAA,QACF;AAEA,cAAM,IAAI,MAAM,aAAa;AAG7B,YAAI,KAAK,aAAa;AACpB,gBAAM,KAAK,aAAa,MAAM,cAAc,KAAK;AAAA,QACnD;AAGA,YAAI,KAAK,YAAY;AACnB,eAAK,WAAW,QAAQ,CAAC,SAAS;AAChC,gBAAI,KAAK,SAAS,KAAK,UAAU;AAC/B,4BAAc,KAAK;AAAA,gBACjB,QAAQ;AAAA,gBACR,QAAQ,EAAE,MAAM,KAAK,MAAM,MAAM,MAAM,WAAW;AAAA,gBAClD,MAAM;AAAA,gBACN,UAAU;AAAA,kBACR,aAAa,KAAK,SAAS;AAAA,kBAC3B,eAAe,KAAK,QAAQ;AAAA,gBAC9B;AAAA,cACF,CAAC;AAAA,YACH;AAAA,UACF,CAAC;AAAA,QACH;AAEA;AAAA,MACF;AAAA,MAEA,KAAK,4BAA4B;AAE/B,YAAI,OAAO;AACX,YAAI,KAAK,aAAa;AACpB,cACE,KAAK,YAAY,SAAS,yBAC1B,KAAK,YAAY,SAAS,oBAC1B;AACA,mBAAO,KAAK,YAAY,IAAI,QAAQ;AAAA,UACtC,WAAW,KAAK,YAAY,SAAS,cAAc;AACjD,mBAAO,KAAK,YAAY;AAAA,UAC1B;AAAA,QACF;AAEA,wBAAgB;AAAA,UACd;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,YACE,eAAe,KAAK;AAAA,YACpB,YAAY;AAAA,UACd;AAAA,QACF;AAEA,cAAM,IAAI,MAAM,aAAa;AAG7B,YAAI,KAAK,aAAa;AACpB,gBAAM,KAAK,aAAa,MAAM,cAAc,KAAK;AAAA,QACnD;AAGA,sBAAc,KAAK;AAAA,UACjB,QAAQ;AAAA,UACR,QAAQ,EAAE,MAAY,MAAM,WAAW;AAAA,UACvC,MAAM;AAAA,UACN,UAAU,EAAE,YAAY,KAAK;AAAA,QAC/B,CAAC;AAED;AAAA,MACF;AAAA,MAEA,KAAK,wBAAwB;AAE3B,cAAM,OAAO,KAAK,IAAI,QAAQ;AAE9B,wBAAgB;AAAA,UACd;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,YACE,eAAe,KAAK;AAAA,UACtB;AAAA,QACF;AAEA,cAAM,IAAI,MAAM,aAAa;AAG7B,YAAI,KAAK,SAAS;AAChB,eAAK,QAAQ,QAAQ,CAAC,QAAQ;AAC5B,gBAAI,IAAI,cAAc,IAAI,WAAW,SAAS,cAAc;AAC1D,4BAAc,KAAK;AAAA,gBACjB,QAAQ;AAAA,gBACR,QAAQ,EAAE,MAAM,IAAI,WAAW,MAAM,MAAM,YAAY;AAAA,gBACvD,MAAM;AAAA,cACR,CAAC;AAAA,YACH;AAAA,UACF,CAAC;AAAA,QACH;AAGA,YAAI,KAAK,MAAM;AACb,gBAAM,KAAK,MAAM,MAAM,eAAe,IAAI;AAAA,QAC5C;AAEA;AAAA,MACF;AAAA,MAEA,KAAK,wBAAwB;AAE3B,cAAM,OAAO,KAAK,IAAI,QAAQ;AAE9B,wBAAgB;AAAA,UACd;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,YACE,eAAe,KAAK;AAAA,UACtB;AAAA,QACF;AAEA,cAAM,IAAI,MAAM,aAAa;AAG7B,YAAI,KAAK,gBAAgB;AACvB,gBAAM,KAAK,gBAAgB,MAAM,eAAe,IAAI;AAAA,QACtD;AAEA;AAAA,MACF;AAAA,MAEA,KAAK,mBAAmB;AAEtB,cAAM,OAAO,KAAK,IAAI,QAAQ;AAE9B,wBAAgB;AAAA,UACd;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,YACE,eAAe,KAAK;AAAA,UACtB;AAAA,QACF;AAEA,cAAM,IAAI,MAAM,aAAa;AAG7B,YAAI,KAAK,SAAS;AAChB,eAAK,QAAQ,QAAQ,CAAC,WAAW;AAC/B,kBAAM,QAAQ,MAAM,eAAe,IAAI;AAAA,UACzC,CAAC;AAAA,QACH;AAEA;AAAA,MACF;AAAA,MAEA,KAAK,kBAAkB;AAErB,YAAI,cAAc;AAChB,cAAI,KAAK,OAAO,SAAS,cAAc;AACrC,0BAAc,KAAK;AAAA,cACjB,QAAQ;AAAA,cACR,QAAQ,EAAE,MAAM,KAAK,OAAO,MAAM,MAAM,WAAW;AAAA,cACnD,MAAM;AAAA,YACR,CAAC;AAAA,UACH,WAAW,KAAK,OAAO,SAAS,oBAAoB;AAClD,gBACE,KAAK,OAAO,YACZ,KAAK,OAAO,SAAS,SAAS,cAC9B;AACA,4BAAc,KAAK;AAAA,gBACjB,QAAQ;AAAA,gBACR,QAAQ,EAAE,MAAM,KAAK,OAAO,SAAS,MAAM,MAAM,SAAS;AAAA,gBAC1D,MAAM;AAAA,gBACN,UAAU;AAAA,kBACR,QACE,KAAK,OAAO,OAAO,SAAS,eACxB,KAAK,OAAO,OAAO,OACnB;AAAA,gBACR;AAAA,cACF,CAAC;AAAA,YACH;AAAA,UACF;AAAA,QACF;AAGA,YAAI,KAAK,QAAQ;AACf,gBAAM,KAAK,QAAQ,MAAM,cAAc,KAAK;AAAA,QAC9C;AAEA,YAAI,KAAK,WAAW;AAClB,eAAK,UAAU,QAAQ,CAAC,QAAQ;AAC9B,kBAAM,KAAK,MAAM,cAAc,KAAK;AAAA,UACtC,CAAC;AAAA,QACH;AAEA;AAAA,MACF;AAAA,MAGA,SAAS;AACP,mBAAW,OAAO,MAAM;AACtB,gBAAM,QAAQ,KAAK,GAAG;AAGtB,cACE,QAAQ,UACR,QAAQ,SACR,QAAQ,WACR,QAAQ,UACR;AACA;AAAA,UACF;AAEA,cAAI,MAAM,QAAQ,KAAK,GAAG;AAExB,uBAAW,QAAQ,OAAO;AACxB,oBAAM,MAAM,MAAM,gBAAgB,eAAe,KAAK;AAAA,YACxD;AAAA,UACF,WAAW,SAAS,OAAO,UAAU,UAAU;AAE7C,kBAAM,OAAO,MAAM,gBAAgB,eAAe,KAAK;AAAA,UACzD;AAAA,QACF;AAAA,MACF;AAAA,IACF;AAAA,EACF;AAGA,QAAM,GAAG;AAET,SAAO,EAAE,UAAU,cAAc;AACnC;AAUA,eAAsB,cAAc,UAAU,aAAa,cAAc;AACvE,MAAI;AAEF,UAAM,cAAc,qBAAqB,WAAW;AAGpD,UAAM,WAAW,gBAAgB,QAAQ;AAGzC,UAAM,WAAW,eAAe,UAAU,YAAY;AAGtD,UAAM,oBAAoB;AAAA;AAAA;AAAA;AAAA;AAM1B,UAAM,eAAe,MAAM,aAAa,mBAAmB,CAAC,QAAQ,CAAC;AAErE,QAAI;AAEJ,QAAI,gBAAgB,aAAa,SAAS,GAAG;AAC3C,qBAAe,aAAa,CAAC,EAAE;AAG/B,UAAI,aAAa,CAAC,EAAE,iBAAiB,aAAa;AAChD,gBAAQ,IAAI,QAAQ,QAAQ,kCAAkC;AAC9D;AAAA,MACF;AAGA,YAAM;AAAA,QACJ;AAAA;AAAA;AAAA;AAAA;AAAA,QAKA,CAAC,aAAa,aAAa,UAAU,YAAY;AAAA,MACnD;AAGA,YAAM;AAAA,QACJ;AAAA;AAAA;AAAA;AAAA,QAIA,CAAC,YAAY;AAAA,MACf;AAGA,YAAM;AAAA,QACJ;AAAA;AAAA;AAAA;AAAA,QAIA,CAAC,YAAY;AAAA,MACf;AAAA,IACF,OAAO;AAEL,qBAAeC,QAAO;AAEtB,YAAM;AAAA,QACJ;AAAA;AAAA;AAAA;AAAA;AAAA,QAKA;AAAA,UACE;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,QACF;AAAA,MACF;AAAA,IACF;AAGA,QAAI,eAAe,CAAC;AACpB,QAAI,gBAAgB,CAAC;AAGrB,QAAI,aAAa,gBAAgB,aAAa,cAAc;AAC1D,YAAM,MAAM,MAAM,SAAS,aAAa,QAAQ;AAEhD,UAAI,OAAO,CAAC,IAAI,OAAO;AACrB,cAAM,YAAY,uBAAuB,KAAK,WAAW;AACzD,uBAAe,UAAU;AACzB,wBAAgB,UAAU;AAAA,MAC5B,OAAO;AACL,gBAAQ;AAAA,UACN,0BAA0B,QAAQ;AAAA,UAClC,KAAK,SAAS;AAAA,QAChB;AAEA,uBAAe,yBAAyB,aAAa,QAAQ;AAAA,MAC/D;AAAA,IACF,OAEK;AACH,qBAAe,yBAAyB,aAAa,QAAQ;AAAA,IAC/D;AAGA,eAAW,UAAU,cAAc;AACjC,YAAM,WAAWA,QAAO;AAGxB,YAAM,qBAAqB,OAAO,kBAC9B,KAAK,UAAU,OAAO,eAAe,IACrC;AAGJ,YAAM;AAAA,QACJ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,QAOA;AAAA,UACE;AAAA,UACA;AAAA;AAAA,UACA;AAAA,UACA,OAAO;AAAA,UACP,OAAO;AAAA,UACP,OAAO;AAAA,UACP,OAAO;AAAA,UACP,OAAO;AAAA,UACP;AAAA,UACA;AAAA,QACF;AAAA,MACF;AAGA,YAAM,SAAS,SAAS,OAAO,WAAW;AAC1C,YAAM,WAAW,gBAAgB,QAAQ,IAAI,QAAQ;AAGrD,iBAAW,WAAW,UAAU;AAC9B,cAAM;AAAA,UACJ;AAAA;AAAA;AAAA;AAAA;AAAA,UAKA;AAAA,YACE;AAAA,YACA,QAAQ;AAAA,YACR,QAAQ,SAAS;AAAA,YACjB,QAAQ,SAAS;AAAA,YACjB;AAAA,UACF;AAAA,QACF;AAAA,MACF;AAGA,aAAO,eAAe;AAAA,IACxB;AAIA,eAAW,OAAO,eAAe;AAE/B,UAAI,CAAC,IAAI,UAAU,CAAC,IAAI;AAAQ;AAEhC,YAAM,WAAW,IAAI,OAAO;AAC5B,YAAM,WAAW,IAAI,OAAO;AAG5B,UAAI,IAAI,SAAS,cAAc,YAAY,UAAU;AACnD,cAAM;AAAA,UACJ;AAAA;AAAA;AAAA;AAAA;AAAA,UAKA,CAAC,UAAU,QAAQ;AAAA,QACrB;AAAA,MACF,WAES,YAAY,UAAU;AAC7B,cAAM;AAAA,UACJ;AAAA,UACA;AAAA,UACA,IAAI;AAAA,UACJ;AAAA,UACA,IAAI,YAAY,CAAC;AAAA,QACnB;AAAA,MACF,WAES,YAAY,CAAC,YAAY,IAAI,OAAO,MAAM;AAEjD,cAAM,cAAc;AAAA;AAAA;AAAA;AAAA;AAMpB,cAAM,eAAe,MAAM,aAAa,aAAa;AAAA,UACnD,IAAI,OAAO;AAAA,UACX,IAAI,OAAO;AAAA,QACb,CAAC;AAED,YAAI,gBAAgB,aAAa,SAAS,GAAG;AAC3C,gBAAM;AAAA,YACJ;AAAA,YACA,aAAa,CAAC,EAAE;AAAA,YAChB,IAAI;AAAA,YACJ;AAAA,YACA,IAAI,YAAY,CAAC;AAAA,UACnB;AAAA,QACF;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,IAAI,6BAA6B,QAAQ,EAAE;AAAA,EACrD,SAAS,OAAO;AACd,YAAQ,MAAM,uBAAuB,QAAQ,KAAK,KAAK;AACvD,UAAM;AAAA,EACR;AACF;AAwBA,eAAsB,yBAAyB,SAAS;AACtD,MAAI;AAEF,QACE,CAAC,QAAQ,cACT,CAAC,QAAQ,mBACT,CAAC,QAAQ,QACT,CAAC,QAAQ,SACT;AACA,YAAM,IAAI,MAAM,4CAA4C;AAAA,IAC9D;AAEA,YAAQ,IAAI,mCAAmC;AAC/C,YAAQ,IAAI,mBAAmB;AAC/B,YAAQ,IAAI,iBAAiB,QAAQ,UAAU;AAC/C,YAAQ,IAAI,sBAAsB,QAAQ,eAAe;AACzD,YAAQ,IAAI,WAAW,QAAQ,IAAI;AACnC,YAAQ;AAAA,MACN;AAAA,MACA,QAAQ,WACN,QAAQ,QAAQ,UAAU,GAAG,EAAE,KAC5B,QAAQ,QAAQ,SAAS,KAAK,QAAQ;AAAA,IAC7C;AACA,YAAQ,IAAI,gBAAgB,QAAQ,SAAS;AAG7C,UAAM,0BAA0B,QAAQ,0BACpC,QAAQ,0BACR;AAEJ,UAAM,kBAAkB,QAAQ,mBAC5B,QAAQ,mBACR;AAEJ,UAAM,sBAAsB,QAAQ,uBAChC,QAAQ,uBACR;AAGJ,UAAM,YACJ,QAAQ,qBAAqB,OACzB,QAAQ,UAAU,YAAY,IAC9B,QAAQ,cAAa,oBAAI,KAAK,GAAE,YAAY;AAGlD,UAAM,uBAAuB;AAAA;AAAA;AAAA;AAK7B,YAAQ,IAAI,+BAA+B,QAAQ,UAAU;AAC7D,UAAM,kBAAkB,MAAM,aAAa,sBAAsB;AAAA,MAC/D,QAAQ;AAAA,IACV,CAAC;AAED,YAAQ;AAAA,MACN;AAAA,MACA,KAAK,UAAU,eAAe;AAAA,IAChC;AAEA,QACE,mBACA,gBAAgB,QAChB,gBAAgB,KAAK,SAAS,GAC9B;AACA,cAAQ,IAAI,8BAA8B,QAAQ,UAAU;AAE5D,UAAI;AACF,cAAM,cAAc;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAUpB,cAAM,eAAe;AAAA,UACnB,QAAQ;AAAA,UACR,QAAQ,WAAW;AAAA,UACnB,QAAQ,cAAc;AAAA,UACtB,QAAQ,kBAAkB;AAAA,UAC1B;AAAA,UACA;AAAA,UACA;AAAA,UACA,QAAQ;AAAA,QACV;AAEA,gBAAQ,IAAI,4BAA4B;AAAA,UACtC,YAAY,QAAQ;AAAA,UACpB,gBAAgB,QAAQ,UAAU,QAAQ,QAAQ,SAAS;AAAA,QAC7D,CAAC;AAED,cAAM,eAAe,MAAM,aAAa,aAAa,YAAY;AACjE,gBAAQ,IAAI,0BAA0B,KAAK,UAAU,YAAY,CAAC;AAAA,MACpE,SAAS,aAAa;AACpB,gBAAQ,MAAM,iBAAiB,WAAW;AAC1C,cAAM;AAAA,MACR;AAAA,IACF,OAAO;AACL,cAAQ,IAAI,0BAA0B,QAAQ,UAAU;AAExD,UAAI;AACF,cAAM,cAAc;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAcpB,cAAM,eAAe;AAAA,UACnB,QAAQ;AAAA,UACR,QAAQ;AAAA,UACR,QAAQ;AAAA,UACR,QAAQ;AAAA,UACR;AAAA,UACA,QAAQ,WAAW;AAAA,UACnB,QAAQ,cAAc;AAAA,UACtB,QAAQ,kBAAkB;AAAA,UAC1B;AAAA,UACA;AAAA,UACA;AAAA,QACF;AAEA,gBAAQ,IAAI,4BAA4B;AAAA,UACtC,YAAY,QAAQ;AAAA,UACpB,iBAAiB,QAAQ;AAAA,UACzB,MAAM,QAAQ;AAAA,UACd;AAAA,QACF,CAAC;AAED,cAAM,eAAe,MAAM,aAAa,aAAa,YAAY;AACjE,gBAAQ,IAAI,0BAA0B,KAAK,UAAU,YAAY,CAAC;AAAA,MACpE,SAAS,aAAa;AACpB,gBAAQ,MAAM,iBAAiB,WAAW;AAC1C,gBAAQ,MAAM,gBAAgB,YAAY,KAAK;AAC/C,cAAM;AAAA,MACR;AAAA,IACF;AAGA,UAAM,SAAS,SAAS,QAAQ,OAAO;AACvC,UAAM,WAAW,gBAAgB,MAAM;AAEvC,YAAQ,IAAI,sCAAsC;AAClD,YAAQ,IAAI,iCAAiC,QAAQ,UAAU;AAE/D,WAAO;AAAA,MACL,WAAW,QAAQ;AAAA,MACnB;AAAA,IACF;AAAA,EACF,SAAS,OAAO;AACd,YAAQ,MAAM,mCAAmC;AACjD,YAAQ,MAAM,0BAA0B,SAAS,UAAU,KAAK,KAAK;AACrE,YAAQ,MAAM,gBAAgB,MAAM,KAAK;AACzC,UAAM;AAAA,EACR;AACF;;;AE1yCA;AACA,SAAS,MAAMC,eAAc;;;ACsBtB,SAAS,kBAAkB,iBAAiB,QAAQ,gBAAgB,CAAC,GAAG;AAC7E,MAAI,CAAC,mBAAmB,gBAAgB,WAAW,GAAG;AACpD,WAAO,CAAC;AAAA,EACV;AAGA,QAAM,oBAAoB,CAAC;AAG3B,MAAI,kBAAkB;AAGtB,QAAM,aAAa,gBAAgB;AAAA,IACjC,CAAC,KAAK,YAAY,MAAM,QAAQ;AAAA,IAChC;AAAA,EACF;AACA,QAAM,oBAAoB,gBAAgB,IAAI,CAAC,YAAY;AAEzD,WAAO,KAAK,IAAI,KAAK,KAAK,MAAO,QAAQ,QAAQ,aAAc,MAAM,CAAC;AAAA,EACxE,CAAC;AAGD,WAAS,IAAI,GAAG,IAAI,gBAAgB,QAAQ,KAAK;AAC/C,UAAM,UAAU,gBAAgB,CAAC;AAGjC,UAAM,UAAU,QAAQ,WAAW,QAAQ,OAAO,eAAe;AAGjE,QAAI,CAAC,SAAS;AACZ;AAAA,IACF;AAGA,QAAI,gBAAgB,KAAK,IAAI,kBAAkB,CAAC,GAAG,eAAe;AAGlE,QAAI,gBAAgB,IAAI;AACtB;AAAA,IACF;AAGA,QAAI,QAAQ,UAAU,eAAe;AAEnC,wBAAkB,KAAK;AAAA,QACrB,WAAW,QAAQ,OAAO;AAAA,QAC1B,mBAAmB;AAAA,QACnB,eAAe,QAAQ;AAAA,MACzB,CAAC;AAED,yBAAmB,QAAQ;AAAA,IAC7B,OAAO;AAEL,UAAI;AAGJ,UAAI,QAAQ,OAAO,aAAa;AAC9B,4BAAoB;AAAA,UAClB,QAAQ;AAAA,UACR;AAAA,UACA;AAAA,QACF;AAAA,MACF,OAAO;AACL,4BAAoB,cAAc,SAAS,aAAa;AAAA,MAC1D;AAGA,UAAI,mBAAmB;AACrB,0BAAkB,KAAK;AAAA,UACrB,WAAW,QAAQ,OAAO;AAAA,UAC1B;AAAA,UACA,eAAe,QAAQ;AAAA,QACzB,CAAC;AAED,2BAAmB,kBAAkB;AAAA,MACvC;AAAA,IACF;AAGA,QAAI,mBAAmB,IAAI;AACzB;AAAA,IACF;AAGA,QAAI,IAAI,gBAAgB,SAAS,GAAG;AAClC,YAAM,oBAAoB,gBAAgB,SAAS,IAAI;AACvD,YAAM,kBAAkB,gBACrB,MAAM,IAAI,CAAC,EACX,OAAO,CAAC,KAAK,MAAM,MAAM,EAAE,OAAO,CAAC;AAGtC,eAAS,IAAI,IAAI,GAAG,IAAI,gBAAgB,QAAQ,KAAK;AACnD,0BAAkB,CAAC,IAAI,KAAK;AAAA,UAC1B;AAAA,UACA,KAAK;AAAA,YACF,gBAAgB,CAAC,EAAE,QAAQ,kBAAmB;AAAA,UACjD;AAAA,QACF;AAAA,MACF;AAAA,IACF;AAAA,EACF;AAIA,MAAI,kBAAkB,OAAO,kBAAkB,SAAS,GAAG;AACzD;AAAA,MACE;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,EACF;AAEA,SAAO;AACT;AAUA,SAAS,4BACP,mBACA,kBACA,iBACA,eACA;AAEA,QAAM,eAAe,oBAAI,IAAI;AAC7B,oBAAkB,QAAQ,CAAC,OAAO;AAChC,iBAAa,IAAI,GAAG,WAAW,EAAE;AAAA,EACnC,CAAC;AAID,QAAM,mBAAmB,iBACtB,OAAO,CAAC,MAAM,aAAa,IAAI,EAAE,OAAO,SAAS,CAAC,EAClD,KAAK,CAAC,GAAG,MAAM,EAAE,QAAQ,EAAE,KAAK;AAGnC,QAAM,6BAA6B,KAAK;AAAA,IACtC,kBAAkB,iBAAiB;AAAA,EACrC;AAGA,aAAW,WAAW,kBAAkB;AACtC,UAAM,mBAAmB,aAAa,IAAI,QAAQ,OAAO,SAAS;AAClE,UAAM,gBAAgB,iBAAiB,kBAAkB;AACzD,UAAM,YAAY,gBAAgB;AAGlC,UAAM,UAAU,QAAQ,WAAW,QAAQ,OAAO,eAAe;AAGjE,QAAI,QAAQ,UAAU,WAAW;AAC/B,uBAAiB,oBAAoB;AACrC,yBAAmB,QAAQ,SAAS;AAAA,IACtC,OAAO;AAEL,UAAI;AAEJ,UAAI,QAAQ,OAAO,aAAa;AAC9B,0BAAkB;AAAA,UAChB,QAAQ;AAAA,UACR;AAAA,UACA;AAAA,QACF;AAAA,MACF,OAAO;AACL,0BAAkB,cAAc,SAAS,SAAS;AAAA,MACpD;AAEA,UAAI,mBAAmB,gBAAgB,SAAS,eAAe;AAC7D,2BAAmB,gBAAgB,SAAS;AAC5C,yBAAiB,oBAAoB;AAAA,MACvC;AAAA,IACF;AAGA,QAAI,kBAAkB,KAAK;AACzB;AAAA,IACF;AAAA,EACF;AACF;AAUO,SAAS,cAAc,MAAM,WAAW,SAAS,cAAc;AAEpE,MAAI,CAAC;AAAM,WAAO;AAClB,MAAI,KAAK,UAAU;AAAW,WAAO;AAGrC,MAAI,WAAW,YAAY;AACzB,YAAQ;AAAA,MACN;AAAA,IACF;AACA,aAAS;AAAA,EACX;AAGA,SAAO,UAAU,MAAM,SAAS;AAClC;AAUO,SAAS,oBAAoB,QAAQ,QAAQ,gBAAgB,CAAC,GAAG;AAEtE,MAAI,CAAC;AAAQ,WAAO;AAGpB,MAAI,OAAO,WAAW,OAAO,QAAQ,UAAU,QAAQ;AACrD,WAAO,OAAO;AAAA,EAChB;AAGA,MAAI,CAAC,OAAO,aAAa;AACvB,WAAO,GAAG,OAAO,IAAI,KAAK,OAAO,WAAW;AAAA,EAC9C;AAGA,MAAI,OAAO,YAAY,UAAU,QAAQ;AACvC,WAAO,OAAO;AAAA,EAChB;AAGA,QAAM,cAAc,OAAO,eAAe,IAAI,YAAY;AAE1D,UAAQ,YAAY;AAAA,IAClB,KAAK;AAAA,IACL,KAAK;AACH,aAAO,kBAAkB,QAAQ,QAAQ,aAAa;AAAA,IAExD,KAAK;AACH,aAAO,eAAe,QAAQ,QAAQ,aAAa;AAAA,IAErD,KAAK;AACH,aAAO,cAAc,QAAQ,QAAQ,aAAa;AAAA,IAEpD;AAEE,aAAO,cAAc,OAAO,aAAa,MAAM;AAAA,EACnD;AACF;AAUA,SAAS,kBAAkB,QAAQ,QAAQ,eAAe;AACxD,QAAM,UAAU,OAAO;AACvB,QAAM,QAAQ,QAAQ,MAAM,IAAI;AAGhC,QAAM,gBAAgB,yBAAyB,KAAK;AAGpD,MAAI,cAAc,UAAU,SAAS,IAAI;AACvC,WAAO,oBAAoB,eAAe,MAAM;AAAA,EAClD;AAGA,QAAM,cAAc,eAAe,OAAO,eAAe,UAAU;AAGnE,MAAI,UAAU;AACd,MAAI,kBAAkB,SAAS,cAAc;AAG7C,QAAM,eAAe,oBAAoB,KAAK;AAC9C,MAAI,gBAAgB,aAAa,SAAS,kBAAkB,KAAK;AAC/D,eAAW,OAAO;AAClB,uBAAmB,aAAa;AAAA,EAClC;AAGA,aAAW,OAAO,qBAAqB,aAAa,eAAe;AAGnE,SAAO,oBAAoB,SAAS,MAAM;AAC5C;AAUA,SAAS,eAAe,QAAQ,QAAQ,eAAe;AACrD,QAAM,UAAU,OAAO;AACvB,QAAM,QAAQ,QAAQ,MAAM,IAAI;AAGhC,QAAM,iBAAiB,sBAAsB,KAAK;AAClD,QAAM,aAAa,kBAAkB,KAAK;AAG1C,MAAI,UAAU;AACd,MAAI,kBAAkB,SAAS,eAAe;AAG9C,MAAI,cAAc,WAAW,SAAS,iBAAiB;AACrD,eAAW,OAAO;AAClB,uBAAmB,WAAW;AAAA,EAChC;AAGA,MAAI,kBAAkB,IAAI;AACxB,UAAM,cAAc,eAAe,OAAO,eAAe,OAAO;AAChE,eAAW,OAAO,qBAAqB,aAAa,eAAe;AAAA,EACrE;AAEA,SAAO,oBAAoB,SAAS,MAAM;AAC5C;AAUA,SAAS,cAAc,QAAQ,QAAQ,eAAe;AACpD,QAAM,UAAU,OAAO;AACvB,QAAM,QAAQ,QAAQ,MAAM,IAAI;AAGhC,QAAM,aACH,OAAO,QAAQ,IAAI,YAAY,EAAE,SAAS,QAAQ,MAClD,OAAO,QAAQ,IAAI,YAAY,EAAE,SAAS,KAAK;AAElD,MAAI,WAAW;AAEb,WAAO,cAAc,SAAS,MAAM;AAAA,EACtC;AAGA,QAAM,mBAAmB,MACtB;AAAA,IACC,CAAC,SACC,KAAK,KAAK,EAAE,WAAW,SAAS,KAChC,KAAK,KAAK,EAAE,WAAW,UAAU,KACjC,KAAK,KAAK,EAAE,WAAW,OAAO,KAC9B,KAAK,KAAK,EAAE,SAAS,QAAQ;AAAA,EACjC,EACC,KAAK,IAAI;AAGZ,QAAM,mBAAmB,MACtB;AAAA,IACC,CAAC,SACC,KAAK,KAAK,EAAE,WAAW,SAAS,KAChC,KAAK,KAAK,EAAE,WAAW,gBAAgB;AAAA,EAC3C,EACC,KAAK,IAAI;AAGZ,MAAI,UAAU,YAAY,OAAO,QAAQ,SAAS;AAAA;AAGlD,MAAI,oBAAoB,iBAAiB,SAAS,SAAS,KAAK;AAC9D,eAAW;AAAA,EAAgB,gBAAgB;AAAA;AAAA,EAC7C;AAGA,QAAM,wBAAwB,SAAS,QAAQ;AAC/C,MACE,oBACA,iBAAiB,SAAS,wBAAwB,KAClD;AACA,eAAW;AAAA,EAAgB,gBAAgB;AAAA;AAAA,EAC7C;AAGA,QAAM,kBAAkB,SAAS,QAAQ;AACzC,MAAI,kBAAkB,KAAK;AACzB,UAAM,cAAc,eAAe,OAAO,eAAe,MAAM;AAC/D,eAAW;AAAA,EAAqB;AAAA,MAC9B;AAAA,MACA;AAAA,IACF,CAAC;AAAA,EACH;AAEA,SAAO,oBAAoB,SAAS,MAAM;AAC5C;AAQA,SAAS,yBAAyB,OAAO;AAEvC,WAAS,IAAI,GAAG,IAAI,MAAM,QAAQ,KAAK;AACrC,UAAM,OAAO,MAAM,CAAC,EAAE,KAAK;AAC3B,QACE,KAAK;AAAA,MACH;AAAA,IACF,GACA;AAEA,UAAI,YAAY;AAGhB,UAAI,CAAC,KAAK,SAAS,GAAG,KAAK,CAAC,KAAK,SAAS,IAAI,GAAG;AAC/C,YAAI,IAAI,IAAI;AACZ,eAAO,IAAI,MAAM,UAAU,CAAC,MAAM,CAAC,EAAE,SAAS,GAAG,GAAG;AAClD,uBAAa,MAAM,MAAM,CAAC,EAAE,KAAK;AACjC;AAAA,QACF;AACA,YAAI,IAAI,MAAM,QAAQ;AACpB,uBAAa,MAAM,MAAM,CAAC,EAAE,KAAK,EAAE,MAAM,GAAG,EAAE,CAAC,IAAI;AAAA,QACrD;AAAA,MACF,WAAW,KAAK,SAAS,GAAG,GAAG;AAC7B,oBAAY,UAAU,MAAM,GAAG,EAAE,CAAC,IAAI;AAAA,MACxC,WAAW,KAAK,SAAS,IAAI,GAAG;AAC9B,cAAM,aAAa,UAAU,MAAM,IAAI;AACvC,oBAAY,WAAW,CAAC,IAAI;AAAA,MAC9B;AAEA,aAAO;AAAA,IACT;AAAA,EACF;AAGA,SAAO;AACT;AAQA,SAAS,sBAAsB,OAAO;AAEpC,WAAS,IAAI,GAAG,IAAI,MAAM,QAAQ,KAAK;AACrC,UAAM,OAAO,MAAM,CAAC,EAAE,KAAK;AAC3B,QAAI,KAAK,WAAW,QAAQ,GAAG;AAE7B,UAAI,YAAY;AAGhB,UAAI,CAAC,KAAK,SAAS,GAAG,GAAG;AACvB,YAAI,IAAI,IAAI;AACZ,eAAO,IAAI,MAAM,UAAU,CAAC,MAAM,CAAC,EAAE,SAAS,GAAG,GAAG;AAClD,uBAAa,MAAM,MAAM,CAAC,EAAE,KAAK;AACjC;AAAA,QACF;AACA,YAAI,IAAI,MAAM,QAAQ;AACpB,uBAAa,MAAM,MAAM,CAAC,EAAE,KAAK,EAAE,MAAM,GAAG,EAAE,CAAC,IAAI;AAAA,QACrD;AAAA,MACF,OAAO;AACL,oBAAY,UAAU,MAAM,GAAG,EAAE,CAAC,IAAI;AAAA,MACxC;AAEA,aAAO;AAAA,IACT;AAAA,EACF;AAGA,SAAO;AACT;AAQA,SAAS,kBAAkB,OAAO;AAChC,QAAM,UAAU,CAAC;AAGjB,QAAM,cAAc;AAGpB,QAAM,gBAAgB,KAAK,IAAI,GAAG,MAAM,MAAM;AAE9C,WAAS,IAAI,eAAe,IAAI,MAAM,QAAQ,KAAK;AACjD,UAAM,QAAQ,MAAM,CAAC,EAAE,MAAM,WAAW;AACxC,QAAI,SAAS,CAAC,MAAM,CAAC,EAAE,KAAK,EAAE,WAAW,IAAI,GAAG;AAC9C,cAAQ,KAAK,MAAM,CAAC,CAAC;AAAA,IACvB;AAAA,EACF;AAEA,MAAI,QAAQ,WAAW,GAAG;AACxB,WAAO;AAAA,EACT;AAEA,SAAO,eAAe,QAAQ,KAAK,IAAI,CAAC;AAC1C;AAQA,SAAS,oBAAoB,OAAO;AAClC,MAAI,YAAY;AAChB,MAAI,eAAe,CAAC;AAEpB,WAAS,IAAI,GAAG,IAAI,KAAK,IAAI,IAAI,MAAM,MAAM,GAAG,KAAK;AACnD,UAAM,OAAO,MAAM,CAAC,EAAE,KAAK;AAG3B,QAAI,KAAK,WAAW,KAAK,GAAG;AAC1B,kBAAY;AACZ,mBAAa,KAAK,IAAI;AACtB;AAAA,IACF;AAGA,QAAI,WAAW;AACb,mBAAa,KAAK,IAAI;AACtB,UAAI,KAAK,SAAS,IAAI,GAAG;AACvB;AAAA,MACF;AAAA,IACF;AAGA,QAAI,CAAC,aAAa,aAAa,WAAW,KAAK,KAAK,WAAW,IAAI,GAAG;AACpE,mBAAa,KAAK,IAAI;AAAA,IACxB,WAAW,CAAC,aAAa,aAAa,SAAS,KAAK,KAAK,WAAW,IAAI,GAAG;AACzE,mBAAa,KAAK,IAAI;AAAA,IACxB,WAAW,CAAC,aAAa,aAAa,SAAS,GAAG;AAEhD;AAAA,IACF;AAAA,EACF;AAEA,SAAO,aAAa,KAAK,IAAI;AAC/B;AAUA,SAAS,eAAe,OAAO,eAAe,YAAY;AACxD,QAAM,cAAc,CAAC;AAGrB,QAAM,oBAAoB;AAAA,IACxB,UAAU;AAAA,MACR;AAAA;AAAA,MACA;AAAA;AAAA,MACA;AAAA;AAAA,MACA;AAAA;AAAA,MACA;AAAA;AAAA,MACA;AAAA;AAAA,MACA;AAAA;AAAA,MACA;AAAA;AAAA,MACA;AAAA;AAAA,IACF;AAAA,IACA,OAAO;AAAA,MACL;AAAA;AAAA,MACA;AAAA;AAAA,MACA;AAAA;AAAA,MACA;AAAA;AAAA,MACA;AAAA;AAAA,MACA;AAAA;AAAA,MACA;AAAA;AAAA,IACF;AAAA,IACA,MAAM;AAAA,MACJ;AAAA;AAAA,MACA;AAAA;AAAA,MACA;AAAA;AAAA,MACA;AAAA;AAAA,MACA;AAAA;AAAA,MACA;AAAA;AAAA,IACF;AAAA,EACF;AAGA,QAAM,iBAAiB;AAAA,IACrB;AAAA;AAAA,IACA;AAAA;AAAA,IACA;AAAA;AAAA,IACA;AAAA;AAAA,EACF;AAGA,QAAM,WAAW;AAAA,IACf,GAAI,kBAAkB,UAAU,KAAK,CAAC;AAAA,IACtC,GAAG;AAAA,EACL;AAEA,WAAS,IAAI,GAAG,IAAI,MAAM,QAAQ,KAAK;AACrC,UAAM,OAAO,MAAM,CAAC,EAAE,KAAK;AAC3B,QAAI,CAAC;AAAM;AAEX,QAAI,QAAQ;AAGZ,QAAI,cAAc,SAAS,GAAG;AAC5B,YAAM,SAAS,SAAS,MAAM,EAAE,oBAAoB,KAAK,CAAC;AAC1D,YAAM,iBAAiB,cAAc;AAAA,QACnC,CAAC,YACC,OAAO,SAAS,QAAQ,YAAY,CAAC,KACrC,KAAK,YAAY,EAAE,SAAS,QAAQ,YAAY,CAAC;AAAA,MACrD;AAEA,eAAS,eAAe,SAAS;AAAA,IACnC;AAGA,eAAW,WAAW,UAAU;AAC9B,UAAI,QAAQ,KAAK,IAAI,GAAG;AACtB,iBAAS;AACT;AAAA,MACF;AAAA,IACF;AAGA,QAAI,IAAI,GAAG;AACT,eAAS;AAAA,IACX;AAGA,QAAI,KAAK,SAAS,GAAG,KAAK,KAAK,SAAS,GAAG,GAAG;AAC5C,eAAS;AAAA,IACX;AAGA,gBAAY,KAAK;AAAA,MACf;AAAA,MACA;AAAA,MACA,OAAO;AAAA,IACT,CAAC;AAAA,EACH;AAGA,SAAO,YAAY,KAAK,CAAC,GAAG,MAAM,EAAE,QAAQ,EAAE,KAAK;AACrD;AASA,SAAS,qBAAqB,aAAa,QAAQ;AACjD,QAAM,gBAAgB,CAAC;AACvB,MAAI,aAAa;AAGjB,QAAM,iBAAiB,YAAY,OAAO,CAAC,SAAS,KAAK,SAAS,CAAC;AAEnE,aAAW,QAAQ,gBAAgB;AACjC,QAAI,aAAa,KAAK,KAAK,SAAS,KAAK,QAAQ;AAE/C,oBAAc,KAAK,IAAI;AACvB,oBAAc,KAAK,KAAK,SAAS;AAAA,IACnC;AAAA,EACF;AAGA,MAAI,aAAa,QAAQ;AACvB,UAAM,iBAAiB,YACpB,OAAO,CAAC,SAAS,KAAK,QAAQ,CAAC,EAC/B,KAAK,CAAC,GAAG,MAAM,EAAE,QAAQ,EAAE,KAAK;AAEnC,eAAW,QAAQ,gBAAgB;AACjC,UAAI,aAAa,KAAK,KAAK,SAAS,KAAK,QAAQ;AAC/C,sBAAc,KAAK,IAAI;AACvB,sBAAc,KAAK,KAAK,SAAS;AAAA,MACnC;AAAA,IACF;AAAA,EACF;AAGA,gBAAc,KAAK,CAAC,GAAG,MAAM,EAAE,QAAQ,EAAE,KAAK;AAE9C,SAAO,cAAc,IAAI,CAAC,SAAS,KAAK,IAAI,EAAE,KAAK,IAAI;AACzD;AASA,SAAS,UAAU,MAAM,WAAW;AAElC,QAAM,YAAY,mBAAmB,IAAI;AAGzC,MAAI,UAAU,UAAU,GAAG;AAEzB,WAAO,oBAAoB,MAAM,SAAS;AAAA,EAC5C;AAGA,QAAM,kBAAkB,UAAU,IAAI,CAAC,UAAU,WAAW;AAAA,IAC1D,MAAM;AAAA,IACN,OAAO,cAAc,UAAU,OAAO,UAAU,MAAM;AAAA,IACtD;AAAA,EACF,EAAE;AAGF,kBAAgB,KAAK,CAAC,GAAG,MAAM,EAAE,QAAQ,EAAE,KAAK;AAGhD,QAAM,oBAAoB,CAAC;AAC3B,MAAI,gBAAgB;AAEpB,aAAW,UAAU,iBAAiB;AAEpC,QAAI,gBAAgB,OAAO,KAAK,SAAS,KAAK,WAAW;AAEvD,wBAAkB,KAAK,MAAM;AAC7B,uBAAiB,OAAO,KAAK,SAAS;AAAA,IACxC,OAAO;AAEL,UAAI,kBAAkB,WAAW,GAAG;AAClC,eAAO,oBAAoB,OAAO,MAAM,SAAS;AAAA,MACnD;AAEA;AAAA,IACF;AAAA,EACF;AAGA,oBAAkB,KAAK,CAAC,GAAG,MAAM,EAAE,QAAQ,EAAE,KAAK;AAGlD,QAAM,UAAU,kBAAkB,IAAI,CAAC,MAAM,EAAE,IAAI,EAAE,KAAK,GAAG;AAG7D,SAAO,oBAAoB,SAAS,SAAS;AAC/C;AAQA,SAAS,mBAAmB,MAAM;AAGhC,QAAM,gBAAgB;AACtB,QAAM,UAAU,KAAK,MAAM,aAAa;AAExC,MAAI,CAAC,SAAS;AAGZ,WAAO,CAAC,IAAI;AAAA,EACd;AAGA,SAAO,QAAQ,IAAI,CAAC,MAAM,EAAE,KAAK,CAAC,EAAE,OAAO,CAAC,MAAM,EAAE,SAAS,CAAC;AAChE;AAUA,SAAS,cAAc,UAAU,OAAO,gBAAgB;AACtD,MAAI,QAAQ;AAGZ,MAAI,UAAU,GAAG;AACf,aAAS;AAAA,EACX,WAAW,UAAU,iBAAiB,GAAG;AACvC,aAAS;AAAA,EACX,WAAW,UAAU,KAAK,UAAU,iBAAiB,GAAG;AACtD,aAAS;AAAA,EACX;AAGA,QAAM,YAAY,SAAS,MAAM,KAAK,EAAE;AACxC,MAAI,aAAa,KAAK,aAAa,IAAI;AACrC,aAAS;AAAA,EACX,WAAW,YAAY,KAAK,YAAY,IAAI;AAC1C,aAAS;AAAA,EACX;AAGA,QAAM,mBAAmB;AAAA,IACvB;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF;AAEA,QAAM,gBAAgB,SAAS,YAAY;AAE3C,aAAW,UAAU,kBAAkB;AACrC,QAAI,cAAc,SAAS,MAAM,GAAG;AAClC,eAAS;AACT;AAAA,IACF;AAAA,EACF;AAGA,MACE,cAAc,SAAS,UAAU,KACjC,cAAc,SAAS,OAAO,KAC9B,cAAc,SAAS,GAAG,KAC1B,cAAc,SAAS,QAAQ,KAC/B,SAAS,SAAS,IAAI,KACtB,SAAS,SAAS,IAAI,KACtB,SAAS,SAAS,IAAI,GACtB;AACA,aAAS;AAAA,EACX;AAEA,SAAO;AACT;AASA,SAAS,oBAAoB,MAAM,WAAW;AAC5C,MAAI,KAAK,UAAU,WAAW;AAC5B,WAAO;AAAA,EACT;AAGA,WAAS,IAAI,YAAY,GAAG,KAAK,GAAG,KAAK;AACvC,QAAI,KAAK,CAAC,MAAM,OAAO,KAAK,CAAC,MAAM,OAAO,KAAK,CAAC,MAAM,KAAK;AACzD,aAAO,KAAK,UAAU,GAAG,IAAI,CAAC;AAAA,IAChC;AAAA,EACF;AAGA,WAAS,IAAI,YAAY,GAAG,KAAK,GAAG,KAAK;AACvC,QAAI,KAAK,CAAC,MAAM,KAAK;AACnB,aAAO,KAAK,UAAU,GAAG,CAAC,IAAI;AAAA,IAChC;AAAA,EACF;AAGA,SAAO,KAAK,UAAU,GAAG,YAAY,CAAC,IAAI;AAC5C;AAYA,eAAsB,gBAAgB,cAAc,UAAU,CAAC,GAAG;AAChE,MAAI,CAAC,gBAAgB,aAAa,WAAW,GAAG;AAC9C,WAAO,CAAC;AAAA,EACV;AAEA,QAAM,cAAc,QAAQ,eAAe;AAC3C,QAAM,eAAe,QAAQ,gBAAgB;AAC7C,QAAM,gBAAgB,QAAQ,iBAAiB,CAAC;AAIhD,QAAM,gBAAgB,IAAI;AAG1B,QAAM,aAAa,KAAK,MAAM,eAAe,aAAa;AAG1D,QAAM,iBAAiB,aAAa,IAAI,CAAC,UAAU;AAAA,IACjD,QAAQ;AAAA,MACN,WAAW,KAAK;AAAA,MAChB,aAAa,KAAK;AAAA,MAClB,aAAa,KAAK;AAAA,MAClB,MAAM,KAAK;AAAA,MACX,WAAW,KAAK;AAAA,IAClB;AAAA,IACA,OAAO,KAAK,kBAAkB;AAAA,IAC9B,SAAS,KAAK;AAAA,EAChB,EAAE;AAGF,MAAI,iBAAiB;AACrB,UAAQ,aAAa;AAAA,IACnB,KAAK;AAEH,uBAAiB,KAAK,MAAM,aAAa,GAAG;AAC5C;AAAA,IACF,KAAK;AAEH,uBAAiB,KAAK,MAAM,aAAa,GAAG;AAC5C;AAAA,IACF;AAEE;AAAA,EACJ;AAGA,QAAM,oBAAoB;AAAA,IACxB;AAAA,IACA;AAAA,IACA;AAAA,EACF;AAGA,SAAO,kBACJ,IAAI,CAAC,cAAc;AAElB,UAAM,eAAe,aAAa;AAAA,MAChC,CAAC,SAAS,KAAK,cAAc,UAAU;AAAA,IACzC;AACA,QAAI,CAAC;AAAc,aAAO;AAE1B,WAAO;AAAA,MACL,GAAG;AAAA,MACH,SAAS,UAAU;AAAA;AAAA,MAEnB,aAAa;AAAA,QACX,gBAAgB,aAAa,QAAQ;AAAA,QACrC,kBAAkB,UAAU,kBAAkB;AAAA,QAC9C,kBACE,UAAU,kBAAkB,SAAS,aAAa,QAAQ;AAAA,QAC5D;AAAA,MACF;AAAA,IACF;AAAA,EACF,CAAC,EACA,OAAO,OAAO;AACnB;;;AD99BA,IAAM,sBAAsB;AAAA,EAC1B;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF;AAGA,IAAM,oBAAoB;AAAA,EACxB;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF;AAUA,eAAsB,iBAAiB,YAAY,qBAAqB;AACtE,MAAI;AACF,QACE,CAAC,YAAY,WACb,CAAC,uBACD,oBAAoB,WAAW,GAC/B;AACA,aAAO;AAAA,IACT;AAGA,UAAM,gBAAgB,oBAAoB,MAAM,EAAE;AAGlD,UAAM,sBAAsB;AAAA,MAC1B;AAAA,MACA;AAAA,IACF;AAGA,UAAM,mBAAmB,qBAAqB,YAAY,aAAa;AAGvE,UAAM,2BAA2B;AAAA,MAC/B,WAAW;AAAA,IACb;AAGA,UAAM,qBAAqB;AAAA,MACzB;AAAA,MACA;AAAA,IACF;AAGA,UAAM,kBACJ,sBAAsB,MACtB,mBAAmB,OAClB,2BAA2B,MAAM,KAAK,MACvC,qBAAqB;AAGvB,WAAO,kBAAkB;AAAA,EAC3B,SAAS,OAAO;AACd,YAAQ,MAAM,gCAAgC,KAAK;AACnD,WAAO;AAAA,EACT;AACF;AASA,SAAS,wBAAwB,YAAY,eAAe;AAE1D,QAAM,YAA+B,SAAS,WAAW,OAAO;AAGhE,QAAM,cAAiC,gBAAgB,WAAW,EAAE;AACpE,QAAM,gBAAgB,IAAI,IAAI,WAAW;AAEzC,MAAI,cAAc,SAAS,GAAG;AAC5B,WAAO;AAAA,EACT;AAGA,QAAM,oBAAoB,oBAAI,IAAI;AAClC,aAAW,WAAW,eAAe;AACnC,UAAM,gBAAmC,SAAS,QAAQ,OAAO;AACjE,UAAM,kBAAqC;AAAA,MACzC;AAAA,MACA;AAAA,IACF;AACA,oBAAgB,QAAQ,CAAC,YAAY,kBAAkB,IAAI,OAAO,CAAC;AAAA,EACrE;AAGA,MAAI,oBAAoB;AACxB,aAAW,WAAW,eAAe;AACnC,QAAI,CAAC,kBAAkB,IAAI,OAAO,GAAG;AACnC;AAAA,IACF;AAAA,EACF;AAGA,SAAO,oBAAoB,cAAc;AAC3C;AAUA,SAAS,qBAAqB,YAAY,eAAe;AAEvD,MACE,CAAC,WAAW,cACZ,CAAC,MAAM,QAAQ,WAAW,UAAU,KACpC,WAAW,WAAW,WAAW,GACjC;AACA,WAAO;AAAA,EACT;AAGA,QAAM,mBAAmB,oBAAI,IAAI;AACjC,aAAW,WAAW,eAAe;AACnC,QAAI,QAAQ,cAAc,MAAM,QAAQ,QAAQ,UAAU,GAAG;AAC3D,cAAQ,WAAW,QAAQ,CAAC,OAAO,iBAAiB,IAAI,EAAE,CAAC;AAAA,IAC7D;AAAA,EACF;AAGA,MAAI,iBAAiB,SAAS,GAAG;AAC/B,WAAO,WAAW,WAAW,SAAS,IAAI,IAAI;AAAA,EAChD;AAGA,MAAI,iBAAiB;AACrB,aAAW,YAAY,WAAW,YAAY;AAC5C,QAAI,CAAC,iBAAiB,IAAI,QAAQ,GAAG;AACnC;AAAA,IACF;AAAA,EACF;AAGA,SAAO,iBAAiB,WAAW,WAAW;AAChD;AAQA,SAAS,4BAA4B,gBAAgB;AACnD,MAAI,CAAC;AAAgB,WAAO;AAE5B,QAAM,eAAe,eAAe,YAAY;AAGhD,aAAW,UAAU,qBAAqB;AAExC,UAAM,QAAQ,IAAI,OAAO,MAAM,MAAM,OAAO,GAAG;AAC/C,QAAI,MAAM,KAAK,YAAY,GAAG;AAC5B,aAAO;AAAA,IACT;AAAA,EACF;AAEA,SAAO;AACT;AASA,SAAS,0BAA0B,YAAY,eAAe;AAE5D,QAAM,uBAAuB,WAAW,WAAW,OAAO;AAE1D,MAAI,CAAC,sBAAsB;AACzB,WAAO;AAAA,EACT;AAGA,MAAI,wBAAwB;AAC5B,MAAI,0BAA0B;AAG9B,WAAS,IAAI,GAAG,IAAI,cAAc,SAAS,GAAG,KAAK;AACjD,QACE,cAAc,CAAC,EAAE,SAAS,UAC1B,WAAW,cAAc,CAAC,EAAE,OAAO,GACnC;AACA;AAGA,UACE,IAAI,IAAI,cAAc,UACtB,cAAc,IAAI,CAAC,EAAE,SAAS,aAC9B;AACA;AAAA,MACF;AAAA,IACF;AAAA,EACF;AAIA,MAAI,wBAAwB,KAAK,0BAA0B,GAAG;AAE5D,UAAM,wBAAwB;AAAA,MAC5B;AAAA,MACA,CAAC,QAAQ,IAAI,SAAS,UAAU,WAAW,IAAI,OAAO;AAAA,IACxD;AAEA,QAAI,yBAAyB,GAAG;AAC9B,YAAM,mBAAmB,cAAc,qBAAqB,EAAE;AAC9D,aAAO,4BAA4B,WAAW,SAAS,gBAAgB;AAAA,IACzE;AAAA,EACF;AAEA,SAAO;AACT;AAQA,SAAS,WAAW,SAAS;AAC3B,MAAI,CAAC;AAAS,WAAO;AAGrB,MAAI,QAAQ,SAAS,GAAG,GAAG;AACzB,WAAO;AAAA,EACT;AAGA,QAAM,eAAe,QAAQ,YAAY,EAAE,KAAK;AAChD,aAAW,WAAW,mBAAmB;AACvC,QAAI,aAAa,WAAW,UAAU,GAAG,GAAG;AAC1C,aAAO;AAAA,IACT;AAAA,EACF;AAEA,SAAO;AACT;AASA,SAAS,4BAA4B,aAAa,kBAAkB;AAClE,QAAM,YAA+B,SAAS,WAAW;AACzD,QAAM,aAAgC,SAAS,gBAAgB;AAG/D,QAAM,SAAS,IAAI,IAAI,SAAS;AAChC,QAAM,UAAU,IAAI,IAAI,UAAU;AAGlC,MAAI,mBAAmB;AACvB,aAAW,SAAS,QAAQ;AAC1B,QAAI,QAAQ,IAAI,KAAK,GAAG;AACtB;AAAA,IACF;AAAA,EACF;AAGA,QAAM,YAAY,OAAO,OAAO,QAAQ,OAAO;AAG/C,QAAM,aAAa,YAAY,IAAI,mBAAmB,YAAY;AAGlE,SAAO,IAAI;AACb;AASA,SAAS,cAAc,OAAO,WAAW;AACvC,WAAS,IAAI,MAAM,SAAS,GAAG,KAAK,GAAG,KAAK;AAC1C,QAAI,UAAU,MAAM,CAAC,CAAC,GAAG;AACvB,aAAO;AAAA,IACT;AAAA,EACF;AACA,SAAO;AACT;AAcA,eAAsB,sBACpB,gBACA,gBACA,YAAY,CAAC,GACb;AACA,MAAI;AAEF,UAAM,WAAWC,QAAO;AAGxB,QAAI,aAAa,UAAU;AAC3B,QAAI,CAAC,YAAY;AAGf,mBAAa,cAAa,oBAAI,KAAK,GAAE,YAAY,CAAC;AAGlD,UAAI;AACF,cAAM,eACJ;AACF,cAAM,gBAAgB,MAAM,aAAa,cAAc;AAAA,UACrD;AAAA,QACF,CAAC;AAED,YAAI,iBAAiB,cAAc,SAAS,GAAG;AAC7C,gBAAM,UAAU,cAAc,CAAC,EAAE;AAEjC,gBAAM,QAAQ,QAAQ,MAAM,KAAK,EAAE,MAAM,GAAG,CAAC,EAAE,KAAK,GAAG;AACvD,cAAI,MAAM,SAAS,GAAG;AACpB,yBAAa,UAAU,KAAK,GAC1B,MAAM,SAAS,QAAQ,SAAS,QAAQ,EAC1C;AAAA,UACF;AAAA,QACF;AAAA,MACF,SAAS,OAAO;AACd,gBAAQ;AAAA,UACN;AAAA,UACA;AAAA,QACF;AAAA,MAEF;AAAA,IACF;AAGA,UAAM,mBAAmB,UAAU,kBAC/B,KAAK,UAAU,UAAU,eAAe,IACxC;AAEJ,UAAM,WAAW,UAAU,WACvB,KAAK,UAAU,UAAU,QAAQ,IACjC;AAGJ,UAAM,mBAAkB,oBAAI,KAAK,GAAE,YAAY;AAI/C,UAAM,aAAa,4BAA4B;AAE/C,UAAM,cAAc;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAapB,UAAM,SAAS;AAAA,MACb;AAAA,MACA;AAAA,MACA;AAAA,MACA,UAAU,eAAe;AAAA,MACzB;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAEA,UAAM,aAAa,aAAa,MAAM;AAGtC,UAAM,aAAa,2BAA2B;AAE9C,YAAQ,IAAI,8BAA8B,UAAU,KAAK,QAAQ,GAAG;AAGpE,WAAO;AAAA,EACT,SAAS,OAAO;AACd,YAAQ,MAAM,qCAAqC,KAAK;AACxD,UAAM,IAAI,MAAM,uCAAuC,MAAM,OAAO,EAAE;AAAA,EACxE;AACF;AAgXA,eAAsB,oBAAoB,gBAAgB;AACxD,MAAI;AAEF,UAAM,QAAQ;AAAA;AAAA;AAAA;AAAA;AAMd,UAAM,SAAS,MAAM,aAAa,OAAO,CAAC,cAAc,CAAC;AAEzD,QAAI,CAAC,UAAU,OAAO,WAAW,GAAG;AAClC,aAAO,EAAE,YAAY,CAAC,GAAG,UAAU,CAAC,EAAE;AAAA,IACxC;AAGA,UAAM,WAAW,CAAC;AAElB,eAAW,SAAS,QAAQ;AAE1B,UAAI;AAEF,cAAM,mBAAmB,MAAM,mBAC3B,KAAK,MAAM,MAAM,gBAAgB,IACjC,CAAC;AAGL,cAAM,WAAW,MAAM,WAAW,KAAK,MAAM,MAAM,QAAQ,IAAI,CAAC;AAGhE,cAAM,WAAW,CAAC;AAGlB,iBAAS,MAAM,QAAQ,IAAI;AAAA,MAC7B,SAAS,WAAW;AAClB,gBAAQ;AAAA,UACN,uCAAuC,MAAM,QAAQ;AAAA,UACrD;AAAA,QACF;AAEA,cAAM,mBAAmB,CAAC;AAC1B,cAAM,WAAW,CAAC;AAClB,cAAM,WAAW,CAAC;AAGlB,iBAAS,MAAM,QAAQ,IAAI;AAAA,MAC7B;AAAA,IACF;AAGA,UAAM,aAAa,CAAC;AAEpB,eAAW,SAAS,QAAQ;AAC1B,UAAI,MAAM,mBAAmB,SAAS,MAAM,eAAe,GAAG;AAE5D,iBAAS,MAAM,eAAe,EAAE,SAAS,KAAK,KAAK;AAAA,MACrD,OAAO;AAEL,mBAAW,KAAK,KAAK;AAAA,MACvB;AAAA,IACF;AAEA,WAAO,EAAE,YAAY,SAAS;AAAA,EAChC,SAAS,OAAO;AACd,YAAQ;AAAA,MACN,mDAAmD,cAAc;AAAA,MACjE;AAAA,IACF;AACA,UAAM,IAAI,MAAM,oCAAoC,MAAM,OAAO,EAAE;AAAA,EACrE;AACF;;;AEz3BA;AACA,SAAS,MAAMC,eAAc;AAa7B,IAAM,gBAAgB;AAAA,EACpB,WAAW;AAAA,IACT,UAAU;AAAA,MACR;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,IACA,UAAU;AAAA,MACR;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,IACA,QAAQ;AAAA,EACV;AAAA,EAEA,kBAAkB;AAAA,IAChB,UAAU;AAAA,MACR;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,IACA,UAAU;AAAA,MACR;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,IACA,QAAQ;AAAA,EACV;AAAA,EAEA,aAAa;AAAA,IACX,UAAU;AAAA,MACR;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,IACA,UAAU;AAAA,MACR;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,IACA,QAAQ;AAAA,EACV;AAAA,EAEA,UAAU;AAAA,IACR,UAAU;AAAA,MACR;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,IACA,UAAU;AAAA,MACR;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,IACA,QAAQ;AAAA,EACV;AAAA,EAEA,iBAAiB;AAAA,IACf,UAAU;AAAA,MACR;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,IACA,UAAU;AAAA,MACR;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,IACA,QAAQ;AAAA,EACV;AAAA,EAEA,cAAc;AAAA,IACZ,UAAU;AAAA,MACR;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,IACA,UAAU;AAAA,MACR;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,IACA,QAAQ;AAAA,EACV;AAAA,EAEA,aAAa;AAAA,IACX,UAAU;AAAA,MACR;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,IACA,UAAU;AAAA,MACR;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,IACA,QAAQ;AAAA,EACV;AAAA,EAEA,eAAe;AAAA,IACb,UAAU;AAAA,MACR;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,IACA,UAAU;AAAA,MACR;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,IACA,QAAQ;AAAA;AAAA,EACV;AACF;AAQA,eAAsB,0BAA0B,UAAU;AACxD,MAAI;AACF,QAAI,CAAC,YAAY,SAAS,WAAW,GAAG;AACtC,aAAO,EAAE,aAAa,iBAAiB,YAAY,IAAI;AAAA,IACzD;AAGA,QAAI,sBAAsB;AAC1B,UAAM,eAAe,SAAS,OAAO,CAAC,QAAQ,IAAI,SAAS,MAAM;AAEjE,QAAI,aAAa,SAAS,GAAG;AAE3B,4BAAsB,aAAa,IAAI,CAAC,QAAQ,IAAI,OAAO,EAAE,KAAK,GAAG;AAAA,IACvE,OAAO;AAEL,4BAAsB,SAAS,IAAI,CAAC,QAAQ,IAAI,OAAO,EAAE,KAAK,GAAG;AAAA,IACnE;AAGA,UAAM,SAA4B,SAAS,mBAAmB;AAC9D,UAAM,oBAAuC,gBAAgB,QAAQ,EAAE;AAGvE,UAAM,gBAAgB,CAAC;AAEvB,eAAW,CAAC,aAAa,WAAW,KAAK,OAAO,QAAQ,aAAa,GAAG;AACtE,UAAI,QAAQ;AAGZ,iBAAW,WAAW,YAAY,UAAU;AAC1C,YAAI,oBAAoB,YAAY,EAAE,SAAS,QAAQ,YAAY,CAAC,GAAG;AACrE,mBAAS;AAAA,QACX;AAGA,YACE,kBAAkB;AAAA,UAChB,CAAC,MACC,OAAO,MAAM,YAAY,EAAE,YAAY,MAAM,QAAQ,YAAY;AAAA,QACrE,GACA;AACA,mBAAS;AAAA,QACX;AAAA,MACF;AAGA,iBAAW,WAAW,YAAY,UAAU;AAC1C,YAAI,QAAQ,KAAK,mBAAmB,GAAG;AACrC,mBAAS;AAAA,QACX;AAAA,MACF;AAGA,eAAS,YAAY;AAGrB,oBAAc,WAAW,IAAI;AAAA,IAC/B;AAGA,QAAI,eAAe;AACnB,QAAI,kBAAkB;AAEtB,eAAW,CAAC,aAAa,KAAK,KAAK,OAAO,QAAQ,aAAa,GAAG;AAChE,UAAI,QAAQ,cAAc;AACxB,uBAAe;AACf,0BAAkB;AAAA,MACpB;AAAA,IACF;AAIA,UAAM,mBACJ,cAAc,eAAe,EAAE,SAAS,SAAS;AAAA,IACjD,cAAc,eAAe,EAAE,SAAS,SAAS;AAInD,QAAI,aACF,MACA,OACG,gBACE,mBAAmB,cAAc,eAAe,EAAE;AAGzD,iBAAa,KAAK,IAAI,YAAY,CAAG;AAGrC,QAAI,eAAe,KAAK,oBAAoB,iBAAiB;AAC3D,aAAO,EAAE,aAAa,iBAAiB,YAAY,IAAI;AAAA,IACzD;AAEA,WAAO,EAAE,aAAa,iBAAiB,WAAW;AAAA,EACpD,SAAS,OAAO;AACd,YAAQ,MAAM,yCAAyC,KAAK;AAE5D,WAAO,EAAE,aAAa,iBAAiB,YAAY,IAAI;AAAA,EACzD;AACF;AAkBA,eAAsB,iBAAiB,gBAAgB;AACrD,MAAI;AAEF,UAAM,QAAQ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAQd,UAAM,SAAS,MAAM,aAAa,OAAO,CAAC,cAAc,CAAC;AAGzD,UAAM,OACJ,UAAU,OAAO,QAAQ,MAAM,QAAQ,OAAO,IAAI,IAC9C,OAAO,OACP,MAAM,QAAQ,MAAM,IACpB,SACA,CAAC;AAGP,QAAI,KAAK,WAAW,GAAG;AACrB,aAAO;AAAA,IACT;AAGA,WAAO,KAAK,CAAC;AAAA,EACf,SAAS,OAAO;AACd,YAAQ;AAAA,MACN,iDAAiD,cAAc;AAAA,MAC/D;AAAA,IACF;AACA,UAAM,IAAI,MAAM,iCAAiC,MAAM,OAAO,EAAE;AAAA,EAClE;AACF;AAUA,eAAsB,uBACpB,gBACA,gBACA,YACA;AACA,MAAI;AAEF,UAAM,gBAAgB,MAAM,iBAAiB,cAAc;AAG3D,QAAI,iBAAiB,cAAc,iBAAiB,gBAAgB;AAClE,YAAM,eAAc,oBAAI,KAAK,GAAE,YAAY;AAE3C,YAAM,cAAc;AAAA;AAAA;AAAA;AAAA;AAMpB,YAAM,aAAa,aAAa,CAAC,aAAa,cAAc,UAAU,CAAC;AAEvE,cAAQ;AAAA,QACN,kBAAkB,cAAc,YAAY,qBAAqB,cAAc;AAAA,MACjF;AAAA,IACF,WAAW,iBAAiB,cAAc,iBAAiB,gBAAgB;AAEzE,aAAO,cAAc;AAAA,IACvB;AAGA,UAAM,aAAaA,QAAO;AAG1B,UAAM,mBAAkB,oBAAI,KAAK,GAAE,YAAY;AAG/C,UAAM,cAAc;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAUpB,UAAM,SAAS;AAAA,MACb;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAEA,UAAM,aAAa,aAAa,MAAM;AAEtC,YAAQ;AAAA,MACN,+BAA+B,cAAc,KAAK,UAAU,sBAAsB,cAAc;AAAA,IAClG;AAGA,WAAO;AAAA,EACT,SAAS,OAAO;AACd,YAAQ;AAAA,MACN,sDAAsD,cAAc;AAAA,MACpE;AAAA,IACF;AACA,UAAM,IAAI,MAAM,uCAAuC,MAAM,OAAO,EAAE;AAAA,EACxE;AACF;AAsqBA,eAAsB,qBAAqB,gBAAgB,cAAc;AACvE,MAAI;AAEF,UAAM,UAAU;AAAA,MACd,SAAS;AAAA,MACT,MAAM;AAAA,IACR;AAGA,UAAM,SAAS,MAAM,0BAA0B,CAAC,OAAO,CAAC;AAExD,QAAI,CAAC,UAAU,CAAC,OAAO,aAAa;AAElC,aAAO,cAAc;AACrB,aAAO,aAAa;AAAA,IACtB;AAIA,QAAI;AACF,YAAM;AAAA,QACJ;AAAA,QACA,OAAO;AAAA,QACP,OAAO;AAAA,MACT;AAEA,cAAQ;AAAA,QACN,oCAAoC,cAAc,KAAK,OAAO,WAAW,KAAK,OAAO,UAAU;AAAA,MACjG;AAAA,IACF,SAAS,eAAe;AACtB,cAAQ,MAAM,sCAAsC,aAAa;AACjE,cAAQ;AAAA,QACN;AAAA,MACF;AAAA,IACF;AAEA,WAAO;AAAA,EACT,SAAS,OAAO;AACd,YAAQ,MAAM,oCAAoC,KAAK;AAGvD,WAAO;AAAA,MACL,aAAa;AAAA,MACb,YAAY;AAAA,IACd;AAAA,EACF;AACF;AAUA,eAAsB,iBACpB,gBACA,aACA,YACA;AACA,MAAI;AACF,QAAI,CAAC,gBAAgB;AACnB,YAAM,IAAI,MAAM,6BAA6B;AAAA,IAC/C;AAEA,QAAI,CAAC,aAAa;AAChB,YAAM,IAAI,MAAM,0BAA0B;AAAA,IAC5C;AAGA,iBAAa,KAAK,IAAI,GAAG,KAAK,IAAI,GAAG,UAAU,CAAC;AAGhD,UAAM,SAAS;AAAA;AAAA;AAAA;AAAA;AAKf,UAAM,aAAa,QAAQ,EAAC,oBAAI,KAAK,GAAE,YAAY,GAAG,cAAc,CAAC;AAGrE,UAAM,YAAYC,QAAO;AACzB,UAAM,kBAAiB,oBAAI,KAAK,GAAE,YAAY;AAE9C,UAAM,SAAS;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAWf,UAAM,aAAa,QAAQ;AAAA,MACzB;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAED,YAAQ;AAAA,MACN,uCAAuC,cAAc,OAAO,WAAW,KAAK,UAAU;AAAA,IACxF;AAAA,EACF,SAAS,OAAO;AACd,YAAQ,MAAM,iCAAiC,KAAK;AACpD,UAAM,IAAI,MAAM,mCAAmC,MAAM,OAAO;AAAA,EAClE;AACF;;;ANjuCA,eAAsB,cACpB,gBACA,MACA,gBACA,0BAA0B,CAAC,GAC3B,gBACA;AACA,MAAI;AAEF,UAAM,aAAaC,QAAO;AAC1B,UAAM,aAAY,oBAAI,KAAK,GAAE,YAAY;AAGzC,YAAQ,IAAI,oCAAoC;AAChD,YAAQ,IAAI,mBAAmB;AAC/B,YAAQ,IAAI,iBAAiB,UAAU;AACvC,YAAQ,IAAI,sBAAsB,cAAc;AAChD,YAAQ,IAAI,WAAW,IAAI;AAC3B,YAAQ;AAAA,MACN;AAAA,MACA,kBACE,eAAe,UAAU,GAAG,EAAE,KAC3B,eAAe,SAAS,KAAK,QAAQ;AAAA,IAC5C;AACA,YAAQ,IAAI,gBAAgB,SAAS;AACrC,YAAQ,IAAI,uBAAuB,kBAAkB,MAAM;AAC3D,YAAQ;AAAA,MACN;AAAA,MACA,KAAK,UAAU,2BAA2B,CAAC,CAAC;AAAA,IAC9C;AAGA,QAAI,mBAAmB,CAAC;AACxB,QAAI,SAAS,UAA6B,gCAAgC;AAExE,yBACqB;AAAA,QACjB;AAAA,QACA;AAAA,MACF,KAAK,CAAC;AAAA,IACV,OAAO;AAEL,UAAI,eAAe,SAAS,GAAG;AAAG,yBAAiB,KAAK,UAAU;AAClE,UAAI,eAAe,SAAS,GAAG;AAAG,yBAAiB,KAAK,UAAU;AAAA,IACpE;AAGA,UAAM,mBAAmB;AAAA,MACvB;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AACA,UAAM,mBAAmB;AAAA,MACvB;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AACA,UAAM,gBAAgB,iBAAiB;AAAA,MAAO,CAAC,OAC7C,eAAe,YAAY,EAAE,SAAS,EAAE;AAAA,IAC1C;AACA,UAAM,gBAAgB,iBAAiB;AAAA,MAAO,CAAC,OAC7C,eAAe,YAAY,EAAE,SAAS,EAAE;AAAA,IAC1C;AACA,UAAM,uBAAuB;AAAA,MAC3B,mBAAmB;AAAA,MACnB,mBAAmB;AAAA,IACrB;AAGA,UAAM,gBAAgB;AAAA,MACpB;AAAA,MACA,iBAAiB;AAAA,MACjB;AAAA,MACA,SAAS;AAAA,MACT;AAAA,MACA,yBAAyB,KAAK,UAAU,2BAA2B,CAAC,CAAC;AAAA,MACrE,SAAS;AAAA,MACT,YAAY;AAAA,MACZ,gBAAgB,kBAAkB;AAAA,MAClC,kBAAkB,KAAK,UAAU,gBAAgB;AAAA,MACjD,sBAAsB,KAAK,UAAU,oBAAoB;AAAA,IAC3D;AAEA,YAAQ,IAAI,iCAAiC;AAAA,MAC3C,YAAY,cAAc;AAAA,MAC1B,iBAAiB,cAAc;AAAA,MAC/B,MAAM,cAAc;AAAA,IACtB,CAAC;AAGD,UAA0B,yBAAyB,aAAa;AAEhE,YAAQ,IAAI,uCAAuC;AACnD,YAAQ,IAAI,0CAA0C,UAAU;AAGhE,WAAO;AAAA,EACT,SAAS,OAAO;AACd,YAAQ,MAAM,oCAAoC;AAClD,YAAQ,MAAM,6BAA6B,KAAK;AAChD,YAAQ,MAAM,gBAAgB,MAAM,KAAK;AACzC,UAAM,IAAI,MAAM,+BAA+B,MAAM,OAAO;AAAA,EAC9D;AACF;AA6BA,eAAsB,sBACpB,gBACA,eAAe,OACf;AACA,MAAI,cAAc;AAEhB,WAAO,MAA4B,oBAAoB,cAAc;AAAA,EACvE;AAEA,QAAM,QAAQ;AAAA;AAAA;AAAA;AAAA;AAKd,QAAM,SAAS,MAAM,aAAa,OAAO,CAAC,cAAc,CAAC;AACzD,MAAI,CAAC,UAAU,OAAO,WAAW;AAAG,WAAO,CAAC;AAE5C,SAAO,OAAO,IAAI,CAAC,UAAU;AAC3B,QAAI;AACF,YAAM,mBAAmB,MAAM,mBAC3B,KAAK,MAAM,MAAM,gBAAgB,IACjC,CAAC;AACL,YAAM,WAAW,MAAM,WAAW,KAAK,MAAM,MAAM,QAAQ,IAAI,CAAC;AAAA,IAClE,SAAS,KAAK;AACZ,YAAM,mBAAmB,MAAM,oBAAoB,CAAC;AACpD,YAAM,WAAW,MAAM,YAAY,CAAC;AAAA,IACtC;AACA,WAAO;AAAA,EACT,CAAC;AACH;AAsLA,eAAsB,sBAAsB,gBAAgB;AAE1D,QAAM,QAAQ;AAAA;AAAA;AAAA;AAAA;AAKd,QAAM,WAAW,MAAM,aAAa,OAAO,CAAC,cAAc,CAAC;AAC3D,MAAI,CAAC,YAAY,SAAS,WAAW;AAAG,WAAO;AAG/C,QAAM,eAAe,SAClB,IAAI,CAAC,MAAM,GAAG,EAAE,IAAI,KAAK,EAAE,OAAO,EAAE,EACpC,KAAK,IAAI;AAGZ,QAAM,UAAU,MAA6B,cAAc,cAAc;AAAA,IACvE,cAAc;AAAA,IACd,mBAAmB;AAAA,EACrB,CAAC;AAGD,SAAO;AACT;AASA,eAAsB,uBAAuB,gBAAgB,cAAc;AACzE,MAAI;AAEF,UAAM,aAAY,oBAAI,KAAK,GAAE,YAAY;AACzC,UAAM,YAAYC,QAAO;AAEzB,UAAM,QAAQ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAad,UAAM,aAAa,OAAO;AAAA,MACxB;AAAA,MACA;AAAA,MACA;AAAA,MACA,gBAAgB;AAAA,MAChB;AAAA,MACA,KAAK,UAAU,CAAC,CAAC;AAAA,MACjB;AAAA,MACA;AAAA,IACF,CAAC;AAGD,QAAI,cAAc;AAChB,YAAkC;AAAA,QAChC;AAAA,QACA;AAAA,MACF;AAAA,IACF;AAGA,UAA4B;AAAA,MAC1B;AAAA,MACA;AAAA,MACA;AAAA,QACE,MAAM;AAAA,QACN,aAAa,gBAAgB;AAAA,QAC7B,iBAAiB,CAAC;AAAA,QAClB,UAAU,CAAC;AAAA,MACb;AAAA,IACF;AAEA,YAAQ,IAAI,qCAAqC,cAAc,EAAE;AAAA,EACnE,SAAS,OAAO;AACd,YAAQ,MAAM,oCAAoC,KAAK;AACvD,UAAM,IAAI,MAAM,wCAAwC,MAAM,OAAO;AAAA,EACvE;AACF;AAUA,eAAsB,uBACpB,gBACA,QAAQ,IACR,SAAS,GACT;AACA,MAAI;AACF,QAAI,CAAC,gBAAgB;AACnB,YAAM,IAAI,MAAM,6BAA6B;AAAA,IAC/C;AAEA,UAAM,QAAQ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAsBd,UAAM,UAAU,MAAM,aAAa,OAAO,CAAC,gBAAgB,OAAO,MAAM,CAAC;AAGzE,QAAI,CAAC,WAAW,CAAC,QAAQ,QAAQ,CAAC,MAAM,QAAQ,QAAQ,IAAI,GAAG;AAC7D,cAAQ,KAAK,wDAAwD;AACrE,aAAO,CAAC;AAAA,IACV;AAGA,WAAO,QAAQ,KAAK,IAAI,CAAC,YAAY;AACnC,UAAI;AAEF,cAAM,gBAAgB;AAAA,UACpB,WAAW,QAAQ;AAAA,UACnB,gBAAgB,QAAQ;AAAA,UACxB,MAAM,QAAQ;AAAA,UACd,SAAS,QAAQ;AAAA,UACjB,WAAW,QAAQ;AAAA,UACnB,yBAAyB,CAAC;AAAA,UAC1B,SAAS,QAAQ;AAAA,UACjB,YAAY,QAAQ;AAAA,UACpB,gBAAgB,QAAQ;AAAA,UACxB,iBAAiB,CAAC;AAAA,UAClB,qBAAqB,CAAC;AAAA,QACxB;AAEA,YAAI,QAAQ,4BAA4B;AACtC,wBAAc,0BAA0B,KAAK;AAAA,YAC3C,QAAQ;AAAA,UACV;AAAA,QACF;AAEA,YAAI,QAAQ,kBAAkB;AAC5B,wBAAc,kBAAkB,KAAK,MAAM,QAAQ,gBAAgB;AAAA,QACrE;AAEA,YAAI,QAAQ,sBAAsB;AAChC,wBAAc,sBAAsB,KAAK;AAAA,YACvC,QAAQ;AAAA,UACV;AAAA,QACF;AAEA,eAAO;AAAA,MACT,SAAS,KAAK;AACZ,gBAAQ;AAAA,UACN;AAAA,UACA;AAAA,QACF;AACA,eAAO;AAAA,UACL,WAAW,QAAQ;AAAA,UACnB,gBAAgB,QAAQ;AAAA,UACxB,MAAM,QAAQ;AAAA,UACd,SAAS,QAAQ;AAAA,UACjB,WAAW,QAAQ;AAAA,UACnB,yBAAyB,CAAC;AAAA,UAC1B,SAAS,QAAQ;AAAA,UACjB,YAAY,QAAQ;AAAA,UACpB,gBAAgB,QAAQ;AAAA,UACxB,iBAAiB,CAAC;AAAA,UAClB,qBAAqB,CAAC;AAAA,QACxB;AAAA,MACF;AAAA,IACF,CAAC;AAAA,EACH,SAAS,OAAO;AACd,YAAQ;AAAA,MACN,0CAA0C,cAAc;AAAA,MACxD;AAAA,IACF;AACA,WAAO,CAAC;AAAA,EACV;AACF;AAQA,eAAsB,uBAAuB,gBAAgB;AAC3D,MAAI;AACF,QAAI,CAAC,gBAAgB;AACnB,YAAM,IAAI,MAAM,6BAA6B;AAAA,IAC/C;AAGA,UAAM,gBAAgB,MAAkC;AAAA,MACtD;AAAA,IACF;AAEA,QAAI,CAAC,eAAe;AAElB,aAAO;AAAA,QACL,aAAa;AAAA,QACb,YAAY;AAAA,QACZ,iBAAgB,oBAAI,KAAK,GAAE,YAAY;AAAA,MACzC;AAAA,IACF;AAEA,WAAO;AAAA,EACT,SAAS,OAAO;AACd,YAAQ;AAAA,MACN,0CAA0C,cAAc;AAAA,MACxD;AAAA,IACF;AAGA,WAAO;AAAA,MACL,aAAa;AAAA,MACb,YAAY;AAAA,MACZ,iBAAgB,oBAAI,KAAK,GAAE,YAAY;AAAA,IACzC;AAAA,EACF;AACF;AASA,eAAsB,kBAAkB,gBAAgB,QAAQ,GAAG;AACjE,MAAI;AACF,QAAI,CAAC,gBAAgB;AACnB,YAAM,IAAI,MAAM,6BAA6B;AAAA,IAC/C;AAEA,UAAM,QAAQ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAsBd,UAAM,UAAU,MAAM,aAAa,OAAO,CAAC,gBAAgB,KAAK,CAAC;AAGjE,QAAI,CAAC,WAAW,CAAC,QAAQ,QAAQ,CAAC,MAAM,QAAQ,QAAQ,IAAI,GAAG;AAC7D,cAAQ,KAAK,mDAAmD;AAChE,aAAO,CAAC;AAAA,IACV;AAGA,WAAO,QAAQ,KAAK,IAAI,CAAC,YAAY;AACnC,UAAI;AAEF,cAAM,gBAAgB;AAAA,UACpB,WAAW,QAAQ;AAAA,UACnB,gBAAgB,QAAQ;AAAA,UACxB,MAAM,QAAQ;AAAA,UACd,SAAS,QAAQ;AAAA,UACjB,WAAW,QAAQ;AAAA,UACnB,yBAAyB,CAAC;AAAA,UAC1B,SAAS,QAAQ;AAAA,UACjB,YAAY,QAAQ;AAAA,UACpB,gBAAgB,QAAQ;AAAA,UACxB,iBAAiB,CAAC;AAAA,UAClB,qBAAqB,CAAC;AAAA,QACxB;AAEA,YAAI,QAAQ,4BAA4B;AACtC,wBAAc,0BAA0B,KAAK;AAAA,YAC3C,QAAQ;AAAA,UACV;AAAA,QACF;AAEA,YAAI,QAAQ,kBAAkB;AAC5B,wBAAc,kBAAkB,KAAK,MAAM,QAAQ,gBAAgB;AAAA,QACrE;AAEA,YAAI,QAAQ,sBAAsB;AAChC,wBAAc,sBAAsB,KAAK;AAAA,YACvC,QAAQ;AAAA,UACV;AAAA,QACF;AAEA,eAAO;AAAA,MACT,SAAS,KAAK;AACZ,gBAAQ;AAAA,UACN;AAAA,UACA;AAAA,QACF;AACA,eAAO;AAAA,UACL,WAAW,QAAQ;AAAA,UACnB,gBAAgB,QAAQ;AAAA,UACxB,MAAM,QAAQ;AAAA,UACd,SAAS,QAAQ;AAAA,UACjB,WAAW,QAAQ;AAAA,UACnB,yBAAyB,CAAC;AAAA,UAC1B,SAAS,QAAQ;AAAA,UACjB,YAAY,QAAQ;AAAA,UACpB,gBAAgB,QAAQ;AAAA,UACxB,iBAAiB,CAAC;AAAA,UAClB,qBAAqB,CAAC;AAAA,QACxB;AAAA,MACF;AAAA,IACF,CAAC;AAAA,EACH,SAAS,OAAO;AACd,YAAQ;AAAA,MACN,qCAAqC,cAAc;AAAA,MACnD;AAAA,IACF;AACA,WAAO,CAAC;AAAA,EACV;AACF;;;AOpuBA;AACA,SAAS,MAAMC,eAAc;;;ACD7B;AADA,SAAS,MAAMC,eAAc;AAiC7B,eAAsB,YACpB,MACA,MACA,sBAAsB,CAAC,GACvB,iBAAiB,MACjB;AACA,MAAI;AAEF,UAAM,UAAUA,QAAO;AAGvB,UAAM,WAAW,KAAK,UAAU,IAAI;AACpC,UAAM,gBAAgB,KAAK,UAAU,mBAAmB;AAGxD,UAAM,YAAY,KAAK,IAAI;AAG3B,UAAM,QAAQ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAYd,UAAM,aAAa,OAAO;AAAA,MACxB;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAED,WAAO;AAAA,EACT,SAAS,OAAO;AACd,YAAQ,MAAM,mCAAmC,IAAI,MAAM,KAAK;AAChE,UAAM;AAAA,EACR;AACF;AAWA,eAAsB,eACpB,mBACA,OAAO,MACP,cAAc,MACd,oBAAoB,MACpB;AACA,MAAI;AAEF,UAAM,cAAcA,QAAO;AAG3B,UAAM,gBAAgB,KAAK,UAAU,iBAAiB;AAGtD,UAAM,QAAQ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAWd,UAAM,aAAa,OAAO;AAAA,MACxB;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAED,WAAO;AAAA,EACT,SAAS,OAAO;AACd,YAAQ,MAAM,oCAAoC,KAAK;AACvD,UAAM;AAAA,EACR;AACF;AA+JA,eAAsB,UAAU,UAAU,CAAC,GAAG;AAC5C,MAAI;AACF,UAAM;AAAA,MACJ;AAAA,MACA;AAAA,MACA;AAAA,MACA,oBAAoB;AAAA,MACpB;AAAA,IACF,IAAI;AAGJ,QAAI,QAAQ;AACZ,UAAM,SAAS,CAAC;AAGhB,QAAI,SAAS,MAAM,SAAS,GAAG;AAC7B,eAAS,uBAAuB,MAAM,IAAI,MAAM,GAAG,EAAE,KAAK,GAAG,CAAC;AAC9D,aAAO,KAAK,GAAG,KAAK;AAAA,IACtB;AAEA,QAAI,gBAAgB;AAClB,eAAS;AACT,aAAO,KAAK,cAAc;AAAA,IAC5B;AAEA,QAAI,uBAAuB;AACzB,eAAS;AACT,aAAO,KAAK,qBAAqB;AAAA,IACnC;AAIA,QAAI,CAAC,mBAAmB;AAEtB,YAAM,sBAAsB;AAAA,QAC1B;AAAA,QACA;AAAA,QACA;AAAA,MACF;AACA,eAAS,2BAA2B,oBACjC,IAAI,MAAM,GAAG,EACb,KAAK,GAAG,CAAC;AACZ,aAAO,KAAK,GAAG,mBAAmB;AAGlC,eAAS;AAAA;AAAA;AAAA;AAAA,IAIX;AAGA,aAAS;AAGT,QAAI,SAAS,OAAO,UAAU,KAAK,KAAK,QAAQ,GAAG;AACjD,eAAS;AACT,aAAO,KAAK,KAAK;AAAA,IACnB;AAGA,UAAM,SAAS,MAAM,aAAa,OAAO,MAAM;AAG/C,UAAM,OACJ,UAAU,OAAO,QAAQ,MAAM,QAAQ,OAAO,IAAI,IAC9C,OAAO,OACP,MAAM,QAAQ,MAAM,IACpB,SACA,CAAC;AAGP,QAAI,KAAK,WAAW,GAAG;AACrB,cAAQ,KAAK,gCAAgC;AAC7C,aAAO,CAAC;AAAA,IACV;AAGA,WAAO,KAAK,IAAI,CAAC,WAAW;AAAA,MAC1B,GAAG;AAAA,MACH,MAAM,KAAK,MAAM,MAAM,QAAQ,IAAI;AAAA,MACnC,uBAAuB,KAAK,MAAM,MAAM,yBAAyB,IAAI;AAAA,IACvE,EAAE;AAAA,EACJ,SAAS,OAAO;AACd,YAAQ,MAAM,qCAAqC,KAAK;AACxD,UAAM;AAAA,EACR;AACF;AAuGA,eAAsB,+BACpB,gBACA,QAAQ,IACR,aAAa,MACb;AACA,MAAI;AACF,QAAI,CAAC,gBAAgB;AACnB,YAAM,IAAI,MAAM,6BAA6B;AAAA,IAC/C;AAGA,QAAI,QAAQ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAcZ,UAAM,SAAS,CAAC,cAAc;AAG9B,QAAI,cAAc,MAAM,QAAQ,UAAU,KAAK,WAAW,SAAS,GAAG;AACpE,YAAM,eAAe,WAAW,IAAI,MAAM,GAAG,EAAE,KAAK,GAAG;AACvD,eAAS,uBAAuB,YAAY;AAC5C,aAAO,KAAK,GAAG,UAAU;AAAA,IAC3B;AAGA,aAAS;AAAA;AAAA;AAAA;AAAA;AAKT,WAAO,KAAK,KAAK;AAGjB,UAAM,UAAU,MAAM,aAAa,OAAO,MAAM;AAGhD,UAAM,OACJ,WAAW,QAAQ,QAAQ,MAAM,QAAQ,QAAQ,IAAI,IACjD,QAAQ,OACR,MAAM,QAAQ,OAAO,IACrB,UACA,CAAC;AAGP,QAAI,KAAK,WAAW,GAAG;AACrB,cAAQ,KAAK,4CAA4C,cAAc;AACvE,aAAO,CAAC;AAAA,IACV;AAGA,WAAO,KAAK,IAAI,CAAC,WAAW;AAAA,MAC1B,GAAG;AAAA,MACH,MAAM,KAAK,MAAM,MAAM,QAAQ,IAAI;AAAA,MACnC,uBAAuB,KAAK,MAAM,MAAM,yBAAyB,IAAI;AAAA,IACvE,EAAE;AAAA,EACJ,SAAS,OAAO;AACd,YAAQ;AAAA,MACN,gDAAgD,cAAc;AAAA,MAC9D;AAAA,IACF;AACA,WAAO,CAAC;AAAA,EACV;AACF;;;ADneO,SAAS,qBAAqB,OAAO,sBAAsB,CAAC,GAAG;AAEpE,QAAM,UAAU;AAAA,IACd,eAAe;AAAA,IACf,aAAa;AAAA,IACb,qBAAqB;AAAA,IACrB,kBAAkB;AAAA,IAClB,wBAAwB;AAAA,IACxB,wBAAwB;AAAA,IACxB,uBAAuB;AAAA,EACzB;AAGA,QAAM,eAAe;AAAA,IACnB,CAAC,QAAQ,aAAa,GAAG;AAAA;AAAA,IACzB,CAAC,QAAQ,WAAW,GAAG;AAAA,IACvB,CAAC,QAAQ,mBAAmB,GAAG;AAAA,IAC/B,CAAC,QAAQ,gBAAgB,GAAG;AAAA,IAC5B,CAAC,QAAQ,sBAAsB,GAAG;AAAA,IAClC,CAAC,QAAQ,sBAAsB,GAAG;AAAA,IAClC,CAAC,QAAQ,qBAAqB,GAAG;AAAA,EACnC;AAGA,QAAM,kBAAkB,MAAM,YAAY;AAG1C,QAAM,SAA4B,SAAS,KAAK;AAChD,QAAM,WAA8B,gBAAgB,MAAM;AAG1D,MAAI,gBAAgB,SAAS,GAAG,GAAG;AACjC,iBAAa,QAAQ,mBAAmB,KAAK;AAAA,EAC/C;AAGA,QAAM,eAAe;AAAA,IACnB;AAAA;AAAA,IACA;AAAA;AAAA,IACA;AAAA;AAAA,IACA;AAAA;AAAA,IACA;AAAA;AAAA,EACF;AAEA,aAAW,WAAW,cAAc;AAClC,QAAI,QAAQ,KAAK,KAAK,GAAG;AACvB,mBAAa,QAAQ,WAAW,KAAK;AACrC,mBAAa,QAAQ,gBAAgB,KAAK;AAC1C;AAAA,IACF;AAAA,EACF;AAGA,QAAM,kBAAkB;AAAA;AAAA,IAEtB;AAAA,MACE,UAAU,CAAC,QAAQ,UAAU,YAAY,UAAU,UAAU;AAAA,MAC7D,QAAQ,QAAQ;AAAA,MAChB,OAAO;AAAA,IACT;AAAA;AAAA,IAEA;AAAA,MACE,UAAU;AAAA,QACR;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MACF;AAAA,MACA,QAAQ,QAAQ;AAAA,MAChB,OAAO;AAAA,IACT;AAAA;AAAA,IAEA;AAAA,MACE,UAAU;AAAA,QACR;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MACF;AAAA,MACA,QAAQ,QAAQ;AAAA,MAChB,OAAO;AAAA,IACT;AAAA;AAAA,IAEA;AAAA,MACE,UAAU;AAAA,QACR;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MACF;AAAA,MACA,QAAQ,QAAQ;AAAA,MAChB,OAAO;AAAA,IACT;AAAA;AAAA,IAEA;AAAA,MACE,UAAU;AAAA,QACR;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MACF;AAAA,MACA,QAAQ,QAAQ;AAAA,MAChB,OAAO;AAAA,IACT;AAAA;AAAA,IAEA;AAAA,MACE,UAAU;AAAA,QACR;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MACF;AAAA,MACA,QAAQ,QAAQ;AAAA,MAChB,OAAO;AAAA,IACT;AAAA,EACF;AAEA,aAAW,EAAE,UAAU,QAAQ,MAAM,KAAK,iBAAiB;AACzD,eAAW,WAAW,UAAU;AAC9B,UAAI,gBAAgB,SAAS,OAAO,GAAG;AACrC,qBAAa,MAAM,KAAK;AACxB;AAAA,MACF;AAAA,IACF;AAAA,EACF;AAGA,MAAI,uBAAuB,oBAAoB,SAAS,GAAG;AAEzD,UAAM,iBAAiB,oBACpB,MAAM,EAAE,EACR,OAAO,CAAC,QAAQ,IAAI,OAAO;AAE9B,eAAW,WAAW,gBAAgB;AACpC,YAAM,oBAAoB,QAAQ,QAAQ,YAAY;AAGtD,UACE,+DAA+D;AAAA,QAC7D;AAAA,MACF,GACA;AACA,qBAAa,QAAQ,gBAAgB,KAAK;AAAA,MAC5C;AAGA,UACE,kEAAkE;AAAA,QAChE;AAAA,MACF,GACA;AACA,qBAAa,QAAQ,sBAAsB,KAAK;AAAA,MAClD;AAGA,UACE,iDAAiD,KAAK,iBAAiB,GACvE;AACA,qBAAa,QAAQ,mBAAmB,KAAK;AAAA,MAC/C;AAAA,IACF;AAAA,EACF;AAGA,MAAI,WAAW;AACf,MAAI,iBAAiB,QAAQ;AAE7B,aAAW,CAAC,QAAQ,KAAK,KAAK,OAAO,QAAQ,YAAY,GAAG;AAC1D,QAAI,QAAQ,UAAU;AACpB,iBAAW;AACX,uBAAiB;AAAA,IACnB;AAAA,EACF;AAEA,SAAO;AAAA,IACL,QAAQ;AAAA,IACR;AAAA,EACF;AACF;AASA,eAAsB,iBACpB,iBAAiB,CAAC,GAClB,mBAAmB,CAAC,GACpB;AACA,MAAI;AAEF,UAAM,gBAAgB,oBAAI,IAAI;AAC9B,UAAM,kBAAkB,oBAAI,IAAI;AAChC,UAAM,gBAAgB,oBAAI,IAAI;AAC9B,QAAI,cAAc,oBAAI,IAAI;AAG1B,eAAW,SAAS,gBAAgB;AAElC,oBAAc;AAAA,QACZ,MAAM;AAAA,SACL,cAAc,IAAI,MAAM,UAAU,KAAK,KAAK;AAAA,MAC/C;AAGA,UAAI,MAAM,QAAQ,MAAM,KAAK,MAAM;AACjC,cAAMC,QAAO,MAAM,KAAK;AACxB,sBAAc,IAAIA,QAAO,cAAc,IAAIA,KAAI,KAAK,KAAK,CAAC;AAG1D,cAAM,WAAWA,MAAK,MAAM,GAAG;AAC/B,iBAAS,IAAI,GAAG,IAAI,SAAS,QAAQ,KAAK;AACxC,gBAAM,UAAU,SAAS,MAAM,GAAG,CAAC,EAAE,KAAK,GAAG;AAC7C,cAAI,SAAS;AACX,0BAAc,IAAI,UAAU,cAAc,IAAI,OAAO,KAAK,KAAK,GAAG;AAAA,UACpE;AAAA,QACF;AAAA,MACF;AAGA,UACE,MAAM,yBACN,MAAM,sBAAsB,SAAS,GACrC;AACA,mBAAW,YAAY,MAAM,uBAAuB;AAClD,0BAAgB;AAAA,YACd;AAAA,aACC,gBAAgB,IAAI,QAAQ,KAAK,KAAK;AAAA,UACzC;AAAA,QACF;AAAA,MACF;AAGA,UAAI,MAAM,QAAQ,OAAO,MAAM,SAAS,UAAU;AAEhD,cAAM,aAAa;AAAA,UACjB,MAAM,KAAK;AAAA,UACX,MAAM,KAAK;AAAA,UACX,MAAM,KAAK;AAAA,UACX,MAAM,KAAK;AAAA,QACb,EAAE,OAAO,OAAO;AAEhB,mBAAW,QAAQ,YAAY;AAC7B,cAAI,QAAQ,OAAO,SAAS,UAAU;AACpC,kBAAM,SAA4B,SAAS,IAAI;AAC/C,kBAAM,oBACe,gBAAgB,MAAM;AAC3C,8BAAkB,QAAQ,CAAC,YAAY,YAAY,IAAI,OAAO,CAAC;AAAA,UACjE;AAAA,QACF;AAAA,MACF;AAAA,IACF;AAGA,eAAW,QAAQ,kBAAkB;AACnC,YAAMA,QAAO,KAAK;AAElB,oBAAc,IAAIA,QAAO,cAAc,IAAIA,KAAI,KAAK,KAAK,CAAC;AAG1D,YAAM,WAAWA,MAAK,MAAM,GAAG;AAC/B,eAAS,IAAI,GAAG,IAAI,SAAS,QAAQ,KAAK;AACxC,cAAM,UAAU,SAAS,MAAM,GAAG,CAAC,EAAE,KAAK,GAAG;AAC7C,YAAI,SAAS;AACX,wBAAc,IAAI,UAAU,cAAc,IAAI,OAAO,KAAK,KAAK,GAAG;AAAA,QACpE;AAAA,MACF;AAGA,UAAI,KAAK,SAAS;AAChB,cAAM,SAA4B,SAAS,KAAK,OAAO;AACvD,cAAM,oBAAuC,gBAAgB,MAAM;AACnE,0BAAkB,QAAQ,CAAC,YAAY,YAAY,IAAI,OAAO,CAAC;AAAA,MACjE;AAAA,IACF;AAGA,QAAI,mBAAmB;AACvB,QAAI,eAAe;AACnB,QAAI,YAAY;AAEhB,eAAW,CAACA,OAAM,SAAS,KAAK,cAAc,QAAQ,GAAG;AACvD,UAAI,YAAY,cAAc;AAC5B,uBAAe;AACf,2BAAmBA;AAGnB,oBACEA,MAAK,SAAS,GAAG,KAAK,CAACA,MAAK,SAAS,GAAG,IAAI,SAAS;AAAA,MACzD;AAAA,IACF;AAGA,QAAI,CAAC,oBAAoB,cAAc,OAAO,GAAG;AAC/C,UAAI,sBAAsB;AAC1B,qBAAe;AAEf,iBAAW,CAAC,MAAM,SAAS,KAAK,cAAc,QAAQ,GAAG;AACvD,YAAI,YAAY,cAAc;AAC5B,yBAAe;AACf,gCAAsB;AAAA,QACxB;AAAA,MACF;AAEA,UAAI,qBAAqB;AACvB,2BAAmB,YAAY,mBAAmB;AAClD,oBAAY;AAAA,MACd;AAAA,IACF;AAGA,QAAI,CAAC,kBAAkB;AACrB,aAAO;AAAA,IACT;AAGA,QAAI,cAAc;AAClB,QAAI,cAAc,QAAQ;AACxB,oBAAc,mBAAmB,gBAAgB;AAAA,IACnD,WAAW,cAAc,aAAa;AACpC,oBAAc,wBAAwB,gBAAgB;AAAA,IACxD,OAAO;AACL,oBAAc,GAAG,iBAAiB,QAAQ,aAAa,EAAE,CAAC;AAAA,IAC5D;AAGA,UAAM,mBAAmB,MAAM,KAAK,gBAAgB,QAAQ,CAAC,EAC1D,KAAK,CAAC,GAAG,MAAM,EAAE,CAAC,IAAI,EAAE,CAAC,CAAC,EAC1B,MAAM,GAAG,EAAE,EACX,IAAI,CAAC,CAAC,QAAQ,MAAM,QAAQ;AAG/B,UAAM,WAAW,MAAM,KAAK,WAAW,EAAE,MAAM,GAAG,EAAE;AAGpD,UAAM,YAAY;AAAA,MAChB,UAAUC,QAAO;AAAA,MACjB,YAAY;AAAA,MACZ,YAAY;AAAA,MACZ;AAAA,MACA,oBAAoB,KAAK,UAAU,gBAAgB;AAAA,MACnD,UAAU,KAAK,UAAU,QAAQ;AAAA,MACjC,mBAAmB,KAAK,IAAI;AAAA,MAC5B,WAAW;AAAA,IACb;AAIA,QAAI;AACF,YAAM,oBAAoB,SAAS;AAAA,IACrC,SAAS,OAAO;AAEd,cAAQ,MAAM,0CAA0C,KAAK;AAAA,IAC/D;AAEA,WAAO;AAAA,EACT,SAAS,OAAO;AACd,YAAQ,MAAM,gCAAgC,KAAK;AACnD,WAAO;AAAA,EACT;AACF;AAQA,eAAsB,oBAAoB,OAAO;AAC/C,MAAI;AAEF,UAAM,mBACJ,OAAO,MAAM,uBAAuB,WAChC,MAAM,qBACN,KAAK,UAAU,MAAM,sBAAsB,CAAC,CAAC;AAEnD,UAAM,WACJ,OAAO,MAAM,aAAa,WACtB,MAAM,WACN,KAAK,UAAU,MAAM,YAAY,CAAC,CAAC;AAGzC,UAAM,gBAAgB,MAAM,qBAAqB,KAAK,IAAI;AAG1D,UAAM,aAAa,mBAAmB;AAEtC,QAAI;AAEF,YAAM;AAAA,QACJ;AAAA,MACF;AAGA,YAAM,gBAAgB,MAAM;AAAA,QAC1B;AAAA,QACA,CAAC,MAAM,UAAU;AAAA,MACnB;AAEA,UAAI,iBAAiB,cAAc,SAAS,GAAG;AAE7C,cAAM;AAAA,UACJ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,UAQA;AAAA,YACE,MAAM;AAAA,YACN,MAAM;AAAA,YACN;AAAA,YACA;AAAA,YACA;AAAA,YACA,cAAc,CAAC,EAAE;AAAA,UACnB;AAAA,QACF;AAAA,MACF,OAAO;AAEL,cAAM;AAAA,UACJ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,UAUA;AAAA,YACE,MAAM;AAAA,YACN,MAAM;AAAA,YACN,MAAM;AAAA,YACN,MAAM;AAAA,YACN;AAAA,YACA;AAAA,YACA;AAAA,UACF;AAAA,QACF;AAAA,MACF;AAGA,YAAM,aAAa,QAAQ;AAAA,IAC7B,SAAS,OAAO;AAEd,YAAM,aAAa,UAAU;AAC7B,YAAM;AAAA,IACR;AAAA,EACF,SAAS,OAAO;AACd,YAAQ,MAAM,0CAA0C,KAAK;AAC7D,UAAM;AAAA,EACR;AACF;AA0GA,eAAsB,aAAa,QAAQ;AACzC,MAAI;AACF,UAAM;AAAA,MACJ;AAAA,MACA;AAAA,MACA,SAAS;AAAA,MACT;AAAA,MACA,cAAc,CAAC;AAAA,IACjB,IAAI;AAEJ,QAAI,YAAY;AAChB,QAAI,eAAe;AACnB,QAAI,eAAe;AAGnB,QAAI,cAAc,QAAQ;AAExB,YAAM,iBAAiB,MAAM;AAAA,QAC3B;AAAA;AAAA;AAAA;AAAA;AAAA,QAKA,CAAC,cAAc;AAAA,MACjB;AAGA,YAAM,WAAW,eAAe,IAAI,CAAC,SAAS;AAAA,QAC5C,SAAS,IAAI;AAAA,QACb,MAAM,IAAI;AAAA,MACZ,EAAE;AAGF,eAAS,QAAQ;AAAA,QACf,SAAS;AAAA,QACT,MAAM;AAAA,MACR,CAAC;AAGD,YAAM,EAAE,QAAQ,SAAS,IAAI,qBAAqB,YAAY,QAAQ;AAGtE,YAAM,mBAAmB,MAAM;AAAA,QAC7B;AAAA,MACF;AAEA,UAAI,YAAY;AAChB,UAAI,oBAAoB,iBAAiB,SAAS,GAAG;AACnD,cAAM,eAAe,iBAAiB,CAAC;AAGvC,oBAAY;AAAA,UACV,GAAG;AAAA,UACH,oBAAoB,KAAK;AAAA,YACvB,aAAa,sBAAsB;AAAA,UACrC;AAAA,UACA,UAAU,KAAK,MAAM,aAAa,YAAY,IAAI;AAAA,QACpD;AAAA,MACF;AAGA,UAAI,aAAa;AAEjB,UAAI,WAAW,mBAAmB,WAAW;AAC3C,qBAAa;AAGb,YAAI,UAAU,YAAY,UAAU;AAClC,gBAAM,mBAAmB,SAAS;AAAA,YAAO,CAAC,MACxC,UAAU,SAAS,SAAS,CAAC;AAAA,UAC/B;AAEA,cAAI,iBAAiB,SAAS,GAAG;AAC/B,0BAAc,KAAK,IAAI,KAAK,iBAAiB,SAAS,IAAI;AAAA,UAC5D;AAAA,QACF;AAAA,MACF;AAGA,kBAAY;AAAA,QACV;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MACF;AAAA,IACF;AAIA,UAAM,eAAe,CAAC;AAGtB,QAAI,YAAY;AACd,mBAAa,KAAK;AAAA,QAChB,MAAM;AAAA,MACR,CAAC;AAAA,IACH;AAGA,QAAI,eAAe,YAAY,SAAS,GAAG;AACzC,mBAAa,KAAK,GAAG,WAAW;AAAA,IAClC;AAGA,UAAM,eAAe,MAA2B,UAAU;AAAA,MACxD,OAAO;AAAA,MACP,OAAO,CAAC,eAAe,aAAa,eAAe,YAAY;AAAA,IACjE,CAAC;AAGD,QAAI,aAAa,SAAS,KAAK,aAAa,SAAS,GAAG;AAEtD,YAAM,eAAe,MAAM,iBAAiB,cAAc,YAAY;AAEtE,UAAI,cAAc;AAEhB,uBAAe;AACf,uBAAe;AAAA,MACjB,OAAO;AAEL,cAAM,mBAAmB,MAAM;AAAA,UAC7B;AAAA,QACF;AAEA,YAAI,oBAAoB,iBAAiB,SAAS,GAAG;AACnD,gBAAM,eAAe,iBAAiB,CAAC;AAGvC,yBAAe;AAAA,YACb,GAAG;AAAA,YACH,oBAAoB,KAAK;AAAA,cACvB,aAAa,sBAAsB;AAAA,YACrC;AAAA,YACA,UAAU,KAAK,MAAM,aAAa,YAAY,IAAI;AAAA,UACpD;AAAA,QACF;AAAA,MACF;AAAA,IACF,OAAO;AAEL,YAAM,mBAAmB,MAAM;AAAA,QAC7B;AAAA,MACF;AAEA,UAAI,oBAAoB,iBAAiB,SAAS,GAAG;AACnD,cAAM,eAAe,iBAAiB,CAAC;AAGvC,uBAAe;AAAA,UACb,GAAG;AAAA,UACH,oBAAoB,KAAK;AAAA,YACvB,aAAa,sBAAsB;AAAA,UACrC;AAAA,UACA,UAAU,KAAK,MAAM,aAAa,YAAY,IAAI;AAAA,QACpD;AAAA,MACF;AAAA,IACF;AAGA,QAAI,aAAa,CAAC,UAAU,aAAa,cAAc;AACrD,gBAAU,YAAY;AAAA,IACxB;AAGA,WAAO;AAAA,MACL;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,EACF,SAAS,OAAO;AACd,YAAQ,MAAM,0BAA0B,KAAK;AAE7C,WAAO;AAAA,MACL,cAAc;AAAA,IAChB;AAAA,EACF;AACF;;;AEhzBA;AA4DA,eAAsB,iBAAiB,UAAU,UAAU,CAAC,GAAG;AAC7D,MAAI;AAEF,QAAI,CAAC,YAAY,CAAC,MAAM,QAAQ,QAAQ,KAAK,SAAS,WAAW,GAAG;AAClE,YAAM,IAAI,MAAM,gDAAgD;AAAA,IAClE;AAGA,QACE,SAAS,WAAW,KACpB,gCAAgC,KAAK,SAAS,CAAC,CAAC,GAChD;AAAA,IAEF,OAAO;AAEL,iBAAW,SAAS,IAAI,CAAC,OAAO,GAAG,KAAK,CAAC,EAAE,OAAO,CAAC,OAAO,GAAG,SAAS,CAAC;AAAA,IACzE;AAGA,cAAU;AAAA,MACR,UAAU;AAAA;AAAA,MACV,iBAAiB;AAAA;AAAA,MACjB,OAAO;AAAA;AAAA,MACP,GAAG;AAAA,IACL;AAGA,QAAI,gBAAgB,CAAC;AAGrB,QAAI,QAAQ,aAAa,SAAS,QAAQ,aAAa,YAAY;AACjE,YAAM,aAAa,MAAM,eAAe,UAAU,OAAO;AACzD,sBAAgB,CAAC,GAAG,UAAU;AAAA,IAChC;AAGA,QACE,QAAQ,aAAa,cACrB,QAAQ,aAAa,cACpB,QAAQ,aAAa,SAAS,cAAc,WAAW,GACxD;AACA,YAAM,iBAAiB,MAAM,oBAAoB,UAAU,OAAO;AAElE,UAAI,QAAQ,aAAa,cAAc,cAAc,SAAS,GAAG;AAE/D,wBAAgB,mBAAmB,eAAe,cAAc;AAAA,MAClE,OAAO;AACL,wBAAgB;AAAA,MAClB;AAAA,IACF;AAGA,QAAI,QAAQ,cAAc;AACxB,sBAAgB,cAAc;AAAA,QAC5B,CAAC,WAAW,OAAO,kBAAkB,QAAQ;AAAA,MAC/C;AAAA,IACF;AAGA,QAAI,QAAQ,SAAS,cAAc,SAAS,QAAQ,OAAO;AACzD,sBAAgB,cAAc,MAAM,GAAG,QAAQ,KAAK;AAAA,IACtD;AAGA,WAAO;AAAA,EACT,SAAS,OAAO;AACd,YAAQ,MAAM,8BAA8B,KAAK;AACjD,UAAM;AAAA,EACR;AACF;AASA,eAAe,eAAe,UAAU,SAAS;AAC/C,MAAI;AAEF,UAAM,oBAAoB,SAAS,IAAI,CAAC,YAAY;AAElD,YAAM,UAAU,KAAK,QAAQ,YAAY,CAAC;AAI1C,YAAM,YAAY,QAAQ;AAAA,QACxB;AAAA,QACA,CAAC,SAAS,KAAK,IAAI;AAAA,MACrB;AAEA,aAAO;AAAA,IACT,CAAC;AAID,UAAM,kBACJ,QAAQ,iBAAiB,YAAY,MAAM,QAAQ,QAAQ;AAG7D,QAAI;AAEJ,QAAI,QAAQ,eAAe;AAEzB,iBAAW,IAAI,kBAAkB,KAAK,GAAG,CAAC;AAAA,IAC5C,WAAW,QAAQ,gBAAgB,kBAAkB,SAAS,GAAG;AAE/D,YAAM,WAAW,QAAQ,qBAAqB;AAC9C,iBAAW,GAAG,kBAAkB,KAAK,SAAS,QAAQ,GAAG,CAAC;AAAA,IAC5D,OAAO;AAEL,iBAAW,kBAAkB,KAAK,IAAI,eAAe,GAAG;AAAA,IAC1D;AAIA,QACE,SAAS,WAAW,KACpB,gCAAgC,KAAK,SAAS,CAAC,CAAC,GAChD;AACA,iBAAW,SAAS,CAAC;AAAA,IACvB;AAGA,QAAI,MAAM;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAaV,UAAM,cAAc,CAAC,QAAQ;AAG7B,QAAI,QAAQ,eAAe,QAAQ,YAAY,SAAS,GAAG;AACzD,YAAM,eAAe,QAAQ,YAAY,IAAI,MAAM,GAAG,EAAE,KAAK,IAAI;AACjE,aAAO,0BAA0B,YAAY;AAC7C,kBAAY,KAAK,GAAG,QAAQ,WAAW;AAAA,IACzC;AAGA,QAAI,QAAQ,aAAa,QAAQ,UAAU,SAAS,GAAG;AACrD,aAAO;AAEP,YAAM,qBAAqB,CAAC;AAE5B,iBAAW,eAAe,QAAQ,WAAW;AAE3C,YAAI,aAAa,YACd,QAAQ,OAAO,GAAG,EAClB,QAAQ,OAAO,GAAG;AAGrB,qBAAa,WAAW,QAAQ,SAAS,GAAG;AAE5C,2BAAmB,KAAK,oBAAoB;AAC5C,oBAAY,KAAK,UAAU;AAAA,MAC7B;AAEA,aAAO,mBAAmB,KAAK,MAAM;AACrC,aAAO;AAAA,IACT;AAGA,QAAI,QAAQ,WAAW;AACrB,UAAI,QAAQ,UAAU,OAAO;AAC3B,eAAO;AACP,oBAAY,KAAK,QAAQ,UAAU,MAAM,YAAY,CAAC;AAAA,MACxD;AAEA,UAAI,QAAQ,UAAU,KAAK;AACzB,eAAO;AACP,oBAAY,KAAK,QAAQ,UAAU,IAAI,YAAY,CAAC;AAAA,MACtD;AAAA,IACF;AAGA,QAAI,QAAQ,eAAe;AACzB,aAAO,aAAa,QAAQ,aAAa;AAAA,IAC3C,OAAO;AAEL,aAAO;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAUT;AAGA,UAAM,QAAQ,QAAQ,SAAS,QAAQ,QAAQ,IAAI,QAAQ,QAAQ;AACnE,WAAO;AACP,gBAAY,KAAK,KAAK;AAGtB,UAAM,UAAU,MAAM,aAAa,KAAK,WAAW;AAGnD,WAAO,mBAAmB,OAAO;AAAA,EACnC,SAAS,OAAO;AACd,YAAQ,MAAM,4BAA4B,KAAK;AAC/C,UAAM;AAAA,EACR;AACF;AASA,eAAe,oBAAoB,UAAU,SAAS;AACpD,MAAI;AAEF,QAAI;AACJ,QAAI,SAAS,WAAW,KAAK,sBAAsB,KAAK,SAAS,CAAC,CAAC,GAAG;AAEpE,0BAAoB,SAAS,CAAC,EAC3B,MAAM,uBAAuB,EAC7B,IAAI,CAAC,SAAS,KAAK,KAAK,CAAC,EACzB,OAAO,CAAC,SAAS,KAAK,SAAS,CAAC;AAAA,IACrC,OAAO;AACL,0BAAoB;AAAA,IACtB;AAGA,UAAM,kBAAkB,kBAAkB;AAAA,MAAI,CAAC,YAC7C,KAAK,QAAQ,YAAY,CAAC;AAAA,IAC5B;AAGA,QAAI,MAAM;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,wBAYU,gBAAgB,IAAI,MAAM,GAAG,EAAE,KAAK,GAAG,CAAC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAS5D,UAAM,cAAc,CAAC,GAAG,eAAe;AAGvC,UAAM,aAAa,KAAK,SAAS,WAAW;AAG5C,QAAI,QAAQ,QAAQ;AAClB,aAAO,eAAe,QAAQ,MAAM;AAAA,IACtC,OAAO;AAEL,aAAO;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAUT;AAGA,UAAM,QAAQ,QAAQ,SAAS,QAAQ,QAAQ,IAAI,QAAQ,QAAQ;AACnE,WAAO;AACP,gBAAY,KAAK,KAAK;AAGtB,UAAM,UAAU,MAAM,aAAa,KAAK,WAAW;AAGnD,WAAO,mBAAmB,OAAO;AAAA,EACnC,SAAS,OAAO;AACd,YAAQ,MAAM,iCAAiC,KAAK;AACpD,UAAM;AAAA,EACR;AACF;AAYA,SAAS,aAAa,KAAK,SAAS,aAAa;AAI/C,MAAI,QAAQ,eAAe,QAAQ,YAAY,SAAS,GAAG;AACzD,UAAM,eAAe,QAAQ,YAAY,IAAI,MAAM,GAAG,EAAE,KAAK,IAAI;AACjE,WAAO,0BAA0B,YAAY;AAC7C,gBAAY,KAAK,GAAG,QAAQ,WAAW;AAAA,EACzC;AAGA,MAAI,QAAQ,aAAa,QAAQ,UAAU,SAAS,GAAG;AACrD,WAAO;AAEP,UAAM,qBAAqB,CAAC;AAE5B,eAAW,eAAe,QAAQ,WAAW;AAE3C,UAAI,aAAa,YACd,QAAQ,OAAO,GAAG,EAClB,QAAQ,OAAO,GAAG;AAGrB,mBAAa,WAAW,QAAQ,SAAS,GAAG;AAE5C,yBAAmB,KAAK,oBAAoB;AAC5C,kBAAY,KAAK,UAAU;AAAA,IAC7B;AAEA,WAAO,mBAAmB,KAAK,MAAM;AACrC,WAAO;AAAA,EACT;AAGA,MAAI,QAAQ,WAAW;AACrB,QAAI,QAAQ,UAAU,OAAO;AAC3B,aAAO;AACP,kBAAY,KAAK,QAAQ,UAAU,MAAM,YAAY,CAAC;AAAA,IACxD;AAEA,QAAI,QAAQ,UAAU,KAAK;AACzB,aAAO;AACP,kBAAY,KAAK,QAAQ,UAAU,IAAI,YAAY,CAAC;AAAA,IACtD;AAAA,EACF;AAEA,SAAO;AACT;AAQA,SAAS,mBAAmB,SAAS;AAEnC,QAAM,OACJ,WAAW,QAAQ,QAAQ,MAAM,QAAQ,QAAQ,IAAI,IACjD,QAAQ,OACR,MAAM,QAAQ,OAAO,IACrB,UACA,CAAC;AAGP,MAAI,KAAK,WAAW,GAAG;AACrB,YAAQ,KAAK,sCAAsC;AACnD,WAAO,CAAC;AAAA,EACV;AAEA,SAAO,KAAK,IAAI,CAAC,SAAS;AAAA,IACxB,QAAQ;AAAA,MACN,WAAW,IAAI;AAAA,MACf,WAAW,IAAI;AAAA,MACf,aAAa,IAAI;AAAA,MACjB,MAAM,IAAI;AAAA,MACV,kBAAkB,IAAI;AAAA,MACtB,cAAc,IAAI;AAAA,MAClB,aAAa,IAAI;AAAA,MACjB,YAAY,IAAI;AAAA,MAChB,UAAU,IAAI;AAAA,MACd,UAAU,IAAI;AAAA,MACd,YAAY,IAAI;AAAA,MAChB,kBAAkB,IAAI;AAAA,IACxB;AAAA,IACA,gBAAgB,IAAI;AAAA,EACtB,EAAE;AACJ;AASA,SAAS,mBAAmB,UAAU,UAAU;AAE9C,QAAM,YAAY,oBAAI,IAAI;AAG1B,aAAW,UAAU,UAAU;AAC7B,cAAU,IAAI,OAAO,OAAO,WAAW,MAAM;AAAA,EAC/C;AAIA,aAAW,UAAU,UAAU;AAC7B,UAAM,WAAW,OAAO,OAAO;AAE/B,QAAI,UAAU,IAAI,QAAQ,GAAG;AAG3B,YAAM,iBAAiB,UAAU,IAAI,QAAQ;AAC7C,YAAM,gBACJ,eAAe,iBAAiB,MAAM,OAAO,iBAAiB;AAEhE,gBAAU,IAAI,UAAU;AAAA,QACtB,GAAG;AAAA,QACH,gBAAgB;AAAA,MAClB,CAAC;AAAA,IACH,OAAO;AAEL,gBAAU,IAAI,UAAU,MAAM;AAAA,IAChC;AAAA,EACF;AAGA,SAAO,MAAM,KAAK,UAAU,OAAO,CAAC,EAAE;AAAA,IACpC,CAAC,GAAG,MAAM,EAAE,iBAAiB,EAAE;AAAA,EACjC;AACF;;;ACtfA;;;ACAA;AACA;AACA;;;ADyCA,IAAM,kBAAkB,oBAAI,IAAI;AAMhC,IAAI,cAAc;AAMlB,IAAM,iBAAiB,CAAC;AAOjB,SAAS,iBAAiB;AAC/B,SAAO;AACT;AAOO,SAAS,eAAe,OAAO;AACpC,gBAAc;AAGd,MAAI,SAAS,MAAM,QAAQ,MAAM,kBAAkB,GAAG;AACpD,wBAAoB,MAAM,oBAAoB,CAAC,CAAC;AAAA,EAClD;AACF;AAQO,SAAS,oBAAoB,eAAe,CAAC,GAAG,kBAAkB,CAAC,GAAG;AAC3E,QAAM,eAAe;AAAA,IACnB,WAAW,KAAK,IAAI;AAAA,EACtB;AAGA,MAAI,aAAa,SAAS,GAAG;AAC3B,iBAAa,QAAQ,CAAC,OAAO,gBAAgB,IAAI,EAAE,CAAC;AACpD,iBAAa,QAAQ,CAAC,GAAG,YAAY;AAAA,EACvC;AAGA,MAAI,gBAAgB,SAAS,GAAG;AAC9B,oBAAgB,QAAQ,CAAC,OAAO,gBAAgB,OAAO,EAAE,CAAC;AAC1D,iBAAa,UAAU,CAAC,GAAG,eAAe;AAAA,EAC5C;AAGA,MAAI,aAAa,SAAS,KAAK,gBAAgB,SAAS,GAAG;AACzD,mBAAe,KAAK,YAAY;AAGhC,QAAI,eAAe,SAAS,IAAI;AAC9B,qBAAe,MAAM;AAAA,IACvB;AAAA,EACF;AACF;AAOO,SAAS,4BAA4B;AAC1C,SAAO,CAAC,GAAG,eAAe;AAC5B;AAKO,SAAS,qBAAqB;AACnC,kBAAgB,MAAM;AACtB,gBAAc;AAGd,iBAAe,KAAK;AAAA,IAClB,WAAW,KAAK,IAAI;AAAA,IACpB,OAAO;AAAA,EACT,CAAC;AACH;AAiBA,eAAsB,6BAA6B;AAEjD,QAAM,YAAY,0BAA0B;AAG5C,MAAI,UAAU,WAAW,GAAG;AAC1B,WAAO,CAAC;AAAA,EACV;AAEA,MAAI;AAEF,UAAM,eAAe,UAAU,IAAI,MAAM,GAAG,EAAE,KAAK,GAAG;AAGtD,UAAM,QAAQ,4CAA4C,YAAY;AACtE,UAAM,WAAW,MAAM,aAAa,OAAO,SAAS;AAEpD,WAAO;AAAA,EACT,SAAS,OAAO;AACd,YAAQ,MAAM,6CAA6C,KAAK;AAEhE,WAAO,CAAC;AAAA,EACV;AACF;AA8GA,eAAsB,wBAAwB;AAC5C,MAAI;AAEF,UAAM,WAAW,MAAM,2BAA2B;AAGlD,UAAM,QAAQ,eAAe;AAG7B,UAAM,gBAAgB,eAAe,MAAM,GAAG;AAG9C,WAAO;AAAA,MACL,iBAAiB,CAAC,GAAG,eAAe;AAAA,MACpC,aAAa;AAAA,MACb;AAAA,MACA,eAAe;AAAA,MACf,WAAW,KAAK,IAAI;AAAA,IACtB;AAAA,EACF,SAAS,OAAO;AACd,YAAQ,MAAM,uCAAuC,KAAK;AAE1D,WAAO;AAAA,MACL,iBAAiB,CAAC,GAAG,eAAe;AAAA,MACpC;AAAA,MACA,UAAU,CAAC;AAAA,MACX,eAAe,CAAC;AAAA,MAChB,WAAW,KAAK,IAAI;AAAA,MACpB,OAAO,MAAM;AAAA,IACf;AAAA,EACF;AACF;;;AE7TA;AACA,SAAS,MAAMC,eAAc;AAuI7B,eAAsB,uBAAuB,gBAAgB,CAAC,GAAG;AAC/D,MAAI;AACF,UAAM,EAAE,MAAM,eAAe,OAAO,SAAS,IAAI;AAGjD,QAAI,QAAQ;AACZ,UAAM,SAAS,CAAC;AAGhB,QAAI,MAAM;AACR,eAAS;AACT,aAAO,KAAK,IAAI;AAAA,IAClB;AAEA,QAAI,kBAAkB,UAAa,CAAC,MAAM,aAAa,GAAG;AACxD,eAAS;AACT,aAAO,KAAK,aAAa;AAAA,IAC3B;AAGA,QAAI,UAAU;AACZ,eAAS;AACT,aAAO,KAAK,UAAU,KAAK;AAAA,IAC7B;AAGA,aAAS;AAGT,QAAI,UAAU,UAAa,CAAC,MAAM,KAAK,KAAK,QAAQ,GAAG;AACrD,eAAS;AACT,aAAO,KAAK,KAAK;AAAA,IACnB;AAGA,UAAM,WAAW,MAAM,aAAa,OAAO,MAAM;AAGjD,UAAM,OACJ,YAAY,SAAS,QAAQ,MAAM,QAAQ,SAAS,IAAI,IACpD,SAAS,OACT,MAAM,QAAQ,QAAQ,IACtB,WACA,CAAC;AAGP,QAAI,KAAK,WAAW,GAAG;AACrB,cAAQ,KAAK,gCAAgC;AAC7C,aAAO,CAAC;AAAA,IACV;AAGA,WAAO,KAAK,IAAI,CAAC,aAAa;AAAA,MAC5B,GAAG;AAAA,MACH,iBAAiB,KAAK,MAAM,QAAQ,mBAAmB,IAAI;AAAA,MAC3D,WAAW,QAAQ,QAAQ,SAAS;AAAA;AAAA,IACtC,EAAE;AAAA,EACJ,SAAS,OAAO;AACd,YAAQ,MAAM,qCAAqC,KAAK;AACxD,UAAM,IAAI,MAAM,uCAAuC,MAAM,OAAO,EAAE;AAAA,EACxE;AACF;AASA,eAAsB,uBAAuB,WAAW,eAAe;AACrE,MAAI;AAEF,QAAI,QAAQ;AACZ,UAAM,SAAS,CAAC;AAGhB,QAAI,kBAAkB,UAAa,CAAC,MAAM,aAAa,GAAG;AACxD,eAAS;AACT,aAAO,KAAK,aAAa;AAAA,IAC3B;AAGA,UAAM,cAAa,oBAAI,KAAK,GAAE,YAAY;AAC1C,aAAS;AACT,WAAO,KAAK,UAAU;AAGtB,aAAS;AACT,WAAO,KAAK,SAAS;AAGrB,UAAM,SAAS,MAAM,aAAa,OAAO,MAAM;AAG/C,UAAM,UAAU,OAAO,eAAe;AAEtC,QAAI,SAAS;AACX,cAAQ;AAAA,QACN,WAAW,SAAS;AAAA,MACtB;AAEA,UAAI,kBAAkB,QAAW;AAC/B,gBAAQ,IAAI,+BAA+B,aAAa,EAAE;AAAA,MAC5D;AAAA,IACF,OAAO;AACL,cAAQ,KAAK,sBAAsB,SAAS,mBAAmB;AAAA,IACjE;AAEA,WAAO;AAAA,EACT,SAAS,OAAO;AACd,YAAQ,MAAM,2BAA2B,SAAS,eAAe,KAAK;AACtE,UAAM,IAAI,MAAM,8BAA8B,MAAM,OAAO,EAAE;AAAA,EAC/D;AACF;AAUA,eAAsB,iBACpB,WACA,iBACA,cAAc,CAAC,GACf;AACA,MAAI;AAEF,UAAM,iBAAiBC,QAAO;AAC9B,UAAM,aAAY,oBAAI,KAAK,GAAE,YAAY;AAGzC,UAAM,mBAAmB,KAAK,UAAU,eAAe,CAAC,CAAC;AAGzD,UAAM,wBAAwB;AAAA,MAC5B,OAAO;AAAA;AAAA,MACP,cAAc;AAAA;AAAA,MACd,WAAW;AAAA;AAAA,IACb;AAEA,UAAM,qBAAqB;AAAA,MACzB,OAAO;AAAA;AAAA,MACP,cAAc;AAAA;AAAA,MACd,WAAW;AAAA;AAAA,IACb;AAGA,UAAM,aAAa,mBAAmB;AAEtC,QAAI;AAEF,YAAM,yBAAyB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAU/B,YAAM,aAAa,wBAAwB;AAAA,QACzC;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MACF,CAAC;AAGD,YAAM,kBACJ;AACF,YAAM,gBAAgB,MAAM,aAAa,iBAAiB,CAAC,SAAS,CAAC;AAErE,UAAI,cAAc,WAAW,GAAG;AAC9B,cAAM,IAAI,MAAM,mBAAmB,SAAS,YAAY;AAAA,MAC1D;AAEA,YAAM,UAAU,cAAc,CAAC;AAG/B,UAAI,qBACF,QAAQ,oBACP,sBAAsB,eAAe,KAAK;AAC7C,UAAI,kBACF,QAAQ,iBAAiB,mBAAmB,eAAe,KAAK;AAGlE,2BAAqB,KAAK,IAAI,GAAG,KAAK,IAAI,GAAG,kBAAkB,CAAC;AAChE,wBAAkB,KAAK,IAAI,GAAG,KAAK,IAAI,GAAG,eAAe,CAAC;AAG1D,YAAM,qBAAqB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAS3B,YAAM,qBACJ,oBAAoB,UAAU,2BAA2B;AAC3D,YAAM,sBAAsB;AAAA,QAC1B;AAAA,QACA;AAAA,QACA;AAAA,MACF;AAGA,UAAI,oBAAoB,SAAS;AAC/B,4BAAoB,KAAK,SAAS;AAAA,MACpC;AAGA,YAAM,mBACJ,qBAAqB,qBAAqB;AAC5C,0BAAoB,KAAK,SAAS;AAElC,YAAM,aAAa,kBAAkB,mBAAmB;AAGxD,YAAM,aAAa,QAAQ;AAE3B,cAAQ;AAAA,QACN,WAAW,SAAS,qBAAqB,eAAe;AAAA,MAC1D;AAAA,IACF,SAAS,OAAO;AAEd,YAAM,aAAa,UAAU;AAC7B,YAAM;AAAA,IACR;AAAA,EACF,SAAS,OAAO;AACd,YAAQ,MAAM,6BAA6B,SAAS,KAAK,KAAK;AAC9D,UAAM,IAAI,MAAM,gCAAgC,MAAM,OAAO,EAAE;AAAA,EACjE;AACF;;;ACzXA;AADA,SAAS,SAAS;AAQX,IAAM,2CAA2C;AAAA;AAAA,EAEtD,cAAc,EAAE,OAAO,EAAE,SAAS;AAAA,EAClC,WAAW,EACR,OAAO;AAAA,IACN,MAAM,EAAE,OAAO;AAAA,IACf,YAAY,EAAE,OAAO;AAAA,EACvB,CAAC,EACA,SAAS;AAAA,EACZ,qBAAqB,EAAE,QAAQ,EAAE,SAAS,EAAE,QAAQ,IAAI;AAAA,EACxD,4BAA4B,EAAE,QAAQ,EAAE,SAAS,EAAE,QAAQ,IAAI;AAAA,EAC/D,qBAAqB,EAAE,OAAO,EAAE,SAAS,EAAE,QAAQ,CAAC;AAAA,EACpD,kBAAkB,EAAE,OAAO,EAAE,SAAS,EAAE,QAAQ,CAAC;AAAA,EACjD,cAAc,EACX,KAAK,CAAC,WAAW,YAAY,eAAe,CAAC,EAC7C,SAAS,EACT,QAAQ,UAAU;AAAA,EACrB,aAAa,EAAE,OAAO,EAAE,SAAS,EAAE,QAAQ,oBAAoB;AACjE;AAMO,IAAM,4CAA4C;AAAA,EACvD,gBAAgB,EAAE,OAAO;AAAA,EACzB,uBAAuB,EAAE,OAAO;AAAA,EAChC,iBAAiB,EAAE,OAAO,EAAE,SAAS;AAAA,EACrC,sBAAsB,EACnB,OAAO;AAAA,IACN,aAAa,EAAE,MAAM,EAAE,IAAI,CAAC,EAAE,SAAS;AAAA,IACvC,qBAAqB,EAClB,OAAO;AAAA,MACN,SAAS,EAAE,OAAO;AAAA,MAClB,SAAS,EAAE;AAAA,QACT,EAAE,OAAO;AAAA,UACP,MAAM,EAAE,OAAO;AAAA,UACf,MAAM,EAAE,OAAO;AAAA,QACjB,CAAC;AAAA,MACH;AAAA,IACF,CAAC,EACA,SAAS;AAAA,IACZ,qBAAqB,EAClB;AAAA,MACC,EAAE,OAAO;AAAA,QACP,WAAW,EAAE,OAAO;AAAA,QACpB,SAAS,EAAE,OAAO;AAAA,QAClB,SAAS,EAAE,OAAO;AAAA,MACpB,CAAC;AAAA,IACH,EACC,SAAS;AAAA,IACZ,iBAAiB,EACd;AAAA,MACC,EAAE,OAAO;AAAA,QACP,MAAM,EAAE,OAAO;AAAA,QACf,aAAa,EAAE,OAAO;AAAA,QACtB,WAAW,EAAE,OAAO;AAAA,MACtB,CAAC;AAAA,IACH,EACC,SAAS;AAAA,IACZ,kBAAkB,EAAE,IAAI,EAAE,SAAS;AAAA,IACnC,eAAe,EACZ;AAAA,MACC,EAAE,OAAO;AAAA,QACP,WAAW,EAAE,OAAO;AAAA,QACpB,OAAO,EAAE,MAAM,EAAE,OAAO,CAAC;AAAA,QACzB,SAAS,EAAE,OAAO;AAAA,MACpB,CAAC;AAAA,IACH,EACC,SAAS;AAAA,IACZ,gBAAgB,EACb;AAAA,MACC,EAAE,OAAO;AAAA,QACP,MAAM,EAAE,OAAO;AAAA,QACf,MAAM,EAAE,OAAO;AAAA,QACf,aAAa,EAAE,OAAO;AAAA,QACtB,YAAY,EAAE,OAAO;AAAA,MACvB,CAAC;AAAA,IACH,EACC,SAAS;AAAA,EACd,CAAC,EACA,SAAS;AACd;AAOO,IAAM,uCAAuC;AAAA;AAAA,EAElD,gBAAgB,EAAE,OAAO;AAAA,EACzB,aAAa,EACV;AAAA,IACC,EAAE,OAAO;AAAA,MACP,MAAM,EAAE,KAAK,CAAC,QAAQ,aAAa,QAAQ,CAAC;AAAA,MAC5C,SAAS,EAAE,OAAO;AAAA,IACpB,CAAC;AAAA,EACH,EACC,SAAS,EACT,QAAQ,CAAC,CAAC;AAAA,EACb,aAAa,EACV;AAAA,IACC,EAAE,OAAO;AAAA,MACP,UAAU,EAAE,OAAO;AAAA,MACnB,YAAY,EAAE,OAAO;AAAA,MACrB,cAAc,EAAE,OAAO,EAAE,SAAS;AAAA,IACpC,CAAC;AAAA,EACH,EACC,SAAS,EACT,QAAQ,CAAC,CAAC;AAAA,EACb,6BAA6B,EAAE,QAAQ,EAAE,SAAS,EAAE,QAAQ,IAAI;AAAA,EAChE,yBAAyB,EACtB,KAAK,CAAC,WAAW,YAAY,YAAY,CAAC,EAC1C,SAAS,EACT,QAAQ,UAAU;AAAA,EACrB,wBAAwB,EAAE,QAAQ,EAAE,SAAS,EAAE,QAAQ,IAAI;AAAA,EAC3D,aAAa,EAAE,OAAO,EAAE,SAAS,EAAE,QAAQ,oBAAoB;AACjE;AAMO,IAAM,wCAAwC;AAAA,EACnD,QAAQ,EAAE,KAAK,CAAC,WAAW,WAAW,SAAS,CAAC;AAAA,EAChD,cAAc,EACX,OAAO;AAAA,IACN,MAAM,EAAE,OAAO;AAAA,IACf,YAAY,EAAE,OAAO;AAAA,EACvB,CAAC,EACA,SAAS;AAAA,EACZ,mBAAmB,EAAE,OAAO;AAAA,IAC1B,WAAW,EAAE,QAAQ;AAAA,IACrB,YAAY,EAAE,QAAQ;AAAA,IACtB,kBAAkB,EAAE,QAAQ;AAAA,EAC9B,CAAC;AAAA,EACD,kBAAkB,EACf,OAAO;AAAA,IACN,SAAS,EAAE,OAAO;AAAA,IAClB,eAAe,EAAE,MAAM,EAAE,OAAO,CAAC,EAAE,SAAS;AAAA,EAC9C,CAAC,EACA,SAAS;AAAA,EACZ,kBAAkB,EACf,OAAO;AAAA,IACN,MAAM,EAAE,OAAO,EAAE,SAAS;AAAA,IAC1B,IAAI,EAAE,OAAO,EAAE,SAAS;AAAA,IACxB,YAAY,EAAE,OAAO;AAAA,EACvB,CAAC,EACA,SAAS;AACd;AAOO,IAAM,qCAAqC;AAAA;AAAA,EAEhD,gBAAgB,EAAE,OAAO;AAAA,EACzB,OAAO,EAAE,OAAO;AAAA,EAChB,aAAa,EAAE,OAAO,EAAE,SAAS,EAAE,QAAQ,oBAAoB;AAAA,EAC/D,aAAa,EACV,OAAO;AAAA,IACN,aAAa,EAAE,MAAM,EAAE,OAAO,CAAC,EAAE,SAAS;AAAA,IAC1C,WAAW,EAAE,MAAM,EAAE,OAAO,CAAC,EAAE,SAAS;AAAA,IACxC,qBAAqB,EAAE,QAAQ,EAAE,SAAS,EAAE,QAAQ,IAAI;AAAA,IACxD,kBAAkB,EAAE,QAAQ,EAAE,SAAS,EAAE,QAAQ,KAAK;AAAA,IACtD,eAAe,EACZ,OAAO,EAAE,MAAM,EAAE,OAAO,GAAG,YAAY,EAAE,OAAO,EAAE,CAAC,EACnD,SAAS;AAAA,EACd,CAAC,EACA,SAAS,EACT,QAAQ,CAAC,CAAC;AAAA,EACb,gBAAgB,EACb,OAAO;AAAA,IACN,mBAAmB,EAAE,OAAO,EAAE,SAAS,EAAE,QAAQ,GAAG;AAAA,IACpD,cAAc,EAAE,MAAM,EAAE,OAAO,CAAC,EAAE,SAAS;AAAA,IAC3C,oBAAoB,EAAE,MAAM,EAAE,OAAO,CAAC,EAAE,SAAS;AAAA,IACjD,WAAW,EACR,OAAO;AAAA,MACN,MAAM,EAAE,OAAO,EAAE,SAAS;AAAA,MAC1B,IAAI,EAAE,OAAO,EAAE,SAAS;AAAA,IAC1B,CAAC,EACA,SAAS;AAAA,EACd,CAAC,EACA,SAAS,EACT,QAAQ,CAAC,CAAC;AAAA,EACb,mBAAmB,EAChB,KAAK,CAAC,aAAa,WAAW,aAAa,UAAU,CAAC,EACtD,SAAS,EACT,QAAQ,UAAU;AAAA,EACrB,iBAAiB,EACd,KAAK,CAAC,gBAAgB,wBAAwB,gBAAgB,CAAC,EAC/D,SAAS,EACT,QAAQ,cAAc;AAAA,EACzB,gBAAgB,EACb,MAAM;AAAA,IACL,EAAE,KAAK,CAAC,QAAQ,cAAc,YAAY,uBAAuB,CAAC;AAAA,IAClE,EAAE,OAAO;AAAA,MACP,MAAM,EAAE,OAAO,EAAE,SAAS;AAAA,MAC1B,cAAc,EAAE,OAAO,EAAE,SAAS;AAAA,MAClC,eAAe,EAAE,OAAO,EAAE,SAAS;AAAA,MACnC,UAAU,EAAE,OAAO,EAAE,SAAS;AAAA,IAChC,CAAC;AAAA,EACH,CAAC,EACA,SAAS,EACT,QAAQ,MAAM;AAAA,EACjB,uBAAuB,EACpB,OAAO;AAAA,IACN,iBAAiB,EAAE,QAAQ,EAAE,SAAS,EAAE,QAAQ,IAAI;AAAA,IACpD,sBAAsB,EAAE,QAAQ,EAAE,SAAS,EAAE,QAAQ,IAAI;AAAA,IACzD,qBAAqB,EAAE,QAAQ,EAAE,SAAS,EAAE,QAAQ,KAAK;AAAA,IACzD,oBAAoB,EAAE,QAAQ,EAAE,SAAS,EAAE,QAAQ,KAAK;AAAA,EAC1D,CAAC,EACA,SAAS,EACT,QAAQ,CAAC,CAAC;AACf;AAOO,IAAM,sCAAsC;AAAA,EACjD,iBAAiB,EAAE;AAAA,IACjB,EAAE,OAAO;AAAA,MACP,MAAM,EAAE,OAAO;AAAA;AAAA,MACf,SAAS,EAAE,OAAO;AAAA,MAClB,WAAW,EAAE,OAAO;AAAA,MACpB,gBAAgB,EAAE,OAAO;AAAA,MACzB,iBAAiB,EAAE,OAAO;AAAA,MAC1B,UAAU,EAAE,IAAI;AAAA;AAAA,MAChB,mBAAmB,EAAE,OAAO;AAAA,MAC5B,sBAAsB,EAAE,OAAO;AAAA,IACjC,CAAC;AAAA,EACH;AAAA,EACA,kBAAkB,EAAE,OAAO;AAAA,EAC3B,gBAAgB,EACb,OAAO;AAAA,IACN,YAAY,EAAE,OAAO;AAAA,IACrB,UAAU,EAAE,OAAO;AAAA,IACnB,mBAAmB,EAAE,OAAO;AAAA,IAC5B,kBAAkB,EAAE,OAAO;AAAA,MACzB,MAAM,EAAE,OAAO;AAAA,MACf,cAAc,EAAE,OAAO;AAAA,MACvB,eAAe,EAAE,OAAO;AAAA,MACxB,SAAS,EAAE,OAAO;AAAA,IACpB,CAAC;AAAA,EACH,CAAC,EACA,SAAS;AACd;AAMO,IAAM,oCAAoC;AAAA,EAC/C,gBAAgB,EAAE,OAAO;AAAA,EACzB,MAAM,EAAE,OAAO;AAAA,EACf,aAAa,EAAE,OAAO,EAAE,SAAS;AAAA,EACjC,YAAY,EAAE,IAAI,EAAE,SAAS;AAAA,EAC7B,mBAAmB,EAChB,KAAK;AAAA,IACJ;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF,CAAC,EACA,SAAS,EACT,QAAQ,eAAe;AAAA,EAC1B,cAAc,EAAE,QAAQ,EAAE,SAAS,EAAE,QAAQ,IAAI;AACnD;AAMO,IAAM,qCAAqC;AAAA,EAChD,aAAa,EAAE,OAAO;AAAA,EACtB,QAAQ,EAAE,OAAO;AAAA,EACjB,mBAAmB,EAAE,OAAO;AAAA,EAC5B,sBAAsB,EAAE,OAAO;AAAA,EAC/B,kBAAkB,EACf,OAAO;AAAA,IACN,aAAa,EAAE,OAAO;AAAA,IACtB,aAAa,EAAE,OAAO;AAAA,IACtB,eAAe,EAAE,OAAO;AAAA,IACxB,cAAc,EACX,OAAO;AAAA,MACN,0BAA0B,EAAE,OAAO;AAAA,MACnC,6BAA6B,EAAE,OAAO;AAAA,MACtC,oBAAoB,EAAE,OAAO;AAAA,MAC7B,oBAAoB,EAAE,OAAO;AAAA,IAC/B,CAAC,EACA,SAAS;AAAA,IACZ,eAAe,EAAE,OAAO,EAAE,SAAS;AAAA,IACnC,eAAe,EACZ;AAAA,MACC,EAAE,OAAO;AAAA,QACP,UAAU,EAAE,OAAO;AAAA,QACnB,MAAM,EAAE,OAAO;AAAA,QACf,iBAAiB,EAAE,OAAO;AAAA,MAC5B,CAAC;AAAA,IACH,EACC,SAAS;AAAA,IACZ,wBAAwB,EACrB;AAAA,MACC,EAAE,OAAO;AAAA,QACP,MAAM,EAAE,OAAO;AAAA,QACf,OAAO,EAAE,OAAO;AAAA,MAClB,CAAC;AAAA,IACH,EACC,SAAS;AAAA,IACZ,OAAO,EAAE,OAAO,EAAE,SAAS;AAAA,EAC7B,CAAC,EACA,SAAS;AACd;AAOO,IAAM,yCAAyC;AAAA,EACpD,gBAAgB,EAAE,OAAO;AAAA,EACzB,oBAAoB,EAAE,QAAQ,EAAE,SAAS,EAAE,QAAQ,KAAK;AAAA,EACxD,kBAAkB,EAAE,QAAQ,EAAE,SAAS,EAAE,QAAQ,IAAI;AAAA,EACrD,iBAAiB,EAAE,QAAQ,EAAE,SAAS,EAAE,QAAQ,IAAI;AAAA,EACpD,yBAAyB,EAAE,QAAQ,EAAE,SAAS,EAAE,QAAQ,IAAI;AAAA,EAC5D,mBAAmB,EAAE,QAAQ,EAAE,SAAS,EAAE,QAAQ,IAAI;AAAA,EACtD,SAAS,EACN,KAAK,CAAC,aAAa,aAAa,UAAU,gBAAgB,CAAC,EAC3D,SAAS,EACT,QAAQ,WAAW;AACxB;AAOO,IAAM,0CAA0C;AAAA,EACrD,QAAQ,EAAE,OAAO;AAAA,EACjB,SAAS,EAAE,OAAO;AAAA,EAClB,SAAS,EAAE,OAAO;AAAA;AAAA,EAGlB,oBAAoB,EACjB,OAAO;AAAA,IACN,WAAW,EAAE;AAAA,MACX,EAAE,OAAO;AAAA,QACP,MAAM,EAAE,OAAO;AAAA,QACf,SAAS,EAAE,OAAO;AAAA,QAClB,YAAY,EAAE,OAAO;AAAA;AAAA,QAErB,WAAW,EAAE,OAAO,EAAE,SAAS;AAAA,QAC/B,SAAS,EAAE,MAAM,EAAE,IAAI,CAAC,EAAE,SAAS;AAAA,QACnC,kBAAkB,EAAE,OAAO,EAAE,SAAS;AAAA,QACtC,eAAe,EAAE,MAAM,EAAE,IAAI,CAAC,EAAE,SAAS;AAAA,QACzC,cAAc,EAAE,MAAM,EAAE,OAAO,CAAC,EAAE,SAAS;AAAA,QAC3C,WAAW,EAAE,OAAO,EAAE,SAAS;AAAA,QAC/B,gBAAgB,EAAE,MAAM,EAAE,IAAI,CAAC,EAAE,SAAS;AAAA,QAC1C,eAAe,EAAE,OAAO,EAAE,SAAS;AAAA,MACrC,CAAC;AAAA,IACH;AAAA,IACA,OAAO,EAAE,OAAO;AAAA,IAChB,QAAQ,EAAE,OAAO,EAAE,OAAO,GAAG,EAAE,OAAO,CAAC;AAAA,IACvC,mBAAmB,EAAE,OAAO;AAAA,IAC5B,OAAO,EAAE,OAAO,EAAE,SAAS;AAAA,EAC7B,CAAC,EACA,SAAS;AAAA;AAAA,EAGZ,kBAAkB,EACf,OAAO;AAAA,IACN,UAAU,EAAE,OAAO;AAAA,IACnB,UAAU,EAAE;AAAA,MACV,EAAE,OAAO;AAAA,QACP,WAAW,EAAE,OAAO;AAAA,QACpB,MAAM,EAAE,OAAO;AAAA,QACf,MAAM,EAAE,OAAO;AAAA,QACf,UAAU,EAAE,QAAQ;AAAA,QACpB,YAAY,EAAE,OAAO;AAAA,MACvB,CAAC;AAAA,IACH;AAAA,IACA,OAAO,EAAE,OAAO,EAAE,SAAS;AAAA,EAC7B,CAAC,EACA,SAAS;AAAA;AAAA,EAGZ,sBAAsB,EACnB,OAAO;AAAA,IACN,cAAc,EAAE,OAAO;AAAA,IACvB,eAAe,EAAE;AAAA,MACf,EAAE,OAAO;AAAA,QACP,gBAAgB,EAAE,OAAO;AAAA,QACzB,SAAS,EAAE,OAAO;AAAA,QAClB,WAAW,EAAE,OAAO;AAAA,QACpB,iBAAiB,EAAE,OAAO;AAAA,QAC1B,cAAc,EAAE,MAAM,EAAE,OAAO,CAAC;AAAA,MAClC,CAAC;AAAA,IACH;AAAA,IACA,qBAAqB,EAAE;AAAA,MACrB,EAAE,OAAO;AAAA,QACP,OAAO,EAAE,OAAO;AAAA,QAChB,SAAS,EAAE,OAAO;AAAA,QAClB,mBAAmB,EAAE,OAAO;AAAA,QAC5B,iBAAiB,EAAE;AAAA,UACjB,EAAE,OAAO;AAAA,YACP,gBAAgB,EAAE,OAAO;AAAA,YACzB,SAAS,EAAE,OAAO;AAAA,UACpB,CAAC;AAAA,QACH;AAAA,MACF,CAAC;AAAA,IACH;AAAA,IACA,OAAO,EAAE,OAAO,EAAE,SAAS;AAAA,EAC7B,CAAC,EACA,SAAS;AAAA;AAAA,EAGZ,WAAW,EACR,OAAO;AAAA,IACN,oBAAoB,EAAE;AAAA,MACpB,EAAE,OAAO;AAAA,QACP,QAAQ,EAAE,OAAO;AAAA,QACjB,UAAU,EAAE,KAAK,CAAC,QAAQ,UAAU,KAAK,CAAC;AAAA,QAC1C,WAAW,EAAE,OAAO;AAAA,MACtB,CAAC;AAAA,IACH;AAAA,IACA,gBAAgB,EAAE;AAAA,MAChB,EAAE,OAAO;AAAA,QACP,OAAO,EAAE,OAAO;AAAA,QAChB,UAAU,EAAE,KAAK,CAAC,QAAQ,UAAU,KAAK,CAAC;AAAA,QAC1C,WAAW,EAAE,OAAO;AAAA,MACtB,CAAC;AAAA,IACH;AAAA,IACA,oBAAoB,EAAE;AAAA,MACpB,EAAE,OAAO;AAAA,QACP,OAAO,EAAE,OAAO;AAAA,QAChB,MAAM,EAAE,OAAO;AAAA,QACf,MAAM,EAAE,OAAO;AAAA,QACf,WAAW,EAAE,OAAO;AAAA,MACtB,CAAC;AAAA,IACH;AAAA,IACA,OAAO,EAAE,OAAO,EAAE,SAAS;AAAA,EAC7B,CAAC,EACA,SAAS;AACd;;;Ad3bA;AASA,eAAe,QAAQ,OAAO,YAAY;AACxC,MAAI;AACF,eAAW,QAAQ,gDAAgD;AAAA,MACjE,cAAc,MAAM;AAAA,IACtB,CAAC;AAGD,UAAM,iBAAiB,MAAM,kBAAkBC,QAAO;AACtD,eAAW,SAAS,0BAA0B,cAAc,EAAE;AAG9D,UAAM;AAAA,MACJ,eAAe;AAAA,MACf;AAAA,MACA,sBAAsB;AAAA,MACtB,6BAA6B;AAAA,MAC7B,sBAAsB;AAAA,MACtB,mBAAmB;AAAA,MACnB,eAAe;AAAA,MACf,cAAc;AAAA,IAChB,IAAI;AAGJ,QAAI;AACF,YAA2B,mBAAmB;AAC9C,UAAI,WAAW;AACb,cAA2B;AAAA,UACzB,UAAU;AAAA,UACV,UAAU;AAAA,QACZ;AACA,mBAAW,QAAQ,qBAAqB;AAAA,UACtC,MAAM,UAAU;AAAA,UAChB,YAAY,UAAU;AAAA,QACxB,CAAC;AAAA,MACH;AAAA,IACF,SAAS,KAAK;AACZ;AAAA,QACE;AAAA,QACA;AAAA,QACA;AAAA,UACE,OAAO,IAAI;AAAA,UACX;AAAA,QACF;AAAA,MACF;AAAA,IAEF;AAGA,QAAI;AACF,YAA2B;AAAA,QACzB;AAAA,QACA;AAAA,UACE;AAAA,UACA;AAAA,UACA;AAAA,QACF;AAAA,QACA,CAAC;AAAA;AAAA,QACD;AAAA,MACF;AACA,iBAAW,SAAS,2CAA2C;AAAA,QAC7D;AAAA,MACF,CAAC;AAAA,IACH,SAAS,KAAK;AAEZ,iBAAW,QAAQ,mDAAmD;AAAA,QACpE,OAAO,IAAI;AAAA,QACX;AAAA,MACF,CAAC;AAAA,IACH;AAGA,QAAI;AACF,YAA+B;AAAA,QAC7B;AAAA,QACA;AAAA,MACF;AACA,iBAAW,SAAS,iDAAiD;AAAA,QACnE;AAAA,MACF,CAAC;AAAA,IACH,SAAS,KAAK;AACZ,iBAAW,SAAS,kDAAkD;AAAA,QACpE,OAAO,IAAI;AAAA,QACX;AAAA,MACF,CAAC;AACD,YAAM,IAAI;AAAA,QACR,oDAAoD,IAAI,OAAO;AAAA,MACjE;AAAA,IACF;AAGA,QAAI,kBAAkB;AACtB,QAAI,cAAc;AAChB,UAAI;AACF,cAAM,eAAe,MAA2B;AAAA,UAC9C;AAAA,QACF;AACA,0BAAkB,aAAa;AAC/B,mBAAW,QAAQ,4BAA4B;AAAA,UAC7C,QAAQ;AAAA,UACR,YAAY,aAAa,cAAc;AAAA,QACzC,CAAC;AAAA,MACH,SAAS,KAAK;AAEZ;AAAA,UACE;AAAA,UACA;AAAA,UACA;AAAA,YACE,OAAO,IAAI;AAAA,YACX;AAAA,UACF;AAAA,QACF;AAAA,MACF;AAAA,IACF;AAGA,eAAW,QAAQ,4CAA4C;AAAA,MAC7D;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAED,UAAM,uBAAuB,MAAM;AAAA,MACjC;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,QACE;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MACF;AAAA,IACF;AAEA,UAAM,gBAAgB;AAAA,MACpB,kBAAkB,qBAAqB,aAAa,UAAU;AAAA,MAC9D,mBAAmB,qBAAqB,qBAAqB,UAAU;AAAA,MACvE,eAAe,qBAAqB,eAAe,UAAU;AAAA,MAC7D,UAAU,qBAAqB,gBAAgB,UAAU;AAAA,IAC3D;AAEA;AAAA,MACE;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAGA,UAAM,wBAAwB;AAAA,MAC5B;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAEA,eAAW,QAAQ,qCAAqC;AAAA,MACtD,eAAe,uBAAuB,UAAU;AAAA,IAClD,CAAC;AAGD,UAAM,eAAe;AAAA,MACnB,SAAS,6CAA6C,cAAc;AAAA,MACpE;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAEA,WAAO;AAAA,MACL,SAAS;AAAA,QACP;AAAA,UACE,MAAM;AAAA,UACN,MAAM,KAAK,UAAU,YAAY;AAAA,QACnC;AAAA,MACF;AAAA,IACF;AAAA,EACF,SAAS,OAAO;AAEd,eAAW,SAAS,iDAAiD;AAAA,MACnE,OAAO,MAAM;AAAA,MACb,OAAO,MAAM;AAAA,MACb,OAAO;AAAA,QACL,cAAc,MAAM;AAAA,QACpB,WAAW,MAAM;AAAA,QACjB,cAAc,MAAM;AAAA,MACtB;AAAA,IACF,CAAC;AAGD,UAAM,gBAAgB;AAAA,MACpB,OAAO;AAAA,MACP,WAAW,MAAM,QAAQ;AAAA,MACzB,cAAc,MAAM;AAAA,IACtB;AAEA,WAAO;AAAA,MACL,SAAS;AAAA,QACP;AAAA,UACE,MAAM;AAAA,UACN,MAAM,KAAK,UAAU,aAAa;AAAA,QACpC;AAAA,MACF;AAAA,IACF;AAAA,EACF;AACF;AAWA,eAAe,2BACb,cACA,WACA,gBACA,SACA;AACA,QAAM,UAAU,CAAC;AAEjB,MAAI;AACF,eAAW,SAAS,mCAAmC;AAAA,MACrD,cAAc,cAAc,UAAU,GAAG,EAAE;AAAA,MAC3C;AAAA,IACF,CAAC;AAGD,YAAQ,cAAc,MAAM;AAAA,MAC1B;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAEA,eAAW,SAAS,yBAAyB;AAAA,MAC3C,WAAW,QAAQ,aAAa,UAAU;AAAA,IAC5C,CAAC;AAGD,QAAI,QAAQ,qBAAqB;AAC/B,UAAI;AACF,gBAAQ,sBAAsB,MAAM,0BAA0B,OAAO;AACrE,mBAAW,SAAS,iCAAiC;AAAA,UACnD,WAAW,QAAQ,qBAAqB,UAAU;AAAA,QACpD,CAAC;AAAA,MACH,SAAS,KAAK;AACZ,mBAAW,QAAQ,yCAAyC;AAAA,UAC1D,OAAO,IAAI;AAAA,QACb,CAAC;AACD,gBAAQ,sBAAsB;AAAA,MAChC;AAAA,IACF,OAAO;AACL,cAAQ,sBAAsB;AAAA,IAChC;AAGA,QAAI;AACF,cAAQ,mBAAmB,MAAM,uBAAuB;AACxD,iBAAW,SAAS,8BAA8B;AAAA,QAChD,gBAAgB,QAAQ,kBAAkB,aAAa,UAAU;AAAA,QACjE,WAAW,QAAQ,kBAAkB,OAAO,UAAU;AAAA,MACxD,CAAC;AAAA,IACH,SAAS,KAAK;AACZ,iBAAW,QAAQ,sCAAsC;AAAA,QACvD,OAAO,IAAI;AAAA,MACb,CAAC;AACD,cAAQ,mBAAmB,EAAE,aAAa,CAAC,GAAG,OAAO,CAAC,EAAE;AAAA,IAC1D;AAGA,QAAI,QAAQ,4BAA4B;AACtC,UAAI;AACF,gBAAQ,sBAAsB,MAAM,0BAA0B,OAAO;AACrE,mBAAW,SAAS,iCAAiC;AAAA,UACnD,OAAO,QAAQ,qBAAqB,UAAU;AAAA,QAChD,CAAC;AAAA,MACH,SAAS,KAAK;AACZ,mBAAW,QAAQ,yCAAyC;AAAA,UAC1D,OAAO,IAAI;AAAA,QACb,CAAC;AACD,gBAAQ,sBAAsB,CAAC;AAAA,MACjC;AAAA,IACF;AAGA,QAAI;AACF,cAAQ,gBAAgB,MAAM;AAAA,QAC5B,QAAQ;AAAA,MACV;AACA,iBAAW,SAAS,2BAA2B;AAAA,QAC7C,OAAO,QAAQ,eAAe,UAAU;AAAA,MAC1C,CAAC;AAAA,IACH,SAAS,KAAK;AACZ,iBAAW,QAAQ,mCAAmC;AAAA,QACpD,OAAO,IAAI;AAAA,MACb,CAAC;AACD,cAAQ,gBAAgB,CAAC;AAAA,IAC3B;AAGA,QAAI;AACF,cAAQ,kBAAkB,MAAM,sBAAsB;AACtD,iBAAW,SAAS,6BAA6B;AAAA,QAC/C,OAAO,QAAQ,iBAAiB,UAAU;AAAA,MAC5C,CAAC;AAAA,IACH,SAAS,KAAK;AACZ,iBAAW,QAAQ,qCAAqC;AAAA,QACtD,OAAO,IAAI;AAAA,MACb,CAAC;AACD,cAAQ,kBAAkB,CAAC;AAAA,IAC7B;AAGA,QAAI;AACF,cAAQ,iBAAiB,MAAM;AAAA,QAC7B;AAAA,QACA;AAAA,MACF;AACA,iBAAW,SAAS,4BAA4B;AAAA,QAC9C,OAAO,QAAQ,gBAAgB,UAAU;AAAA,MAC3C,CAAC;AAAA,IACH,SAAS,KAAK;AACZ,iBAAW,QAAQ,oCAAoC;AAAA,QACrD,OAAO,IAAI;AAAA,MACb,CAAC;AACD,cAAQ,iBAAiB,CAAC;AAAA,IAC5B;AAEA,WAAO;AAAA,EACT,SAAS,OAAO;AACd,eAAW,SAAS,yCAAyC;AAAA,MAC3D,OAAO,MAAM;AAAA,MACb;AAAA,IACF,CAAC;AACD,UAAM;AAAA,EACR;AACF;AAUA,eAAe,kBAAkB,OAAO,WAAW,SAAS;AAC1D,MAAI;AAEF,UAAM,oBAAoB;AAAA,MACxB,OAAO,QAAQ,sBAAsB;AAAA;AAAA,IACvC;AAGA,QAAI,WAAW;AACb,UAAI,UAAU,SAAS,UAAU,UAAU,SAAS,aAAa;AAC/D,0BAAkB,YAAY,CAAC,UAAU,UAAU;AAAA,MACrD;AAAA,IACF;AAGA,UAAM,cAAc,QAChB,MAAMC,iBAAgB,KAAK,IAC3B,CAAC,UAAU,QAAQ,SAAS,QAAQ;AAGxC,UAAM,gBAAgB,MAA8B;AAAA,MAClD;AAAA,MACA;AAAA,IACF;AAGA,QAAI,YAAY,cAAc,IAAI,CAAC,YAAY;AAAA,MAC7C,WAAW,OAAO,OAAO;AAAA,MACzB,MAAM,OAAO,OAAO;AAAA,MACpB,MAAM,OAAO,OAAO;AAAA,MACpB,MAAM,OAAO,OAAO;AAAA,MACpB,SAAS,OAAO,OAAO;AAAA,MACvB,gBAAgB,OAAO;AAAA,IACzB,EAAE;AAGF,gBAAY,UAAU,MAAM,GAAG,QAAQ,mBAAmB;AAG1D,UAAM,qBAAqB;AAAA,MACzB,aAAa,QAAQ;AAAA,MACrB,cAAc,KAAK,MAAM,QAAQ,cAAc,GAAG;AAAA;AAAA,IACpD;AAEA,UAAM,kBAAkB,MAA6B;AAAA,MACnD;AAAA,MACA;AAAA,IACF;AAEA,WAAO;AAAA,EACT,SAAS,OAAO;AACd,YAAQ,MAAM,8BAA8B,MAAM,OAAO,EAAE;AAC3D,WAAO,CAAC;AAAA,EACV;AACF;AAQA,eAAe,0BAA0B,SAAS;AAChD,MAAI;AAEF,UAAM,mBAAmB,MAA8B;AAAA,MACrD,CAAC,UAAU,iBAAiB,gBAAgB,YAAY,SAAS,OAAO;AAAA,MACxE;AAAA,QACE,OAAO;AAAA,QACP,UAAU;AAAA,MACZ;AAAA,IACF;AAEA,QAAI,iBAAiB,WAAW,GAAG;AACjC,aAAO;AAAA,IACT;AAGA,UAAM,aAAa,iBAAiB,IAAI,CAAC,YAAY;AAAA,MACnD,MAAM,OAAO,OAAO;AAAA,MACpB,MAAM,OAAO,OAAO;AAAA,IACtB,EAAE;AAGF,UAAM,cAAc,iBACjB,IAAI,CAAC,WAAW,OAAO,OAAO,WAAW,EACzC,KAAK,MAAM;AAGd,UAAM,qBAAqB;AAAA,MACzB,aAAa,QAAQ;AAAA,MACrB,cAAc,KAAK,MAAM,QAAQ,cAAc,GAAG;AAAA;AAAA,IACpD;AAGA,UAAM,UACJ,YAAY,SAAS,MACjB,YAAY,UAAU,GAAG,GAAI,IAAI,QACjC;AAEN,WAAO;AAAA,MACL;AAAA,MACA,SAAS;AAAA,IACX;AAAA,EACF,SAAS,OAAO;AACd,YAAQ,MAAM,sCAAsC,MAAM,OAAO,EAAE;AACnE,WAAO;AAAA,EACT;AACF;AAOA,eAAe,yBAAyB;AACtC,MAAI;AAEF,UAAM,WAAW;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAejB,UAAM,cAAc,MAAM,aAAa,QAAQ;AAG/C,UAAM,OACJ,eAAe,YAAY,QAAQ,MAAM,QAAQ,YAAY,IAAI,IAC7D,YAAY,OACZ,MAAM,QAAQ,WAAW,IACzB,cACA,CAAC;AAGP,QAAI,KAAK,WAAW,GAAG;AACrB,aAAO;AAAA,QACL,cAAc,CAAC;AAAA,QACf,YAAY;AAAA,MACd;AAAA,IACF;AAEA,WAAO;AAAA,MACL,cAAc,KAAK,IAAI,CAAC,SAAS;AAAA,QAC/B,MAAM,IAAI,UAAU,MAAM,GAAG,EAAE,CAAC;AAAA,QAChC,WAAW,IAAI;AAAA,MACjB,EAAE;AAAA,MACF,YAAY,KAAK,OAAO,CAAC,KAAK,QAAQ,MAAM,IAAI,YAAY,CAAC;AAAA,IAC/D;AAAA,EACF,SAAS,OAAO;AACd,YAAQ,MAAM,mCAAmC,MAAM,OAAO,EAAE;AAChE,WAAO;AAAA,EACT;AACF;AAQA,eAAe,0BAA0B,SAAS;AAChD,MAAI;AAEF,UAAM,2BAA2B,MAA2B,UAAU;AAAA,MACpE,OAAO,CAAC,wBAAwB;AAAA,MAChC,OAAO;AAAA,IACT,CAAC;AAED,QAAI,yBAAyB,WAAW,GAAG;AACzC,aAAO,CAAC;AAAA,IACV;AAGA,WAAO,yBAAyB,IAAI,CAAC,WAAW;AAAA,MAC9C,WAAW,MAAM;AAAA,MACjB,SAAS,MAAM,KAAK,WAAW;AAAA,MAC/B,SAAS,MAAM,KAAK,WAAW;AAAA,IACjC,EAAE;AAAA,EACJ,SAAS,OAAO;AACd,YAAQ,MAAM,sCAAsC,MAAM,OAAO,EAAE;AACnE,WAAO,CAAC;AAAA,EACV;AACF;AAQA,eAAe,oBAAoB,YAAY;AAC7C,MAAI;AAEF,UAAM,qBAAqB,MAA2B,UAAU;AAAA,MAC9D,OAAO,CAAC,eAAe,eAAe,aAAa;AAAA,MACnD,OAAO;AAAA,IACT,CAAC;AAED,QAAI,mBAAmB,WAAW,GAAG;AACnC,aAAO,CAAC;AAAA,IACV;AAGA,WAAO,mBAAmB,IAAI,CAAC,WAAW;AAAA,MACxC,WAAW,MAAM;AAAA,MACjB,OAAO,MAAM,KAAK,SAAS,CAAC,MAAM,KAAK,YAAY,cAAc;AAAA,MACjE,SAAS,MAAM,KAAK,WAAW,GAAG,MAAM,UAAU;AAAA,IACpD,EAAE;AAAA,EACJ,SAAS,OAAO;AACd,YAAQ,MAAM,gCAAgC,MAAM,OAAO,EAAE;AAC7D,WAAO,CAAC;AAAA,EACV;AACF;AAOA,eAAe,wBAAwB;AACrC,MAAI;AAEF,UAAM,kBAAkB,MAA2B,UAAU;AAAA,MAC3D,OAAO,CAAC,WAAW;AAAA,MACnB,OAAO;AAAA,MACP,mBAAmB;AAAA,IACrB,CAAC;AAED,QAAI,gBAAgB,WAAW,GAAG;AAChC,aAAO,CAAC;AAAA,IACV;AAGA,WAAO,gBAAgB,IAAI,CAAC,WAAW;AAAA,MACrC,MAAM,MAAM,KAAK,QAAQ;AAAA,MACzB,aAAa,MAAM,KAAK,eAAe;AAAA,MACvC,WAAW,MAAM;AAAA,IACnB,EAAE;AAAA,EACJ,SAAS,OAAO;AACd,YAAQ,MAAM,kCAAkC,MAAM,OAAO,EAAE;AAC/D,WAAO,CAAC;AAAA,EACV;AACF;AASA,eAAe,qBAAqB,OAAO,SAAS;AAClD,MAAI;AAEF,UAAM,iBAAiB,MAA8B;AAAA,MACnD;AAAA,QACE,eAAe;AAAA,QACf,OAAO;AAAA,MACT;AAAA,IACF;AAEA,QAAI,eAAe,WAAW,GAAG;AAC/B,aAAO,CAAC;AAAA,IACV;AAGA,WAAO,eAAe,IAAI,CAAC,aAAa;AAAA,MACtC,MAAM,QAAQ;AAAA,MACd,MAAM,QAAQ;AAAA,MACd,aAAa,QAAQ;AAAA,MACrB,YAAY,QAAQ;AAAA,IACtB,EAAE;AAAA,EACJ,SAAS,OAAO;AACd,YAAQ,MAAM,iCAAiC,MAAM,OAAO,EAAE;AAC9D,WAAO,CAAC;AAAA,EACV;AACF;AAUA,SAAS,8BAA8B,SAAS,OAAO,QAAQ;AAE7D,MAAI,UAAU;AAGd,MAAI,OAAO;AACT,eAAW,gBAAgB,KAAK;AAAA,EAClC;AAGA,MAAI,QAAQ;AACV,eAAW,iBAAiB,MAAM;AAAA,EACpC;AAGA,MAAI,QAAQ,eAAe,QAAQ,YAAY,SAAS,GAAG;AACzD,eAAW,WAAW,QAAQ,YAAY,MAAM;AAAA,EAClD;AAGA,MAAI,QAAQ,qBAAqB;AAC/B,eAAW;AAAA,EACb;AAGA,MAAI,QAAQ,iBAAiB,QAAQ,cAAc,SAAS,GAAG;AAC7D,eAAW,KAAK,QAAQ,cAAc,MAAM;AAAA,EAC9C;AAGA,MAAI,QAAQ,kBAAkB,QAAQ,eAAe,SAAS,GAAG;AAC/D,eAAW,KAAK,QAAQ,eAAe,MAAM;AAAA,EAC/C;AAEA,SAAO;AACT;AAQA,eAAeA,iBAAgB,MAAM;AAEnC,SAAO,KACJ,YAAY,EACZ,QAAQ,YAAY,EAAE,EACtB,MAAM,KAAK,EACX,OAAO,CAAC,SAAS,KAAK,SAAS,CAAC,EAChC,OAAO,CAAC,SAAS,CAAC,CAAC,OAAO,OAAO,OAAO,MAAM,EAAE,SAAS,IAAI,CAAC;AACnE;AAGA,IAAO,6CAAQ;AAAA,EACb,MAAM;AAAA,EACN,aACE;AAAA,EACF,aAAa;AAAA,EACb,cAAc;AAAA,EACd;AACF;;;AevtBA;AADA,SAAS,KAAAC,UAAS;;;ACAlB;AAyBA,eAAsB,kBAAkB,QAAQ;AAC9C,MAAI,CAAC,UAAU,CAAC,OAAO,YAAY,CAAC,OAAO,YAAY;AACrD,YAAQ,MAAM,+BAA+B,MAAM;AACnD,UAAM,IAAI,MAAM,8CAA8C;AAAA,EAChE;AAEA,MAAI;AACF,YAAQ,IAAI,8BAA8B,OAAO,QAAQ,EAAE;AAG3D,UAA0B;AAAA,MACxB,OAAO;AAAA,MACP,OAAO;AAAA,MACP,OAAO;AAAA,IACT;AAGA,UAAM,WAAW,MAAM,4BAA4B,CAAC,OAAO,QAAQ,CAAC;AAEpE,WAAO;AAAA,MACL,UAAU,OAAO;AAAA,MACjB,SAAS;AAAA,MACT,aAAa,SAAS;AAAA,MACtB,YAAW,oBAAI,KAAK,GAAE,YAAY;AAAA,IACpC;AAAA,EACF,SAAS,OAAO;AACd,YAAQ;AAAA,MACN,oCAAoC,OAAO,QAAQ;AAAA,MACnD;AAAA,IACF;AACA,UAAM,IAAI,MAAM,kCAAkC,MAAM,OAAO,EAAE;AAAA,EACnE;AACF;AAoEA,eAAsB,4BAA4B,WAAW;AAC3D,MAAI,CAAC,aAAa,UAAU,WAAW,GAAG;AACxC,WAAO,CAAC;AAAA,EACV;AAEA,MAAI;AAEF,UAAM,eAAe,UAAU,IAAI,MAAM,GAAG,EAAE,KAAK,GAAG;AACtD,UAAM,QAAQ,8CAA8C,YAAY;AAExE,UAAM,eAAe,MAAM,aAAa,OAAO,SAAS;AAGxD,UAAM,gBAAgB,aACnB,OAAO,CAAC,WAAW,OAAO,SAAS,MAAM,EACzC,IAAI,CAAC,WAAW,OAAO,EAAE;AAG5B,QAAI,cAAc,SAAS,GAAG;AAC5B,YAAM,oBAAoB,cAAc,IAAI,MAAM,GAAG,EAAE,KAAK,GAAG;AAC/D,YAAM,aAAa,mDAAmD,iBAAiB;AAEvF,YAAM,gBAAgB,MAAM,aAAa,YAAY,aAAa;AAGlE,YAAM,cAAc,CAAC,GAAG,YAAY;AAGpC,YAAM,cAAc,IAAI,IAAI,YAAY,IAAI,CAAC,WAAW,OAAO,EAAE,CAAC;AAElE,iBAAW,eAAe,eAAe;AACvC,YAAI,CAAC,YAAY,IAAI,YAAY,EAAE,GAAG;AACpC,sBAAY,KAAK,WAAW;AAC5B,sBAAY,IAAI,YAAY,EAAE;AAAA,QAChC;AAAA,MACF;AAEA,aAAO;AAAA,IACT;AAGA,WAAO;AAAA,EACT,SAAS,OAAO;AACd,YAAQ,MAAM,iDAAiD,KAAK;AACpE,UAAM;AAAA,EACR;AACF;;;ADjKA;AAcA,eAAeC,SAAQ,OAAO,YAAY;AACxC,MAAI;AACF,eAAW,QAAQ,4CAA4C;AAAA,MAC7D,gBAAgB,MAAM;AAAA,MACtB,cAAc,MAAM,aAAa,UAAU;AAAA,MAC3C,iBAAiB,MAAM,aAAa,UAAU;AAAA,IAChD,CAAC;AAGD,UAAM;AAAA,MACJ;AAAA,MACA,cAAc,CAAC;AAAA,MACf,cAAc,CAAC;AAAA,MACf,8BAA8B;AAAA,MAC9B,0BAA0B;AAAA,MAC1B,yBAAyB;AAAA,MACzB,cAAc;AAAA,IAChB,IAAI;AAGJ,QAAI,CAAC,gBAAgB;AACnB,YAAM,QAAQ,IAAI,MAAM,4BAA4B;AACpD,YAAM,OAAO;AACb,YAAM;AAAA,IACR;AAEA,eAAW,SAAS,qCAAqC;AAAA,MACvD;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAGD,QAAI,aAAa;AACjB,QAAI,mBAAmB;AACvB,QAAI,iBAAiB;AACrB,QAAI,gBAAgB;AACpB,QAAI,mBAAmB;AACvB,QAAI,eAAe;AAGnB,QAAI;AACF,YAAM,uBACJ,MAA2B,sBAAsB;AACnD,iBAAW,SAAS,oCAAoC;AAAA,QACtD,oBAAoB,CAAC,CAAC;AAAA,MACxB,CAAC;AAED,UAAI,wBAAwB;AAC1B,yBAAiB,MAAkC;AAAA,UACjD;AAAA,QACF;AACA,mBAAW,SAAS,6BAA6B,EAAE,eAAe,CAAC;AAAA,MACrE;AAAA,IACF,SAAS,KAAK;AACZ;AAAA,QACE;AAAA,QACA;AAAA,QACA;AAAA,UACE,OAAO,IAAI;AAAA,QACb;AAAA,MACF;AAAA,IAEF;AAGA,QAAI,YAAY,SAAS,GAAG;AAC1B,iBAAW,QAAQ,cAAc,YAAY,MAAM,eAAe;AAClE,UAAI;AACF,cAAM,oBAAoB,MAAM;AAAA,UAC9B;AAAA,UACA;AAAA,UACA;AAAA,YACE;AAAA,UACF;AAAA,QACF;AAEA,qBAAa,kBAAkB;AAC/B,mBAAW,SAAS,gCAAgC;AAAA,UAClD;AAAA,QACF,CAAC;AAED,YAAI,wBAAwB;AAC1B,6BAAmB,kBAAkB;AACrC,0BAAgB,kBAAkB;AAElC,cAAI,kBAAkB;AACpB,uBAAW,QAAQ,8BAA8B;AAAA,cAC/C,MAAM;AAAA,cACN,IAAI;AAAA,YACN,CAAC;AAAA,UACH;AAAA,QACF;AAAA,MACF,SAAS,KAAK;AACZ,mBAAW,SAAS,kCAAkC;AAAA,UACpD,OAAO,IAAI;AAAA,UACX;AAAA,QACF,CAAC;AAAA,MAEH;AAAA,IACF;AAGA,QAAI,YAAY,SAAS,GAAG;AAC1B,iBAAW,QAAQ,cAAc,YAAY,MAAM,eAAe;AAClE,UAAI;AACF,cAAM,mBAAmB,MAAM;AAAA,UAC7B;AAAA,UACA;AAAA,QACF;AAGA,YAAI,iBAAiB,cAAc;AACjC,qBAAW,QAAQ,qCAAqC;AAAA,YACtD,UAAU,iBAAiB;AAAA,UAC7B,CAAC;AAGD,cAAI,0BAA0B,CAAC,kBAAkB;AAC/C,gBAAI;AAEF,oBAAM,eAAe,MAA2B,aAAa;AAAA,gBAC3D;AAAA,gBACA;AAAA,cACF,CAAC;AAED,kBAAI,aAAa,eAAe;AAC9B,mCAAmB;AACnB,gCAAgB,aAAa;AAC7B,2BAAW,QAAQ,sCAAsC;AAAA,kBACvD,WAAW;AAAA,gBACb,CAAC;AAAA,cACH;AAAA,YACF,SAAS,WAAW;AAClB,yBAAW,QAAQ,6CAA6C;AAAA,gBAC9D,OAAO,UAAU;AAAA,cACnB,CAAC;AAAA,YAEH;AAAA,UACF;AAAA,QACF;AAAA,MACF,SAAS,KAAK;AACZ,mBAAW,SAAS,kCAAkC;AAAA,UACpD,OAAO,IAAI;AAAA,UACX;AAAA,QACF,CAAC;AAAA,MAEH;AAAA,IACF;AAGA,QAAI,cAAc,kBAAkB;AAClC;AAAA,QACE;AAAA,QACA;AAAA,QACA;AAAA,UACE;AAAA,UACA;AAAA,UACA;AAAA,QACF;AAAA,MACF;AAGA,UAAI,CAAC,6BAA6B;AAChC,YAAI;AAEF,gBAA2B,mBAAmB;AAC9C,6BAAmB;AACnB,qBAAW,QAAQ,6CAA6C;AAGhE,cAAI,eAAe;AACjB,gBAAI;AACF,oBAAM,eACJ,MAA2B;AAAA,gBACzB;AAAA,gBACA;AAAA,cACF;AAEF,oBAAM,cAAc,MAA2B;AAAA,gBAC7C;AAAA,gBACA;AAAA,cACF;AAEA,kBAAI,aAAa;AACf,sBAA2B;AAAA,kBACzB,YAAY;AAAA,kBACZ,YAAY;AAAA,gBACd;AACA,+BAAe;AACf,2BAAW,QAAQ,sCAAsC;AAAA,kBACvD,MAAM,YAAY;AAAA,kBAClB,YAAY,YAAY;AAAA,gBAC1B,CAAC;AAAA,cACH;AAAA,YACF,SAAS,UAAU;AACjB,yBAAW,QAAQ,gCAAgC;AAAA,gBACjD,OAAO,SAAS;AAAA,cAClB,CAAC;AAAA,YAEH;AAAA,UACF;AAAA,QACF,SAAS,UAAU;AACjB,qBAAW,SAAS,2BAA2B;AAAA,YAC7C,OAAO,SAAS;AAAA,UAClB,CAAC;AAAA,QAEH;AAAA,MACF,OAAO;AACL,YAAI;AAEF,gBAAM,uBACH,MAA2B,sBAAsB,KAAM,CAAC;AAE3D,gBAAM,oBAAoB,MAAM;AAAA,YAC9B;AAAA,YACA;AAAA,cACE;AAAA,cACA;AAAA,cACA;AAAA,cACA;AAAA,cACA;AAAA,YACF;AAAA,YACA;AAAA,UACF;AAEA,gBAA2B,oBAAoB,iBAAiB;AAChE,6BAAmB;AACnB,qBAAW,QAAQ,uCAAuC;AAAA,YACxD;AAAA,UACF,CAAC;AAAA,QACH,SAAS,cAAc;AACrB,qBAAW,SAAS,gCAAgC;AAAA,YAClD,OAAO,aAAa;AAAA,UACtB,CAAC;AAAA,QAEH;AAAA,MACF;AAAA,IACF,OAAO;AACL;AAAA,QACE;AAAA,QACA;AAAA,MACF;AAAA,IACF;AAGA,QAAI,CAAC,cAAc;AACjB,UAAI;AACF,uBAAe,MAA2B,eAAe;AACzD,mBAAW,SAAS,2BAA2B;AAAA,UAC7C,OAAO,eACH,GAAG,aAAa,IAAI,IAAI,aAAa,UAAU,KAC/C;AAAA,QACN,CAAC;AAAA,MACH,SAAS,UAAU;AACjB,mBAAW,QAAQ,+BAA+B;AAAA,UAChD,OAAO,SAAS;AAAA,QAClB,CAAC;AAAA,MAEH;AAAA,IACF;AAGA,QAAI;AACJ,QAAI;AACF,yBAAmB,MAAM;AAAA,QACvB;AAAA,QACA;AAAA,QACA,cAAc;AAAA,MAChB;AACA,iBAAW,SAAS,+BAA+B;AAAA,QACjD,iBAAiB,kBAAkB,UAAU;AAAA,MAC/C,CAAC;AAAA,IACH,SAAS,cAAc;AACrB,iBAAW,QAAQ,wCAAwC;AAAA,QACzD,OAAO,aAAa;AAAA,MACtB,CAAC;AACD,yBAAmB;AAAA,IACrB;AAGA,QAAI;AACF,YAA2B;AAAA,QACzB;AAAA,QACA;AAAA,UACE,kBAAkB,YAAY;AAAA,UAC9B,kBAAkB,YAAY;AAAA,UAC9B;AAAA,UACA,kBAAkB,mBACd;AAAA,YACE,MAAM;AAAA,YACN,IAAI;AAAA,UACN,IACA;AAAA,UACJ;AAAA,UACA,yBAAyB,mBACrB,0BACA;AAAA,QACN;AAAA,QACA,CAAC;AAAA;AAAA,QACD;AAAA,MACF;AACA,iBAAW,SAAS,qCAAqC;AAAA,IAC3D,SAAS,aAAa;AACpB,iBAAW,QAAQ,+CAA+C;AAAA,QAChE,OAAO,YAAY;AAAA,MACrB,CAAC;AAAA,IAEH;AAGA;AAAA,MACE;AAAA,MACA;AAAA,IACF;AAEA,UAAM,eAAe;AAAA,MACnB,QAAQ;AAAA,MACR,SAAS,oCAAoC,cAAc;AAAA,MAC3D,cAAc,eACV;AAAA,QACE,MAAM,aAAa;AAAA,QACnB,YAAY,aAAa;AAAA,MAC3B,IACA;AAAA,MACJ,mBAAmB;AAAA,QACjB;AAAA,QACA;AAAA,QACA;AAAA,MACF;AAAA,MACA,WAAW;AAAA,IACb;AAEA,WAAO;AAAA,MACL,SAAS;AAAA,QACP;AAAA,UACE,MAAM;AAAA,UACN,MAAM,KAAK,UAAU,YAAY;AAAA,QACnC;AAAA,MACF;AAAA,IACF;AAAA,EACF,SAAS,OAAO;AAEd,eAAW,SAAS,6CAA6C;AAAA,MAC/D,OAAO,MAAM;AAAA,MACb,OAAO,MAAM;AAAA,MACb,OAAO;AAAA,QACL,gBAAgB,MAAM;AAAA,QACtB,cAAc,MAAM,aAAa,UAAU;AAAA,QAC3C,iBAAiB,MAAM,aAAa,UAAU;AAAA,MAChD;AAAA,IACF,CAAC;AAGD,UAAM,gBAAgB;AAAA,MACpB,OAAO;AAAA,MACP,WAAW,MAAM,QAAQ;AAAA,MACzB,cAAc,MAAM;AAAA,IACtB;AAEA,WAAO;AAAA,MACL,SAAS;AAAA,QACP;AAAA,UACE,MAAM;AAAA,UACN,MAAM,KAAK,UAAU,aAAa;AAAA,QACpC;AAAA,MACF;AAAA,IACF;AAAA,EACF;AACF;AAUA,eAAe,mBAAmB,gBAAgB,UAAU,UAAU,CAAC,GAAG;AACxE,MAAI;AACF;AAAA,MACE;AAAA,MACA,cAAc,SAAS,MAAM,8BAA8B,cAAc;AAAA,IAC3E;AAEA,UAAM,SAAS;AAAA,MACb,YAAY;AAAA,MACZ,kBAAkB;AAAA,MAClB,eAAe;AAAA,IACjB;AAGA,eAAW,WAAW,UAAU;AAC9B,UAAI;AAEF,gBAAQ,IAAI,qCAAqC;AAAA,UAC/C,SAAS,QAAQ;AAAA,UACjB,MAAM,QAAQ;AAAA,UACd;AAAA,QACF,CAAC;AAGD,cAAM,YAAY,MAA+B;AAAA,UAC/C,QAAQ;AAAA,UACR,QAAQ;AAAA,UACR;AAAA,UACA,CAAC;AAAA;AAAA,UACD;AAAA;AAAA,QACF;AAGA,gBAAQ,IAAI,gCAAgC;AAAA,UAC1C;AAAA,UACA,MAAM,QAAQ;AAAA,QAChB,CAAC;AAED,mBAAW,SAAS,yBAAyB,QAAQ,IAAI,EAAE;AAAA,MAC7D,SAAS,QAAQ;AAEf,gBAAQ,MAAM,+BAA+B;AAAA,UAC3C,OAAO,OAAO;AAAA,UACd,OAAO,OAAO;AAAA,UACd,aAAa,QAAQ;AAAA,UACrB,gBACE,QAAQ,WAAW,QAAQ,QAAQ,UAAU,GAAG,EAAE,IAAI;AAAA,QAC1D,CAAC;AAED;AAAA,UACE;AAAA,UACA;AAAA,UACA;AAAA,YACE,OAAO,OAAO;AAAA,YACd,aAAa,QAAQ;AAAA,UACvB;AAAA,QACF;AAAA,MAEF;AAAA,IACF;AAGA,QAAI;AACF,YAAM,qBAAqB,MAA4B;AAAA,QACrD;AAAA,QACA;AAAA,MACF;AACA,aAAO,aAAa,mBAAmB;AAEvC,UAAI,OAAO,YAAY;AACrB,mBAAW,QAAQ,wBAAwB;AAAA,UACzC,eAAe,mBAAmB;AAAA,UAClC,UAAU,mBAAmB;AAAA,UAC7B,YAAY,mBAAmB;AAAA,QACjC,CAAC;AAAA,MACH;AAAA,IACF,SAAS,YAAY;AACnB,iBAAW,QAAQ,gCAAgC;AAAA,QACjD,OAAO,WAAW;AAAA,MACpB,CAAC;AAAA,IAEH;AAGA,QAAI,QAAQ,wBAAwB;AAClC,UAAI;AACF,cAAM,iBACJ,MAAkC,iBAAiB,cAAc;AAGnE,cAAM,qBAAqB,MAA2B,aAAa;AAAA,UACjE;AAAA,UACA;AAAA,QACF,CAAC;AAED,YAAI,mBAAmB,eAAe;AACpC,iBAAO,mBAAmB;AAC1B,iBAAO,gBAAgB,mBAAmB;AAE1C,qBAAW,QAAQ,8BAA8B;AAAA,YAC/C,MAAM;AAAA,YACN,IAAI,OAAO;AAAA,YACX,YAAY,mBAAmB;AAAA,UACjC,CAAC;AAGD,gBAAkC;AAAA,YAChC;AAAA,YACA,OAAO;AAAA,UACT;AAAA,QACF,OAAO;AACL,iBAAO,gBAAgB;AAAA,QACzB;AAAA,MACF,SAAS,WAAW;AAClB,mBAAW,QAAQ,qCAAqC;AAAA,UACtD,OAAO,UAAU;AAAA,QACnB,CAAC;AAAA,MAEH;AAAA,IACF;AAEA,WAAO;AAAA,EACT,SAAS,OAAO;AACd,eAAW,SAAS,iCAAiC;AAAA,MACnD,OAAO,MAAM;AAAA,MACb;AAAA,IACF,CAAC;AACD,UAAM;AAAA,EACR;AACF;AASA,eAAe,mBAAmB,gBAAgB,aAAa;AAC7D,MAAI;AACF;AAAA,MACE;AAAA,MACA,cAAc,YAAY,MAAM,kCAAkC,cAAc;AAAA,IAClF;AAEA,UAAM,SAAS;AAAA,MACb,cAAc;AAAA,MACd,UAAU;AAAA,IACZ;AAGA,QAAI,CAAC,YAAY,QAAQ;AACvB,aAAO;AAAA,IACT;AAGA,eAAW,UAAU,aAAa;AAChC,UAAI;AACF,cAAyB,kBAAkB,MAAM;AACjD,mBAAW,SAAS,6BAA6B,OAAO,IAAI,EAAE;AAAA,MAChE,SAAS,YAAY;AACnB,mBAAW,QAAQ,iCAAiC;AAAA,UAClD,OAAO,WAAW;AAAA,UAClB,MAAM,OAAO;AAAA,QACf,CAAC;AAAA,MAEH;AAAA,IACF;AAGA,UAAM,wBAAwB,YAAY,OAAO,CAAC,MAAM,YAAY;AAElE,YAAM,mBAAmB,KAAK,cAAc,UAAU;AACtD,YAAM,sBAAsB,QAAQ,cAAc,UAAU;AAC5D,aAAO,sBAAsB,mBAAmB,UAAU;AAAA,IAC5D,GAAG,YAAY,CAAC,CAAC;AAGjB,QAAI;AACF,YAA2B;AAAA,QACzB;AAAA,QACA,sBAAsB;AAAA,MACxB;AACA,aAAO,eAAe;AACtB,aAAO,WAAW;AAAA,QAChB,MAAM;AAAA,QACN,YAAY,sBAAsB;AAAA,MACpC;AAEA,iBAAW,QAAQ,gDAAgD;AAAA,QACjE,MAAM,sBAAsB;AAAA,QAC5B,cAAc,sBAAsB,cAAc,UAAU;AAAA,MAC9D,CAAC;AAAA,IACH,SAAS,UAAU;AACjB,iBAAW,QAAQ,uCAAuC;AAAA,QACxD,OAAO,SAAS;AAAA,QAChB,MAAM,sBAAsB;AAAA,MAC9B,CAAC;AAAA,IAEH;AAGA,QAAI;AACF,YAA2B;AAAA,QACzB;AAAA,QACA;AAAA,UACE,OAAO,YAAY;AAAA,UACnB,OAAO,YAAY,IAAI,CAAC,MAAM,EAAE,IAAI;AAAA,QACtC;AAAA,QACA,CAAC;AAAA;AAAA,QACD;AAAA,MACF;AACA,iBAAW,SAAS,mCAAmC;AAAA,IACzD,SAAS,aAAa;AACpB,iBAAW,QAAQ,6CAA6C;AAAA,QAC9D,OAAO,YAAY;AAAA,MACrB,CAAC;AAAA,IAEH;AAEA,WAAO;AAAA,EACT,SAAS,OAAO;AACd,eAAW,SAAS,iCAAiC;AAAA,MACnD,OAAO,MAAM;AAAA,MACb;AAAA,IACF,CAAC;AACD,UAAM;AAAA,EACR;AACF;AAUA,eAAe,mBACb,sBACA,SACA,kBACA;AACA,QAAM;AAAA,IACJ;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF,IAAI;AAEJ,MAAI;AACF,eAAW,QAAQ,oCAAoC,gBAAgB,EAAE;AAGzE,UAAM,oBAAoB,EAAE,GAAG,qBAAqB;AAGpD,YAAQ,kBAAkB;AAAA,MACxB,KAAK;AAEH,YAAI,YAAY;AAEd,gBAAM,eAAe,kBAAkB;AACvC,4BAAkB,qBAAqB,CAAC;AACxC,4BAAkB,QAAQ;AAAA,QAC5B;AACA;AAAA,MAEF,KAAK;AAGH,YAAI,kBAAkB;AACpB,4BAAkB,gBAAgB;AAAA,QACpC;AACA;AAAA,MAEF,KAAK;AAAA,MACL;AAEE,YAAI,YAAY;AAEd,gBAAM,eAAe,kBAAkB;AAGvC,cAAI,kBAAkB,oBAAoB;AACxC,kBAAM,mBAAmB,YAAY;AAAA,cACnC,CAAC,WAAW,OAAO;AAAA,YACrB;AAEA,8BAAkB,qBAChB,kBAAkB,mBAAmB,OAAO,CAAC,SAAS;AAEpD,kBACE,KAAK,aACL,KAAK,UAAU,SAAS,cAAc,UAAU,GAChD;AACA,uBAAO;AAAA,cACT;AAGA,kBACE,KAAK,QACL,iBAAiB,KAAK,CAACC,UAAS,KAAK,KAAK,SAASA,KAAI,CAAC,GACxD;AACA,uBAAO;AAAA,cACT;AAGA,kBACE,KAAK,aACL,KAAK,IAAI,IAAI,KAAK,YAAY,IAAI,KAAK,KACvC;AAEA,uBAAO;AAAA,cACT;AAEA,qBAAO;AAAA,YACT,CAAC;AAAA,UACL;AAAA,QACF;AAGA,YAAI,kBAAkB;AACpB,4BAAkB,gBAAgB;AAGlC,cAAI,YAAY,SAAS,KAAK,kBAAkB,oBAAoB;AAElE,8BAAkB,mBAAmB,QAAQ,CAAC,SAAS;AACrD,kBAAI,KAAK,gBAAgB,UAAU,eAAe;AAEhD,oBACE,kBAAkB,eAClB,KAAK,QACL,KAAK,KAAK,SAAS,MAAM,GACzB;AACA,uBAAK,WAAW,KAAK,IAAI,KAAK,WAAW,KAAK,CAAG;AAAA,gBACnD,WACE,kBAAkB,sBAClB,KAAK,QACL,KAAK,KAAK,SAAS,MAAM,GACzB;AACA,uBAAK,WAAW,KAAK,IAAI,KAAK,WAAW,KAAK,CAAG;AAAA,gBACnD;AAAA,cAEF;AAAA,YACF,CAAC;AAGD,8BAAkB,mBAAmB;AAAA,cACnC,CAAC,GAAG,MAAM,EAAE,WAAW,EAAE;AAAA,YAC3B;AAAA,UACF;AAAA,QACF;AACA;AAAA,IACJ;AAEA,WAAO;AAAA,EACT,SAAS,OAAO;AACd,eAAW,SAAS,8BAA8B;AAAA,MAChD,OAAO,MAAM;AAAA,IACf,CAAC;AAED,WAAO;AAAA,EACT;AACF;AAUA,eAAe,yBACb,gBACA,eACA,gBACA;AACA,MAAI;AACF,eAAW,QAAQ,8BAA8B;AAGjD,UAAM,gBAAgB,MAA2B,sBAAsB;AACvE,UAAMC,eAAc,MAA2B,eAAe;AAG9D,UAAM,iBAAiB,MAA+B;AAAA,MACpD;AAAA,MACA;AAAA,IACF;AAGA,QAAI,cAAc;AAElB,QAAI,gBAAgB;AAElB,UAAIA,cAAa;AACf,sBAAc,sCAAsCA,aAAY,IAAI,KAAKA,aAAY,UAAU;AAE/F,YAAI,eAAe;AACjB,yBAAe,wBAAwB,cAAc;AAAA,YACnD;AAAA,YACA;AAAA,UACF,CAAC;AAAA,QACH;AAAA,MACF,WAAW,eAAe;AACxB,sBAAc,kCAAkC,cAAc;AAAA,UAC5D;AAAA,UACA;AAAA,QACF,CAAC;AAAA,MACH;AAGA,UAAI,eAAe,SAAS,GAAG;AAC7B,cAAM,iBAAiB,eACpB,IAAI,CAAC,QAAQ,IAAI,OAAO,EACxB,KAAK,GAAG;AACX,cAAM,iBAAiB,MAA6B;AAAA,UAClD;AAAA,UACA,EAAE,cAAc,IAAI;AAAA,QACtB;AAEA,uBAAe,wBAAwB,cAAc;AAAA,MACvD;AAAA,IACF,OAAO;AAEL,UAAIA,cAAa;AACf,sBAAc,uBAAuBA,aAAY,IAAI,KAAKA,aAAY,UAAU;AAEhF,YAAI,eAAe;AACjB,yBAAe,SAAS,cAAc,QAAQ,MAAM,GAAG,CAAC;AAAA,QAC1D;AAAA,MACF,WAAW,eAAe;AACxB,sBAAc,mBAAmB,cAAc,QAAQ,MAAM,GAAG,CAAC;AAAA,MACnE;AAAA,IACF;AAGA,UAAM,gBAAgB,CAAC;AAEvB,QAAIA,cAAa;AACf,oBAAc;AAAA,QACZ,YAAYA,aAAY,IAAI,KAAKA,aAAY,UAAU;AAAA,MACzD;AAAA,IACF;AAEA,QAAI,eAAe;AACjB,cAAQ,eAAe;AAAA,QACrB,KAAK;AACH,wBAAc,KAAK,qCAAqC;AACxD;AAAA,QACF,KAAK;AACH,wBAAc,KAAK,8BAA8B;AACjD;AAAA,QACF,KAAK;AACH,wBAAc,KAAK,yCAAyC;AAC5D;AAAA,QACF,KAAK;AACH,wBAAc,KAAK,0CAA0C;AAC7D;AAAA,QACF,KAAK;AACH,wBAAc,KAAK,yBAAyB;AAC5C;AAAA,QACF;AACE,wBAAc,KAAK,8BAA8B;AAAA,MACrD;AAAA,IACF;AAGA,QAAI,iBAAiB,cAAc,oBAAoB;AACrD,YAAM,gBAAgB,cAAc,mBACjC,MAAM,GAAG,CAAC,EACV,IAAI,CAAC,SAAS;AACb,YAAI,KAAK,SAAS,QAAQ;AACxB,iBAAO,6BAA6B,KAAK,QAAQ,KAAK,IAAI;AAAA,QAC5D,WAAW,KAAK,SAAS,UAAU;AACjC,iBAAO,kBAAkB,KAAK,IAAI;AAAA,QACpC;AACA,eAAO;AAAA,MACT,CAAC,EACA,OAAO,OAAO;AAEjB,oBAAc,KAAK,GAAG,aAAa;AAAA,IACrC;AAEA,WAAO;AAAA,MACL,SAAS;AAAA,MACT,eAAe,cAAc,SAAS,IAAI,gBAAgB;AAAA,IAC5D;AAAA,EACF,SAAS,OAAO;AACd,eAAW,SAAS,sCAAsC;AAAA,MACxD,OAAO,MAAM;AAAA,IACf,CAAC;AAED,WAAO;AAAA,MACL,SAAS;AAAA,IACX;AAAA,EACF;AACF;AAGA,IAAO,yCAAQ;AAAA,EACb,MAAM;AAAA,EACN,aACE;AAAA,EACF,aAAa;AAAA,EACb,cAAc;AAAA,EACd,SAAAF;AACF;;;AEl5BA;AADA,SAAS,KAAAG,UAAS;;;ACGlB;;;ADIA;AAGA;AACA;AAcA,eAAeC,SAAQ,OAAO,YAAY;AACxC,MAAI;AACF,eAAW,QAAQ,0CAA0C;AAAA,MAC3D,OAAO,MAAM,OAAO,UAAU,GAAG,EAAE;AAAA,MACnC,gBAAgB,MAAM;AAAA,MACtB,aAAa,MAAM,eAAe;AAAA,IACpC,CAAC;AAGD,UAAM;AAAA,MACJ;AAAA,MACA;AAAA,MACA,cAAc;AAAA,MACd,cAAc,CAAC;AAAA,MACf,iBAAiB,CAAC;AAAA,MAClB,oBAAoB;AAAA,MACpB,kBAAkB;AAAA,MAClB,iBAAiB;AAAA,MACjB,wBAAwB,CAAC;AAAA,IAC3B,IAAI;AAGJ,QAAI,CAAC,OAAO;AACV,YAAM,QAAQ,IAAI,MAAM,mBAAmB;AAC3C,YAAM,OAAO;AACb,YAAM;AAAA,IACR;AAEA,QAAI,CAAC,gBAAgB;AACnB,YAAM,QAAQ,IAAI,MAAM,6BAA6B;AACrD,YAAM,OAAO;AACb,YAAM;AAAA,IACR;AAEA,eAAW,SAAS,gCAAgC;AAAA,MAClD;AAAA,MACA;AAAA,MACA,aAAa,OAAO,KAAK,WAAW;AAAA,MACpC,SAAS,OAAO,KAAK,cAAc;AAAA,IACrC,CAAC;AAGD,QAAI,sBAAsB,CAAC;AAC3B,QAAI,eAAe;AACnB,QAAI,iBAAiB;AAErB,QAAI;AACF,4BACE,MAA+B;AAAA,QAC7B;AAAA,QACA;AAAA;AAAA,MACF;AAEF,iBAAW,SAAS,kCAAkC;AAAA,QACpD,cAAc,oBAAoB;AAAA,MACpC,CAAC;AAAA,IACH,SAAS,KAAK;AACZ,iBAAW,QAAQ,2CAA2C;AAAA,QAC5D,OAAO,IAAI;AAAA,QACX;AAAA,MACF,CAAC;AAAA,IAEH;AAIA,UAAM,mBAAmB;AAAA,MACvB,iBAAiB,CAAC;AAAA,MAClB,qBAAqB,oBAAoB,IAAI,CAAC,SAAS;AAAA,QACrD,MAAM;AAAA,QACN,SAAS,IAAI;AAAA,QACb,UAAU;AAAA,UACR,MAAM,IAAI;AAAA,UACV,WAAW,IAAI;AAAA,QACjB;AAAA,QACA,gBAAgB;AAAA,MAClB,EAAE;AAAA,MACF;AAAA,MACA;AAAA,MACA,eAAe;AAAA,MACf,SAAS;AAAA,QACP,eAAe,oBAAoB;AAAA,QACnC,oBAAoB;AAAA,QACpB,YAAY,oBAAoB;AAAA,UAC9B,CAAC,KAAK,QAAQ,MAAM,oBAAoB,IAAI,OAAO;AAAA,UACnD;AAAA,QACF;AAAA,MACF;AAAA,IACF;AAEA;AAAA,MACE;AAAA,MACA,qCAAqC,iBAAiB,oBAAoB,MAAM;AAAA,IAClF;AAEA,WAAO;AAAA,EACT,SAAS,OAAO;AACd,eAAW,SAAS,8CAA8C;AAAA,MAChE,OAAO,MAAM;AAAA,MACb,MAAM,MAAM;AAAA,IACd,CAAC;AAED,UAAM;AAAA,EACR;AACF;AA0dA,SAAS,oBAAoB,MAAM;AACjC,MAAI;AACF,QAAI,CAAC;AAAM,aAAO;AAElB,WAAO,KAAK,KAAK,KAAK,SAAS,CAAC;AAAA,EAClC,SAAS,OAAO;AACd,eAAW,QAAQ,gCAAgC;AAAA,MACjD,OAAO,MAAM;AAAA,MACb,YAAY,MAAM,UAAU;AAAA,IAC9B,CAAC;AAED,WAAO,OAAO,KAAK,KAAK,KAAK,SAAS,CAAC,IAAI;AAAA,EAC7C;AACF;AAmHA,IAAO,uCAAQ;AAAA,EACb,MAAM;AAAA,EACN,aACE;AAAA,EACF,aAAa;AAAA,EACb,cAAc;AAAA,EACd,SAAAC;AACF;;;AE/tBA;AAFA,SAAS,KAAAC,UAAS;AAClB,SAAS,MAAMC,gBAAc;;;ACH7B;;;ACGA;AACA;AACA,SAAS,MAAMC,gBAAc;AA6C7B,eAAsB,kBAAkB,QAAQ;AAC9C,MAAI;AAEF,UAAM,EAAE,SAAS,aAAa,UAAU,MAAM,gBAAgB,IAAI;AAGlE,QAAI,CAAC,WAAW,CAAC,aAAa;AAC5B,aAAO,EAAE,UAAU,CAAC,GAAG,YAAY,EAAE;AAAA,IACvC;AAEA,UAAM,gBAAgB,eAAe;AAGrC,QAAI,qBAAqB,iBAAiB;AAE1C,QAAI,CAAC,oBAAoB;AAEvB,YAAM,MAAM,MAAiC;AAAA,QAC3C;AAAA,QACA;AAAA,MACF;AACA,2BACE,MAAiC,0BAA0B,GAAG;AAAA,IAClE;AAGA,UAAM,mBAAsC,SAAS,aAAa;AAClE,UAAM,WAA8B,gBAAgB,gBAAgB;AACpE,UAAM,aAAgC,cAAc,kBAAkB,CAAC;AAGvE,UAAM,gBAAgB,MAAM,iBAAiB;AAAA,MAC3C;AAAA;AAAA,MACA,eAAe;AAAA;AAAA,IACjB,CAAC;AAED,QAAI,cAAc,WAAW,GAAG;AAC9B,aAAO,EAAE,UAAU,CAAC,GAAG,YAAY,EAAE;AAAA,IACvC;AAGA,UAAM,eAAe,MAAM,QAAQ;AAAA,MACjC,cAAc;AAAA,QAAI,CAAC,YACjB;AAAA,UACE;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,QACF;AAAA,MACF;AAAA,IACF;AAGA,UAAM,kBAAkB,aACrB,OAAO,CAAC,WAAW,OAAO,aAAa,GAAG,EAC1C,KAAK,CAAC,GAAG,MAAM,EAAE,aAAa,EAAE,UAAU;AAG7C,QAAI,oBAAoB;AACxB,QAAI,kBAAkB;AAEtB,QAAI,gBAAgB,SAAS,GAAG;AAC9B,iBAAW,SAAS,iBAAiB;AACnC,cAAM,aAAa,MAAM,QAAQ,cAAc;AAC/C,6BAAqB,MAAM,aAAa;AACxC,2BAAmB;AAAA,MACrB;AAEA,0BACE,kBAAkB,IACd,oBAAoB,kBACpB,gBAAgB,CAAC,EAAE;AAAA,IAC3B;AAGA,WAAO;AAAA,MACL,UAAU,gBAAgB,IAAI,CAAC,UAAU,MAAM,OAAO;AAAA,MACtD,YAAY;AAAA,IACd;AAAA,EACF,SAAS,OAAO;AACd,YAAQ,MAAM,iCAAiC,KAAK;AACpD,WAAO,EAAE,UAAU,CAAC,GAAG,YAAY,EAAE;AAAA,EACvC;AACF;AAWA,eAAsB,iBAAiB,gBAAgB,CAAC,GAAG;AACzD,MAAI;AACF,UAAM,EAAE,MAAM,eAAe,SAAS,IAAI;AAG1C,QAAI,QAAQ;AACZ,UAAM,SAAS,CAAC;AAGhB,QAAI,MAAM;AACR,eAAS;AACT,aAAO,KAAK,IAAI;AAAA,IAClB;AAGA,QAAI,kBAAkB,UAAa,CAAC,MAAM,aAAa,GAAG;AACxD,eAAS;AACT,aAAO,KAAK,aAAa;AAAA,IAC3B;AAGA,QAAI,UAAU;AACZ,eAAS;AACT,aAAO,KAAK,UAAU,KAAK;AAAA,IAC7B;AAGA,aAAS;AAGT,UAAM,WAAW,MAAM,aAAa,OAAO,MAAM;AAGjD,WAAO,SAAS,IAAI,CAAC,aAAa;AAAA,MAChC,GAAG;AAAA,MACH,iBAAiB,KAAK,MAAM,QAAQ,mBAAmB,IAAI;AAAA,IAC7D,EAAE;AAAA,EACJ,SAAS,OAAO;AACd,YAAQ,MAAM,2CAA2C,KAAK;AAC9D,UAAM,IAAI,MAAM,gCAAgC,MAAM,OAAO,EAAE;AAAA,EACjE;AACF;AA6CA,eAAe,aACb,SACA,SACA,oBACA,UACA,YACA,YACA;AACA,MAAI;AACF,UAAM,EAAE,gBAAgB,IAAI;AAC5B,QAAI,oBAAoB;AACxB,QAAI,uBAAuB;AAC3B,QAAI,iBAAiB;AAGrB,QACE,gBAAgB,oBAChB,MAAM,QAAQ,gBAAgB,gBAAgB,GAC9C;AACA,uBAAiB,gBAAgB,iBAAiB,SAAS,UAAU,IACjE,IACA;AAGJ,UAAI,mBAAmB,KAAK,gBAAgB,sBAAsB;AAChE,eAAO,EAAE,SAAS,YAAY,EAAE;AAAA,MAClC;AAAA,IACF,OAAO;AAEL,uBAAiB;AAAA,IACnB;AAGA,QAAI,gBAAgB,YAAY,MAAM,QAAQ,gBAAgB,QAAQ,GAAG;AACvE,YAAM,iBAAiB,gBAAgB,SAAS;AAAA,QAAO,CAAC,YACtD,SAAS,SAAS,OAAO;AAAA,MAC3B;AAEA,0BACE,eAAe,SAAS,gBAAgB,SAAS;AAAA,IACrD;AAGA,QACE,gBAAgB,iBAChB,MAAM,QAAQ,gBAAgB,aAAa,GAC3C;AACA,UAAI,oBAAoB;AAExB,iBAAW,eAAe,gBAAgB,eAAe;AACvD,YAAI,OAAO,gBAAgB,UAAU;AACnC,cAAI,QAAQ,SAAS,WAAW,GAAG;AACjC;AAAA,UACF;AAAA,QACF,WACE,uBAAuB,UACtB,OAAO,gBAAgB,YAAY,YAAY,SAChD;AAEA,gBAAMC,WACJ,uBAAuB,SACnB,cACA,IAAI,OAAO,YAAY,SAAS,YAAY,SAAS,EAAE;AAE7D,cAAIA,SAAQ,KAAK,OAAO,GAAG;AACzB;AAAA,UACF;AAAA,QACF;AAAA,MACF;AAEA,YAAM,mBACJ,gBAAgB,cAAc,SAAS,IACnC,oBAAoB,gBAAgB,cAAc,SAClD;AAGN,0BACE,oBAAoB,KACf,oBAAoB,oBAAoB,IACzC;AAAA,IACR;AAGA,QACE,gBAAgB,oBAChB,MAAM,QAAQ,gBAAgB,gBAAgB,GAC9C;AACA,UAAI,uBAAuB;AAE3B,iBAAW,QAAQ,gBAAgB,kBAAkB;AACnD,cAAM,EAAE,SAAS,WAAW,MAAM,IAAI;AAGtC,YAAI,CAAC,WAAW,CAAC,aAAa,UAAU;AAAW;AAGnD,cAAM,eAAe,mBAAmB,OAAO;AAG/C,YAAI,iBAAiB;AAAW;AAGhC,YAAI,UAAU;AAEd,gBAAQ,WAAW;AAAA,UACjB,KAAK;AACH,sBAAU,iBAAiB;AAC3B;AAAA,UACF,KAAK;AACH,sBAAU,MAAM,QAAQ,YAAY,IAChC,aAAa,SAAS,KAAK,IAC3B,OAAO,YAAY,EAAE,SAAS,OAAO,KAAK,CAAC;AAC/C;AAAA,UACF,KAAK;AACH,sBAAU,OAAO,YAAY,IAAI,OAAO,KAAK;AAC7C;AAAA,UACF,KAAK;AACH,sBAAU,OAAO,YAAY,IAAI,OAAO,KAAK;AAC7C;AAAA,UACF,KAAK;AACH,sBAAU,IAAI,OAAO,KAAK,EAAE,KAAK,OAAO,YAAY,CAAC;AACrD;AAAA,UACF;AACE,sBAAU;AAAA,QACd;AAEA,YAAI,SAAS;AACX;AAAA,QACF;AAAA,MACF;AAEA,6BACE,gBAAgB,iBAAiB,SAAS,IACtC,uBAAuB,gBAAgB,iBAAiB,SACxD;AAAA,IACR;AAGA,UAAM,UAAU,gBAAgB,WAAW;AAAA,MACzC,SAAS;AAAA,MACT,YAAY;AAAA,MACZ,MAAM;AAAA,IACR;AAGA,UAAM,aACJ,oBAAoB,QAAQ,UAC5B,uBAAuB,QAAQ,aAC/B,iBAAiB,QAAQ;AAE3B,WAAO,EAAE,SAAS,WAAW;AAAA,EAC/B,SAAS,OAAO;AACd,YAAQ,MAAM,0BAA0B,QAAQ,IAAI,KAAK,KAAK;AAC9D,WAAO,EAAE,SAAS,YAAY,EAAE;AAAA,EAClC;AACF;;;AD7XA,SAAS,MAAMC,gBAAc;AAyN7B,eAAsB,+BAA+B,qBAAqB;AACxE,MAAI;AACF,YAAQ;AAAA,MACN,yDAAyD,mBAAmB;AAAA,IAC9E;AAEA,UAAM,gBAAgB;AACtB,UAAM,YAAY,MAAM,aAAa,eAAe,CAAC,mBAAmB,CAAC;AACzE,QAAI,CAAC,aAAa,UAAU,WAAW,GAAG;AACxC,cAAQ;AAAA,QACN,4DAA4D,mBAAmB;AAAA,MACjF;AACA;AAAA,IACF;AACA,UAAM,WAAW,UAAU,CAAC;AAC5B,UAAM,EAAE,YAAY,aAAa,gBAAgB,IAAI;AACrD,UAAM,gBAAgB,IAAI,KAAK,UAAU,EAAE,QAAQ;AACnD,UAAM,iBAAiB,IAAI,KAAK,KAAK;AACrC,UAAM,gBAAgB,IAAI,KAAK,KAAK;AACpC,UAAM,cAAc,IAAI,KAAK,gBAAgB,cAAc,EAAE,YAAY;AACzE,UAAM,YAAY,IAAI,KAAK,gBAAgB,aAAa,EAAE,YAAY;AAGtE,UAAM,cAAc;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAOpB,UAAM,SAAS,MAAM,aAAa,aAAa;AAAA,MAC7C;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAGD,UAAM,eAAe;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAOrB,UAAM,WAAW,MAAM,aAAa,cAAc;AAAA,MAChD;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAID,UAAM,qBAAqB,CAAC;AAC5B,eAAW,SAAS,QAAQ;AAC1B,UAAI,MAAM,MAAM;AACd,YAAI;AACF,gBAAM,OACJ,OAAO,MAAM,SAAS,WAClB,KAAK,MAAM,MAAM,IAAI,IACrB,MAAM;AACZ,cAAI,KAAK,YAAY;AACnB,+BAAmB,KAAK,UAAU,KAC/B,mBAAmB,KAAK,UAAU,KAAK,KAAK;AAAA,UACjD;AACA,cAAI,KAAK,gBAAgB,MAAM,QAAQ,KAAK,YAAY,GAAG;AACzD,uBAAW,QAAQ,KAAK,cAAc;AACpC,iCAAmB,IAAI,KAAK,mBAAmB,IAAI,KAAK,KAAK;AAAA,YAC/D;AAAA,UACF;AAAA,QACF,SAAS,KAAK;AAAA,QAEd;AAAA,MACF;AAAA,IACF;AAEA,UAAM,gBAAgB,OACnB,OAAO,CAAC,MAAM,EAAE,SAAS,cAAc,EACvC,IAAI,CAAC,MAAM;AACV,UAAI;AACF,cAAM,OAAO,OAAO,EAAE,SAAS,WAAW,KAAK,MAAM,EAAE,IAAI,IAAI,EAAE;AACjE,eAAO,QAAQ,KAAK,QAAQ,KAAK,QAAQ;AAAA,MAC3C,QAAQ;AACN,eAAO;AAAA,MACT;AAAA,IACF,CAAC,EACA,OAAO,OAAO;AAGjB,UAAM,cAAc,CAAC;AACrB,UAAM,gBAAgB,CAAC;AACvB,eAAW,OAAO,UAAU;AAC1B,UAAI,IAAI,kBAAkB;AACxB,oBAAY,IAAI,gBAAgB,KAC7B,YAAY,IAAI,gBAAgB,KAAK,KAAK;AAAA,MAC/C;AACA,UAAI,IAAI,cAAc;AACpB,sBAAc,IAAI,YAAY,KAC3B,cAAc,IAAI,YAAY,KAAK,KAAK;AAAA,MAC7C;AAAA,IACF;AAGA,YAAQ;AAAA,MACN,8BAA8B,mBAAmB;AAAA,IACnD;AACA,YAAQ;AAAA,MACN;AAAA,MACA,OAAO,QAAQ,kBAAkB,EAC9B,KAAK,CAAC,GAAG,MAAM,EAAE,CAAC,IAAI,EAAE,CAAC,CAAC,EAC1B,MAAM,GAAG,CAAC;AAAA,IACf;AACA,YAAQ,IAAI,iCAAiC,cAAc,MAAM,GAAG,CAAC,CAAC;AACtE,YAAQ;AAAA,MACN;AAAA,MACA,OAAO,QAAQ,WAAW,EACvB,KAAK,CAAC,GAAG,MAAM,EAAE,CAAC,IAAI,EAAE,CAAC,CAAC,EAC1B,MAAM,GAAG,CAAC;AAAA,IACf;AACA,YAAQ;AAAA,MACN;AAAA,MACA,OAAO,QAAQ,aAAa,EACzB,KAAK,CAAC,GAAG,MAAM,EAAE,CAAC,IAAI,EAAE,CAAC,CAAC,EAC1B,MAAM,GAAG,CAAC;AAAA,IACf;AAMA,YAAQ;AAAA,MACN,8CAA8C,mBAAmB;AAAA,IACnE;AAAA,EACF,SAAS,OAAO;AACd,YAAQ;AAAA,MACN,8DAA8D,mBAAmB;AAAA,MACjF;AAAA,IACF;AAAA,EACF;AACF;AAmkBA,eAAsB,gCAAgC,gBAAgB;AACpE,MAAI;AACF,YAAQ;AAAA,MACN,0DAA0D,cAAc;AAAA,IAC1E;AAGA,UAAM,sBACJ,MAA+B,uBAAuB,cAAc;AACtE,QAAI,CAAC,uBAAuB,oBAAoB,WAAW,GAAG;AAC5D,cAAQ;AAAA,QACN,sDAAsD,cAAc;AAAA,MACtE;AACA,aAAO,CAAC;AAAA,IACV;AAGA,UAAM,gBAAgB,oBAAI,IAAI;AAC9B,eAAW,WAAW,qBAAqB;AACzC,UACE,QAAQ,8BACR,MAAM,QAAQ,QAAQ,0BAA0B,GAChD;AACA,gBAAQ,2BAA2B;AAAA,UAAQ,CAAC,OAC1C,cAAc,IAAI,EAAE;AAAA,QACtB;AAAA,MACF;AAAA,IACF;AAEA,QAAI,cAAc,SAAS,GAAG;AAC5B,cAAQ;AAAA,QACN,2DAA2D,cAAc;AAAA,MAC3E;AACA,aAAO,CAAC;AAAA,IACV;AAGA,UAAM,eAAe,CAAC;AACtB,eAAW,YAAY,eAAe;AACpC,YAAM,cAAc;AACpB,YAAM,gBAAgB,MAAM,aAAa,aAAa,CAAC,QAAQ,CAAC;AAEhE,UAAI,iBAAiB,cAAc,SAAS,GAAG;AAC7C,qBAAa,KAAK,cAAc,CAAC,CAAC;AAAA,MACpC;AAAA,IACF;AAGA,UAAM,uBAAuB,oBAAI,IAAI;AAErC,eAAW,UAAU,cAAc;AACjC,UAAI;AACF,cAAM,EAAE,UAAAC,UAAS,IACf,MAAqC,kBAAkB,MAAM;AAE/D,YAAIA,aAAYA,UAAS,SAAS,GAAG;AACnC,UAAAA,UAAS,QAAQ,CAAC,YAAY;AAC5B,gBAAI,QAAQ,YAAY;AACtB,mCAAqB,IAAI,QAAQ,UAAU;AAAA,YAC7C;AAAA,UACF,CAAC;AAAA,QACH;AAAA,MACF,SAAS,OAAO;AACd,gBAAQ;AAAA,UACN,yDAAyD,OAAO,EAAE;AAAA,UAClE;AAAA,QACF;AAAA,MACF;AAAA,IACF;AAGA,QAAI,qBAAqB,SAAS,GAAG;AACnC,cAAQ;AAAA,QACN,2DAA2D,cAAc;AAAA,MAC3E;AACA,aAAO,CAAC;AAAA,IACV;AAGA,UAAM,iBAAiB,MAAM,KAAK,oBAAoB;AAGtD,UAAM,eAAe,eAAe,IAAI,MAAM,GAAG,EAAE,KAAK,GAAG;AAE3D,UAAM,gBAAgB;AAAA;AAAA,6BAEG,YAAY;AAAA;AAAA;AAIrC,UAAM,WAAW,MAAM,aAAa,eAAe,cAAc;AAEjE,YAAQ;AAAA,MACN,0BAA0B,SAAS,MAAM,6BAA6B,cAAc;AAAA,IACtF;AAGA,WAAO,SAAS,IAAI,CAAC,aAAa;AAAA,MAChC,GAAG;AAAA,MACH,iBAAiB,QAAQ,kBACrB,KAAK,MAAM,QAAQ,eAAe,IAClC,CAAC;AAAA,MACL,WAAW,QAAQ,QAAQ,SAAS;AAAA,IACtC,EAAE;AAAA,EACJ,SAAS,OAAO;AACd,YAAQ;AAAA,MACN;AAAA,MACA;AAAA,IACF;AACA,WAAO,CAAC;AAAA,EACV;AACF;AAQA,eAAsB,mBAAmB,gBAAgB;AACvD,MAAI;AACF,YAAQ;AAAA,MACN,8DAA8D,cAAc;AAAA,IAC9E;AAGA,UAAM,WAAW,MAA+B;AAAA,MAC9C;AAAA,IACF;AACA,QAAI,CAAC,YAAY,SAAS,WAAW,GAAG;AACtC,aAAO,CAAC;AAAA,IACV;AAGA,UAAM,gBAAgB;AAAA,MACpB;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAEA,UAAM,cAAc;AAAA,MAClB;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAGA,UAAM,kBAAkB,CAAC;AACzB,UAAM,eAAe,CAAC;AAEtB,eAAW,WAAW,UAAU;AAC9B,YAAM,UAAU,QAAQ;AACxB,UAAI,CAAC;AAAS;AAGd,iBAAW,WAAW,eAAe;AACnC,cAAM,UAAU,QAAQ,MAAM,OAAO;AACrC,YAAI,WAAW,QAAQ,CAAC,GAAG;AACzB,0BAAgB,KAAK;AAAA,YACnB,aAAa,QAAQ,CAAC,EAAE,KAAK;AAAA,YAC7B,YAAY;AAAA,YACZ,WAAW,QAAQ;AAAA,YACnB,MAAM;AAAA,UACR,CAAC;AAAA,QACH;AAAA,MACF;AAGA,iBAAW,WAAW,aAAa;AACjC,cAAM,UAAU,QAAQ,MAAM,OAAO;AACrC,YAAI,WAAW,QAAQ,CAAC,GAAG;AACzB,uBAAa,KAAK;AAAA,YAChB,aAAa,QAAQ,CAAC,EAAE,KAAK;AAAA,YAC7B,YAAY;AAAA,YACZ,WAAW,QAAQ;AAAA,YACnB,MAAM;AAAA,UACR,CAAC;AAAA,QACH;AAAA,MACF;AAGA,YAAM,mBAAmB,QAAQ,MAAM,iBAAiB;AACxD,UAAI,kBAAkB;AACpB,mBAAW,aAAa,kBAAkB;AAExC,cAAI,4CAA4C,KAAK,SAAS,GAAG;AAC/D,4BAAgB,KAAK;AAAA,cACnB,aACE,UAAU,QAAQ,QAAQ,EAAE,EAAE,KAAK,EAAE,UAAU,GAAG,GAAG,IAAI;AAAA,cAC3D,YAAY;AAAA,cACZ,WAAW,QAAQ;AAAA,cACnB,MAAM;AAAA,YACR,CAAC;AAAA,UACH;AAAA,QACF;AAAA,MACF;AAAA,IACF;AAGA,UAAM,cAAc,CAAC;AAGrB,eAAW,OAAO,iBAAiB;AAEjC,YAAM,YAA+B,SAAS,IAAI,WAAW;AAC7D,YAAM,cAAiC,gBAAgB,SAAS;AAGhE,UAAI,eAAe;AACnB,UAAI,YAAY;AAEhB,iBAAW,YAAY,cAAc;AACnC,cAAM,iBAAoC;AAAA,UACxC,SAAS;AAAA,QACX;AACA,cAAM,mBACe,gBAAgB,cAAc;AAGnD,YAAI,aAAa;AACjB,mBAAW,cAAc,aAAa;AACpC,cAAI,iBAAiB,SAAS,UAAU,GAAG;AACzC;AAAA,UACF;AAAA,QACF;AAEA,YAAI,aAAa,WAAW;AAC1B,sBAAY;AACZ,yBAAe;AAAA,QACjB;AAAA,MACF;AAGA,kBAAY,KAAK;AAAA,QACf,aAAa,IAAI;AAAA,QACjB,YAAY,IAAI;AAAA,QAChB,UAAU,eAAe,aAAa,cAAc;AAAA,QACpD,eAAe,CAAC;AAAA;AAAA,MAClB,CAAC;AAAA,IACH;AAGA,eAAW,YAAY,cAAc;AAEnC,YAAM,cAAc,YAAY;AAAA,QAC9B,CAAC,OAAO,GAAG,aAAa,SAAS;AAAA,MACnC;AACA,UAAI,CAAC,aAAa;AAChB,oBAAY,KAAK;AAAA,UACf,aAAa,aAAa,SAAS,WAAW;AAAA,UAC9C,YAAY;AAAA,UACZ,eAAe,CAAC;AAAA,QAClB,CAAC;AAAA,MACH;AAAA,IACF;AAGA,UAAM,mBAAmB,oBAAI,IAAI;AACjC,UAAM,iBAAiB,CAAC;AAExB,eAAW,WAAW,aAAa;AACjC,UAAI,CAAC,iBAAiB,IAAI,QAAQ,WAAW,GAAG;AAC9C,yBAAiB,IAAI,QAAQ,WAAW;AACxC,uBAAe,KAAK,OAAO;AAAA,MAC7B;AAAA,IACF;AAEA,YAAQ;AAAA,MACN,8BAA8B,eAAe,MAAM,mCAAmC,cAAc;AAAA,IACtG;AACA,WAAO;AAAA,EACT,SAAS,OAAO;AACd,YAAQ,MAAM,mDAAmD,KAAK;AACtE,WAAO,CAAC;AAAA,EACV;AACF;AAyYA,eAAsB,qBAAqB,UAAU,gBAAgB;AACnE,MAAI;AACF,YAAQ;AAAA,MACN,iEAAiE,cAAc;AAAA,IACjF;AAEA,QAAI,CAAC,YAAY,SAAS,WAAW,GAAG;AACtC,cAAQ;AAAA,QACN;AAAA,MACF;AACA,aAAO,CAAC;AAAA,IACV;AAGA,UAAM,kBAAkB,SACrB,IAAI,CAAC,QAAQ,IAAI,WAAW,EAAE,EAC9B,OAAO,CAAC,YAAY,QAAQ,KAAK,EAAE,SAAS,CAAC;AAEhD,QAAI,gBAAgB,WAAW,GAAG;AAChC,aAAO,CAAC;AAAA,IACV;AAIA,UAAM,iBAAiB,CAAC;AAExB,eAAW,WAAW,iBAAiB;AAGrC,YAAM,eAAe;AACrB,UAAI;AAEJ,cAAQ,QAAQ,aAAa,KAAK,OAAO,OAAO,MAAM;AACpD,cAAM,MAAM,MAAM,CAAC,EAAE,KAAK;AAC1B,cAAM,QAAQ,MAAM,CAAC,EAAE,KAAK;AAE5B,YAAI,OAAO,SAAS,IAAI,SAAS,OAAO,CAAC,IAAI,SAAS,IAAI,GAAG;AAC3D,yBAAe,KAAK;AAAA,YAClB;AAAA,YACA;AAAA,YACA,YAAY;AAAA,UACd,CAAC;AAAA,QACH;AAAA,MACF;AAGA,YAAM,YAAY;AAElB,cAAQ,QAAQ,UAAU,KAAK,OAAO,OAAO,MAAM;AACjD,cAAM,MAAM,MAAM,CAAC,EAAE,KAAK;AAC1B,cAAM,QAAQ,MAAM,CAAC,EAAE,KAAK;AAE5B,YAAI,OAAO,SAAS,IAAI,SAAS,MAAM,CAAC,IAAI,SAAS,IAAI,GAAG;AAC1D,yBAAe,KAAK;AAAA,YAClB;AAAA,YACA;AAAA,YACA,YAAY;AAAA,UACd,CAAC;AAAA,QACH;AAAA,MACF;AAAA,IACF;AAGA,UAAM,SAAS,oBAAI,IAAI;AAEvB,eAAW,QAAQ,gBAAgB;AACjC,YAAM,eAAe,OAAO,IAAI,KAAK,IAAI,YAAY,CAAC;AAEtD,UAAI,CAAC,gBAAgB,aAAa,aAAa,KAAK,YAAY;AAC9D,eAAO,IAAI,KAAK,IAAI,YAAY,GAAG,IAAI;AAAA,MACzC;AAAA,IACF;AAGA,WAAO,MAAM,KAAK,OAAO,OAAO,CAAC;AAAA,EACnC,SAAS,OAAO;AACd,YAAQ,MAAM,sDAAsD,KAAK;AACzE,WAAO,CAAC;AAAA,EACV;AACF;AAeA,eAAsB,aAAa,SAAS;AAC1C,MAAI;AACF,YAAQ,IAAI,qCAAqC,QAAQ,IAAI,EAAE;AAE/D,QACE,CAAC,WACD,CAAC,QAAQ,QACT,CAAC,QAAQ,eACT,CAAC,QAAQ,gBACT;AACA,YAAM,IAAI,MAAM,0CAA0C;AAAA,IAC5D;AAEA,UAAM,YAAY,QAAQ,MAAMC,SAAO;AACvC,UAAM,OAAM,oBAAI,KAAK,GAAE,YAAY;AACnC,UAAM,aAAa,QAAQ,cAAc;AAGzC,UAAM,QAAQ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAad,UAAM,aAAa,OAAO;AAAA,MACxB;AAAA,MACA,QAAQ,YAAY;AAAA,MACpB,QAAQ;AAAA,MACR,QAAQ;AAAA,MACR,QAAQ;AAAA,MACR,QAAQ,YAAY;AAAA,MACpB;AAAA,MACA;AAAA,MACA;AAAA,MACA,QAAQ,kBAAkB;AAAA,IAC5B,CAAC;AAED,WAAO;AAAA,MACL,IAAI;AAAA,MACJ,GAAG;AAAA,MACH,YAAY;AAAA,MACZ,YAAY;AAAA,IACd;AAAA,EACF,SAAS,OAAO;AACd,YAAQ,MAAM,2CAA2C,KAAK;AAC9D,UAAM,IAAI,MAAM,4BAA4B,MAAM,OAAO,EAAE;AAAA,EAC7D;AACF;AAeA,eAAsB,gBAAgB,YAAY;AAChD,MAAI;AACF,YAAQ,IAAI,yCAAyC,WAAW,IAAI,EAAE;AAEtE,QAAI,CAAC,cAAc,CAAC,WAAW,QAAQ,CAAC,WAAW,aAAa;AAC9D,YAAM,IAAI,MAAM,8CAA8C;AAAA,IAChE;AAEA,UAAM,YAAY,WAAW,MAAMA,SAAO;AAC1C,UAAM,OAAM,oBAAI,KAAK,GAAE,YAAY;AACnC,UAAM,aAAa,WAAW,cAAc;AAG5C,QAAI,iBAAiB,WAAW;AAChC,QAAI,WAAW,YAAY,OAAO,mBAAmB,UAAU;AAC7D,uBAAiB;AAAA,QACf,GAAG,KAAK;AAAA,UACN,OAAO,mBAAmB,WACtB,iBACA,KAAK,UAAU,cAAc;AAAA,QACnC;AAAA,QACA,UAAU,WAAW;AAAA,MACvB;AACA,uBAAiB,KAAK,UAAU,cAAc;AAAA,IAChD,WAAW,WAAW,YAAY,OAAO,mBAAmB,UAAU;AACpE,UAAI;AACF,cAAM,SAAS,KAAK,MAAM,cAAc;AACxC,eAAO,WAAW,WAAW;AAC7B,yBAAiB,KAAK,UAAU,MAAM;AAAA,MACxC,SAAS,GAAG;AAAA,MAEZ;AAAA,IACF;AAGA,UAAM,QAAQ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAad,UAAM,aAAa,OAAO;AAAA,MACxB;AAAA,MACA;AAAA,MACA,WAAW;AAAA,MACX,WAAW;AAAA,MACX;AAAA,MACA,WAAW,YAAY;AAAA,MACvB;AAAA,MACA;AAAA,MACA;AAAA,MACA,WAAW,kBAAkB;AAAA,IAC/B,CAAC;AAED,WAAO;AAAA,MACL,IAAI;AAAA,MACJ,GAAG;AAAA,MACH,YAAY;AAAA,MACZ,YAAY;AAAA,MACZ,cAAc;AAAA,IAChB;AAAA,EACF,SAAS,OAAO;AACd,YAAQ,MAAM,+CAA+C,KAAK;AAClE,UAAM,IAAI,MAAM,gCAAgC,MAAM,OAAO,EAAE;AAAA,EACjE;AACF;AAaA,eAAsB,kBAAkB,cAAc;AACpD,MAAI;AACF,YAAQ;AAAA,MACN,sDAAsD,aAAa,GAAG;AAAA,IACxE;AAEA,QAAI,CAAC,gBAAgB,CAAC,aAAa,OAAO,CAAC,aAAa,OAAO;AAC7D,YAAM,IAAI,MAAM,iDAAiD;AAAA,IACnE;AAEA,UAAM,cAAc,aAAa,MAAMA,SAAO;AAC9C,UAAM,OAAM,oBAAI,KAAK,GAAE,YAAY;AACnC,UAAM,aAAa,aAAa,cAAc;AAC9C,UAAM,WAAW,aAAa,YAAY;AAG1C,UAAM,QAAQ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAad,UAAM,WAAW,KAAK,UAAU;AAAA,MAC9B;AAAA,MACA,QAAQ,aAAa,iBAAiB,iBAAiB;AAAA,MACvD,gBAAgB,aAAa,kBAAkB;AAAA,IACjD,CAAC;AAED,UAAM,aAAa,OAAO;AAAA,MACxB;AAAA,MACA;AAAA,MACA,aAAa;AAAA,MACb,aAAa;AAAA,MACb;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA,aAAa,kBAAkB;AAAA,IACjC,CAAC;AAED,WAAO;AAAA,MACL,IAAI;AAAA,MACJ,GAAG;AAAA,MACH,YAAY;AAAA,MACZ,YAAY;AAAA,MACZ,WAAW;AAAA,IACb;AAAA,EACF,SAAS,OAAO;AACd,YAAQ,MAAM,kDAAkD,KAAK;AACrE,UAAM,IAAI,MAAM,mCAAmC,MAAM,OAAO,EAAE;AAAA,EACpE;AACF;;;ADx3DA;AAEA;AAcA,eAAeC,SAAQ,OAAO,YAAY;AACxC,MAAI;AACF,eAAW,QAAQ,yCAAyC;AAAA,MAC1D,eAAe,MAAM;AAAA,MACrB,UAAU,MAAM,qBAAqB;AAAA,MACrC,gBAAgB,MAAM;AAAA,IACxB,CAAC;AAGD,UAAM;AAAA,MACJ;AAAA,MACA;AAAA,MACA,cAAc;AAAA,MACd,aAAa,CAAC;AAAA,MACd,oBAAoB;AAAA,MACpB,eAAe;AAAA,IACjB,IAAI;AAGJ,QAAI,CAAC,MAAM;AACT,YAAM,QAAQ,IAAI,MAAM,4BAA4B;AACpD,YAAM,OAAO;AACb,YAAM;AAAA,IACR;AAGA,QAAI,wBAAwB,CAAC;AAC7B,QAAIC,eAAc;AAClB,QAAI,mBAAmB,CAAC;AAExB,QAAI;AACF,8BACE,MAA2B,2BAA2B;AACxD,MAAAA,eAAc,MAA2B,eAAe;AACxD,yBAAmB,sBAAsB,IAAI,CAAC,WAAW,OAAO,EAAE;AAElE,iBAAW,SAAS,4BAA4B;AAAA,QAC9C,aAAa,iBAAiB;AAAA,QAC9B,UAAU,CAAC,CAACA;AAAA,MACd,CAAC;AAAA,IACH,SAAS,YAAY;AACnB;AAAA,QACE;AAAA,QACA;AAAA,QACA;AAAA,UACE,OAAO,WAAW;AAAA,QACpB;AAAA,MACF;AAAA,IAEF;AAGA,UAAM,eAAe;AAAA,MACnB;AAAA,MACA;AAAA,MACA;AAAA,MACA,aAAAA;AAAA,MACA,WAAW;AAAA,MACX;AAAA,MACA,WAAW,KAAK,IAAI;AAAA,MACpB;AAAA,IACF;AAGA,QAAI;AACJ,QAAI;AACF,yBAAmB,MAA2B;AAAA,QAC5C;AAAA,QACA;AAAA,UACE;AAAA,UACA,UAAU;AAAA,UACV,aAAa,iBAAiB;AAAA,UAC9B,WAAW,KAAK,IAAI;AAAA,QACtB;AAAA,QACA;AAAA,QACA;AAAA,MACF;AACA,iBAAW,SAAS,wCAAwC;AAAA,QAC1D,SAAS;AAAA,MACX,CAAC;AAAA,IACH,SAAS,aAAa;AACpB,iBAAW,SAAS,gDAAgD;AAAA,QAClE,OAAO,YAAY;AAAA,QACnB;AAAA,QACA,UAAU;AAAA,MACZ,CAAC;AAED,YAAM;AAAA,IACR;AAGA,QAAI;AACJ,QAAI;AACF,oBAAc,MAA2B;AAAA,QACvC;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MACF;AACA,iBAAW,QAAQ,8BAA8B,WAAW,EAAE;AAAA,IAChE,SAAS,aAAa;AACpB,iBAAW,SAAS,uCAAuC;AAAA,QACzD,OAAO,YAAY;AAAA,QACnB;AAAA,QACA,SAAS;AAAA,MACX,CAAC;AAED,YAAM;AAAA,IACR;AAGA,QAAI,mBAAmB;AAGvB,QAAI,cAAc;AAChB,UAAI;AACF,mBAAW,QAAQ,4CAA4C;AAAA,UAC7D;AAAA,UACA,UAAU;AAAA,QACZ,CAAC;AACD,2BAAmB,MAAM;AAAA,UACvB;AAAA,UACA;AAAA,UACA;AAAA,QACF;AAAA,MACF,SAAS,WAAW;AAClB,mBAAW,QAAQ,qCAAqC;AAAA,UACtD,OAAO,UAAU;AAAA,UACjB;AAAA,QACF,CAAC;AAED,2BAAmB;AAAA,UACjB,aAAa;AAAA,UACb,aAAa;AAAA,UACb,eAAe,4BAA4B,UAAU,OAAO;AAAA,UAC5D,OAAO,UAAU;AAAA,UACjB,cAAc;AAAA,YACZ,0BAA0B,iBAAiB;AAAA,YAC3C,6BAA6B;AAAA,YAC7B,oBAAoB;AAAA,YACpB,oBAAoB;AAAA,UACtB;AAAA,QACF;AAAA,MACF;AAAA,IACF,OAAO;AACL,iBAAW,SAAS,4CAA4C;AAAA,IAClE;AAGA,eAAW,MAAM;AACf;AAAA,QACE;AAAA,QACA,uDAAuD,WAAW;AAAA,MACpE;AACA,MAAe,+BAA+B,WAAW,EAAE;AAAA,QACzD,CAAC,UAAU;AACT,qBAAW,SAAS,wCAAwC;AAAA,YAC1D,OAAO,MAAM;AAAA,YACb;AAAA,UACF,CAAC;AAAA,QACH;AAAA,MACF;AAAA,IACF,GAAG,GAAG;AAGN,eAAW,QAAQ,wDAAwD;AAAA,MACzE;AAAA,MACA,aAAa,iBAAiB;AAAA,MAC9B,qBAAqB,CAAC,CAAC;AAAA,IACzB,CAAC;AAED,UAAM,eAAe;AAAA,MACnB,SAAS,cAAc,IAAI,gCAAgC,iBAAiB,MAAM;AAAA,MAClF;AAAA,MACA,QAAQ;AAAA,MACR;AAAA,MACA,sBAAsB,iBAAiB;AAAA,MACvC;AAAA,IACF;AAEA,WAAO;AAAA,MACL,SAAS;AAAA,QACP;AAAA,UACE,MAAM;AAAA,UACN,MAAM,KAAK,UAAU,YAAY;AAAA,QACnC;AAAA,MACF;AAAA,IACF;AAAA,EACF,SAAS,OAAO;AAEd,eAAW,SAAS,0CAA0C;AAAA,MAC5D,OAAO,MAAM;AAAA,MACb,OAAO,MAAM;AAAA,MACb,OAAO;AAAA,QACL,MAAM,MAAM;AAAA,QACZ,UAAU,MAAM;AAAA,QAChB,gBAAgB,MAAM;AAAA,MACxB;AAAA,IACF,CAAC;AAGD,UAAM,gBAAgB;AAAA,MACpB,OAAO;AAAA,MACP,WAAW,MAAM,QAAQ;AAAA,MACzB,cAAc,MAAM;AAAA,MACpB,aAAa;AAAA,MACb,QAAQ;AAAA,MACR,mBAAmB,MAAM,qBAAqB;AAAA,MAC9C,sBAAsB;AAAA,MACtB,kBAAkB;AAAA,QAChB,OAAO,MAAM;AAAA,MACf;AAAA,IACF;AAEA,WAAO;AAAA,MACL,SAAS;AAAA,QACP;AAAA,UACE,MAAM;AAAA,UACN,MAAM,KAAK,UAAU,aAAa;AAAA,QACpC;AAAA,MACF;AAAA,IACF;AAAA,EACF;AACF;AAWA,eAAe,uBAAuB,aAAa,UAAU,kBAAkB;AAC7E,MAAI;AACF,eAAW,SAAS,mCAAmC,WAAW,IAAI;AAAA,MACpE;AAAA,MACA,aAAa,kBAAkB,UAAU;AAAA,IAC3C,CAAC;AAGD,QAAI,CAAC,oBAAoB,iBAAiB,WAAW,GAAG;AACtD;AAAA,QACE;AAAA,QACA;AAAA,MACF;AACA,aAAO;AAAA,QACL,aAAa;AAAA,QACb,aAAa;AAAA,QACb,eAAe;AAAA,QACf,cAAc;AAAA,UACZ,0BAA0B;AAAA,UAC1B,6BAA6B;AAAA,UAC7B,oBAAoB;AAAA,UACpB,oBAAoB;AAAA,QACtB;AAAA,MACF;AAAA,IACF;AAGA,QAAI,WAAW,CAAC;AAChB,QAAI;AACF;AAAA,QACE;AAAA,QACA,wBAAwB,iBAAiB,MAAM;AAAA,MACjD;AAEA,YAAM,gBAAgB,MAAM,QAAQ;AAAA,QAClC,iBAAiB,IAAI,OAAO,OAAO;AACjC,cAAI;AAEF,kBAAM,QAAQ;AACd,kBAAM,SAAS,MAAM,aAAa,OAAO,CAAC,EAAE,CAAC;AAC7C,mBAAO,OAAO,SAAS,IAAI,OAAO,CAAC,IAAI;AAAA,UACzC,SAAS,UAAU;AACjB,uBAAW,QAAQ,kCAAkC;AAAA,cACnD,OAAO,SAAS;AAAA,cAChB,UAAU;AAAA,YACZ,CAAC;AACD,mBAAO;AAAA,UACT;AAAA,QACF,CAAC;AAAA,MACH;AAEA,iBAAW,cAAc,OAAO,OAAO;AACvC;AAAA,QACE;AAAA,QACA,yBAAyB,SAAS,MAAM,IAAI,iBAAiB,MAAM;AAAA,MACrE;AAAA,IACF,SAAS,UAAU;AACjB,iBAAW,SAAS,kCAAkC;AAAA,QACpD,OAAO,SAAS;AAAA,MAClB,CAAC;AAED,aAAO;AAAA,QACL,aAAa;AAAA,QACb,aAAa;AAAA,QACb,eAAe,6DAA6D,SAAS,OAAO;AAAA,QAC5F,cAAc;AAAA,UACZ,0BAA0B,iBAAiB;AAAA,UAC3C,6BAA6B;AAAA,UAC7B,oBAAoB;AAAA,UACpB,oBAAoB;AAAA,QACtB;AAAA,QACA,OAAO,SAAS;AAAA,MAClB;AAAA,IACF;AAGA,UAAM,oBAAoB,IAAI,IAAI,gBAAgB;AAClD,UAAM,gBAAgB,CAAC;AACvB,UAAM,mBAAmB,oBAAI,IAAI;AAGjC,UAAM,gBAAgB,oBAAI,IAAI;AAC9B,aAAS,QAAQ,CAAC,WAAW;AAC3B,oBAAc,IAAI,OAAO,WAAW,OAAO,WAAW;AAAA,IACxD,CAAC;AAGD,QAAI;AACF,iBAAW,UAAU,UAAU;AAE7B,cAAM,wBACJ,MAAsC;AAAA,UACpC,OAAO;AAAA,UACP;AAAA,QACF;AAEF;AAAA,UACE;AAAA,UACA,aAAa,sBAAsB,MAAM;AAAA,UACzC;AAAA,YACE,UAAU,OAAO;AAAA,YACjB,YAAY,OAAO;AAAA,UACrB;AAAA,QACF;AAGA,mBAAW,OAAO,uBAAuB;AAEvC,cAAI,CAAC,kBAAkB,IAAI,IAAI,gBAAgB,GAAG;AAChD,8BAAkB,IAAI,IAAI,gBAAgB;AAG1C,gBACE,IAAI,sBAAsB,WAC1B,IAAI,sBAAsB,aAC1B,IAAI,sBAAsB,cAC1B;AACA,4BAAc,KAAK;AAAA,gBACjB,QAAQ,OAAO;AAAA,gBACf,QAAQ,IAAI;AAAA,gBACZ,MAAM,IAAI;AAAA,gBACV,aAAa;AAAA;AAAA,cACf,CAAC;AAAA,YACH;AAAA,UACF;AAAA,QACF;AAIA,cAAM,WAAW,OAAO,aAAa;AACrC,cAAM,YAAY,SAAS,MAAM,GAAG,EAAE,MAAM,GAAG,CAAC,EAAE,KAAK,GAAG;AAC1D,YAAI,WAAW;AACb,gBAAM,eAAe,iBAAiB,IAAI,SAAS,KAAK;AACxD,2BAAiB,IAAI,WAAW,eAAe,CAAC;AAAA,QAClD;AAAA,MACF;AAAA,IACF,SAAS,QAAQ;AACf,iBAAW,QAAQ,iCAAiC;AAAA,QAClD,OAAO,OAAO;AAAA,QACd;AAAA,MACF,CAAC;AAAA,IAEH;AAEA,eAAW,SAAS,mCAAmC;AAAA,MACrD,kBAAkB,kBAAkB;AAAA,MACpC,eAAe,cAAc;AAAA,MAC7B,gBAAgB,iBAAiB;AAAA,IACnC,CAAC;AAGD,UAAM,wBAAwB,iBAAiB;AAC/C,UAAM,2BACJ,kBAAkB,OAAO;AAC3B,UAAM,0BAA0B,iBAAiB;AACjD,UAAM,qBAAqB,cAAc;AAGzC,QAAI;AACJ,QAAI;AAEJ,QAAI;AAEF,YAAM,kBAAkB,KAAK;AAAA,QAC3B;AAAA,QACA,wBAAwB,OACtB,2BAA2B,OAC3B,0BAA0B,MAC1B,qBAAqB;AAAA,MACzB;AAGA,UAAI,qBAAqB;AACzB,cAAQ,UAAU;AAAA,QAChB,KAAK;AACH,+BAAqB;AACrB;AAAA,QACF,KAAK;AACH,+BAAqB;AACrB;AAAA,QACF,KAAK;AACH,+BAAqB;AACrB;AAAA,QACF,KAAK;AACH,+BAAqB;AACrB;AAAA,QACF;AACE,+BAAqB;AAAA,MACzB;AAEA,oBAAc,KAAK,IAAI,GAAG,kBAAkB,kBAAkB;AAG9D,UAAI,cAAc,KAAK;AACrB,sBAAc;AAAA,MAChB,WAAW,cAAc,KAAK;AAC5B,sBAAc;AAAA,MAChB,WAAW,cAAc,KAAK;AAC5B,sBAAc;AAAA,MAChB,OAAO;AACL,sBAAc;AAAA,MAChB;AAEA,iBAAW,QAAQ,gCAAgC;AAAA,QACjD;AAAA,QACA;AAAA,QACA,kBAAkB;AAAA,QAClB,qBAAqB;AAAA,QACrB,YAAY;AAAA,MACd,CAAC;AAAA,IACH,SAAS,SAAS;AAChB,iBAAW,SAAS,kCAAkC;AAAA,QACpD,OAAO,QAAQ;AAAA,MACjB,CAAC;AAED,oBAAc;AACd,oBAAc;AAAA,IAChB;AAGA,QAAI;AACJ,QAAI;AACF,sBAAgB;AAAA,QACd;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MACF;AAAA,IACF,SAAS,YAAY;AACnB,iBAAW,QAAQ,mCAAmC;AAAA,QACpD,OAAO,WAAW;AAAA,MACpB,CAAC;AAED,sBAAgB,wBAAwB,WAAW,sBAAsB,qBAAqB,gDAAgD,wBAAwB;AAAA,IACxK;AAGA,WAAO;AAAA,MACL;AAAA,MACA;AAAA,MACA;AAAA,MACA,cAAc;AAAA,QACZ,0BAA0B;AAAA,QAC1B,6BAA6B;AAAA,QAC7B,oBAAoB;AAAA,QACpB;AAAA,MACF;AAAA,MACA,oBAAoB,OAAO,YAAY,gBAAgB;AAAA,MACvD,kBAAkB,cAAc,MAAM,GAAG,CAAC;AAAA;AAAA,IAC5C;AAAA,EACF,SAAS,OAAO;AACd,eAAW,SAAS,8BAA8B;AAAA,MAChD,OAAO,MAAM;AAAA,MACb,OAAO,MAAM;AAAA,MACb;AAAA,MACA;AAAA,IACF,CAAC;AAGD,WAAO;AAAA,MACL,aAAa;AAAA,MACb,aAAa;AAAA,MACb,eAAe,2CAA2C,MAAM,OAAO;AAAA,MACvE,OAAO,MAAM;AAAA,MACb,cAAc;AAAA,QACZ,0BAA0B,mBACtB,iBAAiB,SACjB;AAAA,QACJ,6BAA6B;AAAA,QAC7B,oBAAoB;AAAA,QACpB,oBAAoB;AAAA,MACtB;AAAA,IACF;AAAA,EACF;AACF;AAcA,SAAS,uBACP,aACA,aACA,eACA,gBACA,mBACA,UACA;AACA,MAAI;AAEF,QAAI,UAAU,QAAQ,QAAQ,oBAAoB,WAAW;AAG7D,eAAW,sBAAsB,WAAW,uCAAuC,aAAa;AAGhG,QAAI,iBAAiB,GAAG;AACtB,iBAAW,gBAAgB,cAAc,aACvC,mBAAmB,IAAI,KAAK,GAC9B;AAAA,IACF;AAGA,QAAI,oBAAoB,GAAG;AACzB,iBAAW,SAAS,iBAAiB,4BACnC,sBAAsB,IAAI,KAAK,GACjC;AAAA,IACF;AAGA,YAAQ,UAAU;AAAA,MAChB,KAAK;AACH,mBACE;AACF;AAAA,MACF,KAAK;AACH,mBACE;AACF;AAAA,MACF,KAAK;AACH,mBACE;AACF;AAAA,MACF,KAAK;AACH,mBACE;AACF;AAAA,IACJ;AAEA,WAAO;AAAA,EACT,SAAS,OAAO;AACd,eAAW,QAAQ,wCAAwC;AAAA,MACzD,OAAO,MAAM;AAAA,IACf,CAAC;AAED,WAAO,wBAAwB,WAAW,sBAAsB,WAAW;AAAA,EAC7E;AACF;AAGA,IAAO,sCAAQ;AAAA,EACb,MAAM;AAAA,EACN,aACE;AAAA,EACF,aAAa;AAAA,EACb,cAAc;AAAA,EACd,SAAAD;AACF;;;AGpmBA;AADA,SAAS,KAAAE,UAAS;AAUlB;AAcA,eAAeC,SAAQ,OAAO,YAAY;AACxC,MAAI;AACF,eAAW,QAAQ,8CAA8C;AAAA,MAC/D,gBAAgB,MAAM;AAAA,MACtB,SAAS,MAAM,WAAW;AAAA,MAC1B,oBAAoB,MAAM,sBAAsB;AAAA,IAClD,CAAC;AAGD,UAAM;AAAA,MACJ;AAAA,MACA,oBAAAC,sBAAqB;AAAA,MACrB,mBAAmB;AAAA,MACnB,kBAAkB;AAAA,MAClB,0BAA0B;AAAA,MAC1B,oBAAoB;AAAA,MACpB,UAAU;AAAA,IACZ,IAAI;AAGJ,QAAI,CAAC,gBAAgB;AACnB,YAAM,QAAQ,IAAI,MAAM,6BAA6B;AACrD,YAAM,OAAO;AACb,YAAM;AAAA,IACR;AAEA,eAAW,SAAS,sBAAsB;AAAA,MACxC;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAGD,QAAI,sBAAsB,CAAC;AAC3B,QAAI,sBAAsB;AAC1B,QAAI,qBAAqB,CAAC;AAE1B,QAAI;AACF,4BACE,MAA+B,uBAAuB,cAAc;AAEtE,UAAI,CAAC,uBAAuB,oBAAoB,WAAW,GAAG;AAC5D,cAAM,QAAQ,IAAI;AAAA,UAChB,yCAAyC,cAAc;AAAA,QACzD;AACA,cAAM,OAAO;AACb,cAAM;AAAA,MACR;AAEA,iBAAW,SAAS,kCAAkC;AAAA,QACpD,cAAc,oBAAoB;AAAA,MACpC,CAAC;AAAA,IACH,SAAS,YAAY;AACnB,iBAAW,SAAS,2CAA2C;AAAA,QAC7D,OAAO,WAAW;AAAA,QAClB;AAAA,MACF,CAAC;AACD,YAAM;AAAA,IACR;AAGA,QAAI;AACF,4BACE,MAA+B,uBAAuB,cAAc;AACtE;AAAA,QACE;AAAA,QACA,mCAAmC,uBAAuB,SAAS;AAAA,MACrE;AAAA,IACF,SAAS,YAAY;AACnB,iBAAW,QAAQ,2CAA2C;AAAA,QAC5D,OAAO,WAAW;AAAA,QAClB;AAAA,MACF,CAAC;AAAA,IAEH;AAGA,QAAI;AACF,2BAAqB,MAA+B;AAAA,QAClD;AAAA,MACF;AACA;AAAA,QACE;AAAA,QACA,aAAa,mBAAmB,MAAM;AAAA,MACxC;AAAA,IACF,SAAS,WAAW;AAClB,iBAAW,QAAQ,0CAA0C;AAAA,QAC3D,OAAO,UAAU;AAAA,QACjB;AAAA,MACF,CAAC;AAED,2BAAqB,CAAC;AAAA,IACxB;AAGA,QAAI,UAAU;AACd,QAAI;AACF,gBAAU,MAA+B;AAAA,QACvC;AAAA,MACF;AACA,iBAAW,QAAQ,kCAAkC;AAAA,QACnD,eAAe,QAAQ;AAAA,MACzB,CAAC;AAAA,IACH,SAAS,YAAY;AACnB,iBAAW,QAAQ,2CAA2C;AAAA,QAC5D,OAAO,WAAW;AAAA,QAClB;AAAA,MACF,CAAC;AAED,gBAAU,gBAAgB,cAAc,SAAS,oBAAoB,MAAM;AAAA,IAC7E;AAGA,QAAI;AACF,YAA2B;AAAA,QACzB;AAAA,QACA;AAAA,UACE;AAAA,UACA,SAAS;AAAA,UACT,QAAQ,mBAAmB;AAAA,UAC3B;AAAA,QACF;AAAA,QACA,CAAC;AAAA;AAAA,QACD;AAAA,MACF;AACA,iBAAW,SAAS,6CAA6C;AAAA,IACnE,SAAS,aAAa;AACpB,iBAAW,QAAQ,2CAA2C;AAAA,QAC5D,OAAO,YAAY;AAAA,QACnB;AAAA,MACF,CAAC;AAAA,IAEH;AAGA,QAAI,qBAAqB;AACzB,QAAI,mBAAmB;AACvB,QAAI,uBAAuB;AAC3B,QAAI,YAAY;AAGhB,QAAI,kBAAkB;AACpB,UAAI;AACF,mBAAW,QAAQ,wCAAwC;AAC3D,6BAAqB,MAAM;AAAA,UACzB;AAAA,UACA;AAAA,QACF;AACA;AAAA,UACE;AAAA,UACA,aACE,oBAAoB,UAAU,UAAU,CAC1C,iBACE,oBAAoB,aAAa,UAAU,CAC7C;AAAA,QACF;AAAA,MACF,SAAS,aAAa;AACpB,mBAAW,QAAQ,+BAA+B;AAAA,UAChD,OAAO,YAAY;AAAA,UACnB;AAAA,QACF,CAAC;AAED,6BAAqB;AAAA,UACnB,UAAU,CAAC;AAAA,UACX,aAAa,CAAC;AAAA,UACd,oBAAoB,CAAC;AAAA,UACrB,OAAO,YAAY;AAAA,QACrB;AAAA,MACF;AAAA,IACF,OAAO;AACL,iBAAW,SAAS,8CAA8C;AAAA,IACpE;AAGA,QAAI,iBAAiB;AACnB,UAAI;AACF,mBAAW,QAAQ,sCAAsC;AACzD,2BAAmB,MAAM;AAAA,UACvB;AAAA,UACA;AAAA,QACF;AACA,mBAAW,QAAQ,YAAY,kBAAkB,SAAS,CAAC,WAAW;AAAA,MACxE,SAAS,YAAY;AACnB,mBAAW,QAAQ,8BAA8B;AAAA,UAC/C,OAAO,WAAW;AAAA,UAClB;AAAA,QACF,CAAC;AAED,2BAAmB;AAAA,UACjB,OAAO;AAAA,UACP,UAAU,CAAC;AAAA,UACX,OAAO,WAAW;AAAA,QACpB;AAAA,MACF;AAAA,IACF,OAAO;AACL,iBAAW,SAAS,4CAA4C;AAAA,IAClE;AAGA,QAAI,yBAAyB;AAC3B,UAAI;AACF,mBAAW,QAAQ,gDAAgD;AACnE,+BAAuB,MAAM;AAAA,UAC3B;AAAA,UACA;AAAA,UACA;AAAA,QACF;AACA;AAAA,UACE;AAAA,UACA,SACE,sBAAsB,eAAe,UAAU,CACjD;AAAA,QACF;AAAA,MACF,SAAS,YAAY;AACnB,mBAAW,QAAQ,8CAA8C;AAAA,UAC/D,OAAO,WAAW;AAAA,UAClB;AAAA,QACF,CAAC;AAED,+BAAuB;AAAA,UACrB,eAAe,CAAC;AAAA,UAChB,UAAU,CAAC;AAAA,UACX,OAAO,WAAW;AAAA,QACpB;AAAA,MACF;AAAA,IACF,OAAO;AACL,iBAAW,SAAS,kDAAkD;AAAA,IACxE;AAGA,QAAI,mBAAmB;AACrB,UAAI;AACF,mBAAW,QAAQ,kCAAkC;AACrD,oBAAY,MAAM;AAAA,UAChB;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,QACF;AACA;AAAA,UACE;AAAA,UACA,aACE,WAAW,aAAa,UAAU,CACpC;AAAA,QACF;AAAA,MACF,SAAS,cAAc;AACrB,mBAAW,QAAQ,4CAA4C;AAAA,UAC7D,OAAO,aAAa;AAAA,UACpB;AAAA,QACF,CAAC;AAED,oBAAY;AAAA,UACV,aAAa,CAAC;AAAA,UACd,OAAO,aAAa;AAAA,QACtB;AAAA,MACF;AAAA,IACF,OAAO;AACL,iBAAW,SAAS,+CAA+C;AAAA,IACrE;AAGA,QAAIA,qBAAoB;AACtB,UAAI;AACF,cAA2B,mBAAmB;AAC9C,mBAAW,QAAQ,wBAAwB;AAAA,MAC7C,SAAS,UAAU;AACjB,mBAAW,QAAQ,kCAAkC;AAAA,UACnD,OAAO,SAAS;AAAA,QAClB,CAAC;AAAA,MAEH;AAAA,IACF;AAGA;AAAA,MACE;AAAA,MACA;AAAA,IACF;AAEA,UAAM,eAAe;AAAA,MACnB,SAAS,gBAAgB,cAAc,yCAAyC,OAAO;AAAA,MACvF,QAAQ;AAAA,MACR;AAAA,MACA,SAAS,uBAAuB;AAAA,MAChC;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAEA,WAAO;AAAA,MACL,SAAS;AAAA,QACP;AAAA,UACE,MAAM;AAAA,UACN,MAAM,KAAK,UAAU,YAAY;AAAA,QACnC;AAAA,MACF;AAAA,IACF;AAAA,EACF,SAAS,OAAO;AAEd,eAAW,SAAS,+CAA+C;AAAA,MACjE,OAAO,MAAM;AAAA,MACb,OAAO,MAAM;AAAA,MACb,OAAO;AAAA,QACL,gBAAgB,MAAM;AAAA,QACtB,SAAS,MAAM;AAAA,MACjB;AAAA,IACF,CAAC;AAGD,UAAM,gBAAgB;AAAA,MACpB,OAAO;AAAA,MACP,WAAW,MAAM,QAAQ;AAAA,MACzB,cAAc,MAAM;AAAA,MACpB,SAAS;AAAA,MACT,SAAS;AAAA,MACT,oBAAoB;AAAA,MACpB,kBAAkB;AAAA,MAClB,sBAAsB;AAAA,MACtB,WAAW;AAAA,IACb;AAEA,WAAO;AAAA,MACL,SAAS;AAAA,QACP;AAAA,UACE,MAAM;AAAA,UACN,MAAM,KAAK,UAAU,aAAa;AAAA,QACpC;AAAA,MACF;AAAA,IACF;AAAA,EACF;AACF;AAUA,eAAe,8BACb,gBACA,qBACA;AACA,MAAI;AACF;AAAA,MACE;AAAA,MACA,0CAA0C,cAAc;AAAA,MACxD;AAAA,QACE,cAAc,oBAAoB;AAAA,MACpC;AAAA,IACF;AAGA,QAAI,WAAW,CAAC;AAChB,QAAI,cAAc,CAAC;AACnB,QAAI,qBAAqB,CAAC;AAC1B,QAAI,gBAAgB,CAAC;AAGrB,QAAI;AACF,iBAAW,MAAqB;AAAA,QAC9B;AAAA,MACF;AACA,iBAAW,SAAS,aAAa,SAAS,MAAM,WAAW;AAAA,IAC7D,SAAS,YAAY;AACnB,iBAAW,QAAQ,8BAA8B;AAAA,QAC/C,OAAO,WAAW;AAAA,MACpB,CAAC;AACD,iBAAW,CAAC;AAAA,IACd;AAGA,QAAI;AACF,oBAAc,MAAqB,mBAAmB,cAAc;AACpE,iBAAW,SAAS,aAAa,YAAY,MAAM,eAAe;AAAA,IACpE,SAAS,QAAQ;AACf,iBAAW,QAAQ,kCAAkC;AAAA,QACnD,OAAO,OAAO;AAAA,MAChB,CAAC;AACD,oBAAc,CAAC;AAAA,IACjB;AAGA,QAAI;AACF,sBAAgB,MAAqB;AAAA,QACnC;AAAA,MACF;AACA,iBAAW,SAAS,aAAa,cAAc,MAAM,kBAAkB;AAAA,IACzE,SAAS,OAAO;AACd,iBAAW,QAAQ,qCAAqC;AAAA,QACtD,OAAO,MAAM;AAAA,MACf,CAAC;AACD,sBAAgB,CAAC;AAAA,IACnB;AAGA,QAAI;AACF,YAAM,eAAe,oBAAoB;AAAA,QACvC,CAAC,QAAQ,IAAI,SAAS;AAAA,MACxB;AACA,YAAM,oBAAoB,oBAAoB;AAAA,QAC5C,CAAC,QAAQ,IAAI,SAAS;AAAA,MACxB;AAGA,UAAI,aAAa,SAAS,KAAK,kBAAkB,SAAS,GAAG;AAE3D,6BAAqB,MAAM;AAAA,UACzB;AAAA,UACA;AAAA,QACF;AACA;AAAA,UACE;AAAA,UACA,aAAa,mBAAmB,MAAM;AAAA,QACxC;AAAA,MACF;AAAA,IACF,SAAS,YAAY;AACnB,iBAAW,QAAQ,yCAAyC;AAAA,QAC1D,OAAO,WAAW;AAAA,MACpB,CAAC;AACD,2BAAqB,CAAC;AAAA,IACxB;AAGA,QAAI;AAEF,iBAAW,WAAW,UAAU;AAC9B,cAAqB,aAAa;AAAA,UAChC,aAAa,QAAQ;AAAA,UACrB,gBAAgB,QAAQ;AAAA,UACxB,SAAS,QAAQ;AAAA,UACjB,iBAAiB,QAAQ;AAAA,UACzB;AAAA,UACA,WAAW,KAAK,IAAI;AAAA,QACtB,CAAC;AAAA,MACH;AAGA,iBAAW,OAAO,aAAa;AAC7B,cAAqB,gBAAgB;AAAA,UACnC,SAAS,IAAI;AAAA,UACb,OAAO,IAAI;AAAA,UACX,UAAU,IAAI;AAAA,UACd,SAAS,IAAI;AAAA,UACb,iBAAiB,IAAI;AAAA,UACrB;AAAA,UACA,WAAW,KAAK,IAAI;AAAA,QACtB,CAAC;AAAA,MACH;AAGA,iBAAW,MAAM,eAAe;AAC9B,cAAqB,kBAAkB;AAAA,UACrC,KAAK,GAAG;AAAA,UACR,OAAO,GAAG;AAAA,UACV,SAAS,GAAG;AAAA,UACZ,iBAAiB,GAAG;AAAA,UACpB;AAAA,UACA,WAAW,KAAK,IAAI;AAAA,QACtB,CAAC;AAAA,MACH;AAEA,iBAAW,QAAQ,wCAAwC;AAAA,IAC7D,SAAS,UAAU;AACjB,iBAAW,QAAQ,4CAA4C;AAAA,QAC7D,OAAO,SAAS;AAAA,MAClB,CAAC;AAAA,IAEH;AAGA,QAAI;AACF,YAA2B;AAAA,QACzB;AAAA,QACA;AAAA,UACE,UAAU,SAAS;AAAA,UACnB,aAAa,YAAY;AAAA,UACzB,eAAe,cAAc;AAAA,UAC7B,oBAAoB,mBAAmB;AAAA,UACvC,WAAW,KAAK,IAAI;AAAA,QACtB;AAAA,QACA,CAAC;AAAA;AAAA,QACD;AAAA,MACF;AACA,iBAAW,SAAS,gDAAgD;AAAA,IACtE,SAAS,aAAa;AACpB,iBAAW,QAAQ,8CAA8C;AAAA,QAC/D,OAAO,YAAY;AAAA,MACrB,CAAC;AAAA,IAEH;AAGA,WAAO;AAAA,MACL;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA,gBAAgB,KAAK,IAAI;AAAA,IAC3B;AAAA,EACF,SAAS,OAAO;AACd,eAAW,SAAS,2CAA2C;AAAA,MAC7D,OAAO,MAAM;AAAA,MACb,OAAO,MAAM;AAAA,MACb;AAAA,IACF,CAAC;AACD,UAAM;AAAA,EACR;AACF;AAUA,eAAe,iBAAiB,cAAc,mBAAmB;AAC/D,MAAI;AACF;AAAA,MACE;AAAA,MACA,4BAA4B,aAAa,MAAM,sBAAsB,kBAAkB,MAAM;AAAA,IAC/F;AAGA,UAAM,cAAc,aAAa,IAAI,CAAC,QAAQ,IAAI,OAAO,EAAE,KAAK,IAAI;AACpE,UAAM,mBAAmB,kBACtB,IAAI,CAAC,QAAQ,IAAI,OAAO,EACxB,KAAK,IAAI;AAGZ,UAAM,aAAgC,SAAS,WAAW;AAC1D,UAAM,kBAAqC,SAAS,gBAAgB;AAGpE,UAAM,YAAY,wBAAwB,YAAY,EAAE;AACxD,UAAM,iBAAiB,wBAAwB,iBAAiB,EAAE;AAGlE,UAAM,cAAc,UAAU;AAAA,MAAO,CAAC,SACpC,eAAe,KAAK,CAAC,UAAU,MAAM,SAAS,KAAK,IAAI;AAAA,IACzD;AAGA,UAAM,iBAAiB,YAAY,IAAI,CAAC,SAAS;AAE/C,YAAM,WAAW;AAAA,QACf,CAAC,GAAG,cAAc,GAAG,iBAAiB;AAAA,QACtC,KAAK;AAAA,MACP;AAEA,aAAO;AAAA,QACL,SAAS,KAAK;AAAA,QACd,WAAW,KAAK;AAAA,QAChB,YAAY,KAAK,YAAY,WAAW;AAAA;AAAA,QACxC,cAAc,eACX;AAAA,UACC,CAAC,UACC,iBAAiB,KAAK,MAAM,MAAM,IAAI,KACtC,MAAM,SAAS,KAAK;AAAA,QACxB,EACC,IAAI,CAAC,UAAU,MAAM,IAAI,EACzB,MAAM,GAAG,CAAC;AAAA,QACb,UAAU,SAAS,MAAM,GAAG,CAAC;AAAA;AAAA,MAC/B;AAAA,IACF,CAAC;AAED,eAAW,SAAS,aAAa,eAAe,MAAM,kBAAkB;AACxE,WAAO;AAAA,EACT,SAAS,OAAO;AACd,eAAW,SAAS,6BAA6B;AAAA,MAC/C,OAAO,MAAM;AAAA,IACf,CAAC;AACD,UAAM;AAAA,EACR;AACF;AAUA,eAAe,6BAA6B,gBAAgB,SAAS;AACnE,MAAI;AACF,YAAQ;AAAA,MACN,sEAAsE,cAAc;AAAA,IACtF;AAGA,UAAM,WAAW,MAAqB;AAAA,MACpC;AAAA,IACF;AAEA,QAAI,CAAC,YAAY,SAAS,WAAW,GAAG;AACtC,cAAQ;AAAA,QACN,oEAAoE,cAAc;AAAA,MACpF;AACA,aAAO;AAAA,QACL,UAAU;AAAA,QACV,UAAU,CAAC;AAAA,MACb;AAAA,IACF;AAEA,YAAQ;AAAA,MACN,wCAAwC,SAAS,MAAM;AAAA,IACzD;AAGA,UAAM,mBAAmB;AAAA,MACvB,UAAU;AAAA,MACV,UAAU,CAAC;AAAA,IACb;AAGA,QAAI,gBAAgB;AACpB,QAAI,YAAY;AAAa,sBAAgB;AAC7C,QAAI,YAAY;AAAa,sBAAgB;AAG7C,eAAW,WAAW,UAAU;AAC9B,UAAI;AAEF,YAAI,QAAQ,WAAW;AACrB,2BAAiB,SAAS,KAAK;AAAA,YAC7B,WAAW,QAAQ;AAAA,YACnB,MAAM,QAAQ;AAAA,YACd,MAAM,QAAQ;AAAA,YACd,UAAU;AAAA,YACV,YAAY,QAAQ;AAAA,UACtB,CAAC;AACD;AAAA,QACF;AAGA,YAAI,QAAQ,mBAAmB,eAAe;AAC5C,2BAAiB,SAAS,KAAK;AAAA,YAC7B,WAAW,QAAQ;AAAA,YACnB,MAAM,QAAQ;AAAA,YACd,MAAM,QAAQ;AAAA,YACd,UAAU;AAAA,YACV,YAAY,QAAQ;AAAA,UACtB,CAAC;AACD;AAAA,QACF;AAGA,cAA8B;AAAA,UAC5B,QAAQ;AAAA,UACR,QAAQ;AAAA,QACV;AAGA,cAAM,kBACJ,YAAY,eAAe,YAAY,mBACnC,iBACA;AAEN,cAA8B;AAAA,UAC5B,QAAQ;AAAA,UACR;AAAA,UACA,EAAE,eAAe;AAAA,QACnB;AAGA,yBAAiB;AACjB,yBAAiB,SAAS,KAAK;AAAA,UAC7B,WAAW,QAAQ;AAAA,UACnB,MAAM,QAAQ;AAAA,UACd,MAAM,QAAQ;AAAA,UACd,UAAU;AAAA,UACV,YAAY,QAAQ;AAAA,QACtB,CAAC;AAED,gBAAQ;AAAA,UACN,gEAAgE,QAAQ,UAAU;AAAA,QACpF;AAAA,MACF,SAAS,OAAO;AACd,gBAAQ;AAAA,UACN,2DAA2D,QAAQ,UAAU;AAAA,UAC7E;AAAA,QACF;AAAA,MAEF;AAAA,IACF;AAEA,YAAQ;AAAA,MACN,2CAA2C,iBAAiB,QAAQ;AAAA,IACtE;AACA,WAAO;AAAA,EACT,SAAS,OAAO;AACd,YAAQ;AAAA,MACN;AAAA,MACA;AAAA,IACF;AACA,WAAO;AAAA,MACL,UAAU;AAAA,MACV,UAAU,CAAC;AAAA,MACX,OAAO,MAAM;AAAA,IACf;AAAA,EACF;AACF;AAWA,eAAe,uCACb,gBACA,oBACA,qBACA;AACA,MAAI;AACF,YAAQ;AAAA,MACN,8EAA8E,cAAc;AAAA,IAC9F;AAGA,UAAM,gBAAgB,oBAAI,IAAI;AAC9B,uBAAmB,QAAQ,CAAC,UAAU;AACpC,UAAI,MAAM,YAAY,MAAM,QAAQ,MAAM,QAAQ,GAAG;AACnD,cAAM,SAAS,QAAQ,CAAC,OAAO,cAAc,IAAI,EAAE,CAAC;AAAA,MACtD;AAAA,IACF,CAAC;AAED,UAAM,eAAe,MAAM,KAAK,aAAa;AAG7C,UAAM,2BAA2B,MAA2B,UAAU;AAAA,MACpE,OAAO,CAAC,oBAAoB,wBAAwB;AAAA,MACpD,OAAO;AAAA,MACP,uBAAuB;AAAA,IACzB,CAAC;AAED,QAAI,CAAC,4BAA4B,yBAAyB,WAAW,GAAG;AACtE,cAAQ;AAAA,QACN;AAAA,MACF;AACA,aAAO;AAAA,QACL,cAAc;AAAA,QACd,eAAe,CAAC;AAAA,QAChB,qBAAqB,CAAC;AAAA,MACxB;AAAA,IACF;AAGA,UAAM,sBAAsB,CAAC;AAE7B,eAAW,SAAS,0BAA0B;AAC5C,UAAI;AACF,YAAI,CAAC,MAAM,QAAQ,CAAC,MAAM;AAAiB;AAG3C,cAAM,cACJ,MAA+B;AAAA,UAC7B,MAAM;AAAA,QACR;AAGF,cAAM,gBAAgB,oBAAI,IAAI;AAC9B,oBAAY,QAAQ,CAAC,UAAU;AAC7B,cAAI,MAAM,YAAY,MAAM,QAAQ,MAAM,QAAQ,GAAG;AACnD,kBAAM,SAAS,QAAQ,CAAC,OAAO,cAAc,IAAI,EAAE,CAAC;AAAA,UACtD;AAAA,QACF,CAAC;AAGD,cAAM,eAAe,aAAa;AAAA,UAAO,CAAC,OACxC,cAAc,IAAI,EAAE;AAAA,QACtB,EAAE;AACF,cAAM,uBAAsB,oBAAI,IAAI,CAAC,GAAG,cAAc,GAAG,aAAa,CAAC,GACpE;AAEH,cAAM,kBACJ,sBAAsB,IAAI,eAAe,sBAAsB;AAGjE,cAAM,eAAe,CAAC;AACtB,oBAAY,QAAQ,CAAC,eAAe;AAClC,6BAAmB,QAAQ,CAAC,iBAAiB;AAC3C,gBACE,WAAW,cACX,aAAa,cACb,WAAW,WAAW,YAAY,MAChC,aAAa,WAAW,YAAY,GACtC;AACA,2BAAa,KAAK,WAAW,UAAU;AAAA,YACzC;AAAA,UACF,CAAC;AAAA,QACH,CAAC;AAGD,YAAI,kBAAkB,OAAO,aAAa,SAAS,GAAG;AACpD,8BAAoB,KAAK;AAAA,YACvB,gBAAgB,MAAM;AAAA,YACtB,SAAS,MAAM,KAAK,WAAW;AAAA,YAC/B,WAAW,MAAM;AAAA,YACjB;AAAA,YACA;AAAA,UACF,CAAC;AAAA,QACH;AAAA,MACF,SAAS,OAAO;AACd,gBAAQ;AAAA,UACN,mEAAmE,MAAM,QAAQ;AAAA,UACjF;AAAA,QACF;AAAA,MAEF;AAAA,IACF;AAGA,wBAAoB,KAAK,CAAC,GAAG,MAAM,EAAE,kBAAkB,EAAE,eAAe;AAGxE,UAAM,uBAAuB,oBAAoB,MAAM,GAAG,CAAC;AAE3D,YAAQ;AAAA,MACN,kDAAkD,qBAAqB,MAAM;AAAA,IAC/E;AAGA,UAAM,sBACJ,MAAM;AAAA,MACJ;AAAA,MACA;AAAA,IACF;AAEF,WAAO;AAAA,MACL,cAAc,qBAAqB;AAAA,MACnC,eAAe;AAAA,MACf;AAAA,IACF;AAAA,EACF,SAAS,OAAO;AACd,YAAQ;AAAA,MACN;AAAA,MACA;AAAA,IACF;AACA,WAAO;AAAA,MACL,cAAc;AAAA,MACd,eAAe,CAAC;AAAA,MAChB,qBAAqB,CAAC;AAAA,MACtB,OAAO,MAAM;AAAA,IACf;AAAA,EACF;AACF;AAUA,eAAe,4CACb,sBACA,gBACA;AACA,MAAI;AAEF,QAAI,CAAC,wBAAwB,qBAAqB,WAAW,GAAG;AAC9D,aAAO,CAAC;AAAA,IACV;AAGA,UAAM,uBAAuB,CAAC;AAG9B,yBAAqB,QAAQ,CAAC,iBAAiB;AAC7C,UAAI,aAAa,gBAAgB,aAAa,aAAa,SAAS,GAAG;AACrE,qBAAa,aAAa,QAAQ,CAAC,UAAU;AAC3C,cAAI,CAAC,qBAAqB,KAAK,GAAG;AAChC,iCAAqB,KAAK,IAAI,CAAC;AAAA,UACjC;AACA,+BAAqB,KAAK,EAAE,KAAK,YAAY;AAAA,QAC/C,CAAC;AAAA,MACH;AAAA,IACF,CAAC;AAGD,QAAI,OAAO,KAAK,oBAAoB,EAAE,WAAW,KAAK,gBAAgB;AACpE,YAAM,iBAAiB,uBAAuB,cAAc;AAC5D,2BAAqB,cAAc,IAAI;AAAA,IACzC;AAGA,UAAM,WAAW,CAAC;AAElB,eAAW,CAAC,OAAO,aAAa,KAAK,OAAO,QAAQ,oBAAoB,GAAG;AAEzE,UAAI,cAAc,UAAU,GAAG;AAE7B,cAAM,oBAAoB,cACvB,IAAI,CAAC,MAAM,EAAE,OAAO,EACpB,KAAK,KAAK;AAGb,cAAM,UAAU,MAA6B;AAAA,UAC3C;AAAA,UACA;AAAA,YACE,cAAc;AAAA,YACd,mBAAmB;AAAA,UACrB;AAAA,QACF;AAEA,iBAAS,KAAK;AAAA,UACZ;AAAA,UACA;AAAA,UACA,mBAAmB,cAAc;AAAA,UACjC,iBAAiB,cAAc,IAAI,CAAC,OAAO;AAAA,YACzC,gBAAgB,EAAE;AAAA,YAClB,SAAS,EAAE;AAAA,UACb,EAAE;AAAA,QACJ,CAAC;AAAA,MACH;AAAA,IACF;AAEA,WAAO;AAAA,EACT,SAAS,OAAO;AACd,YAAQ;AAAA,MACN;AAAA,MACA;AAAA,IACF;AACA,WAAO,CAAC;AAAA,EACV;AACF;AAYA,eAAe,6BACb,gBACA,SACA,SACA,oBACA;AACA,MAAI;AACF,YAAQ;AAAA,MACN,yEAAyE,cAAc;AAAA,IACzF;AAGA,UAAM,SAAS;AAAA,MACb,oBAAoB,CAAC;AAAA,MACrB,gBAAgB,CAAC;AAAA,MACjB,oBAAoB,CAAC;AAAA,IACvB;AAGA,UAAM,SAA4B,SAAS,OAAO;AAClD,UAAM,WAA8B,gBAAgB,QAAQ,EAAE;AAG9D,QAAI,YAAY,CAAC;AACjB,QAAI,iBAAiB,CAAC;AAEtB,QAAI,SAAS;AAEX,cAAQ,QAAQ,YAAY,GAAG;AAAA,QAC7B,KAAK;AAAA,QACL,KAAK;AACH,oBAAU,KAAK;AAAA,YACb,QAAQ;AAAA,YACR,UAAU;AAAA,YACV,WAAW;AAAA,UACb,CAAC;AACD,oBAAU,KAAK;AAAA,YACb,QAAQ;AAAA,YACR,UAAU;AAAA,YACV,WAAW;AAAA,UACb,CAAC;AACD;AAAA,QAEF,KAAK;AAAA,QACL,KAAK;AACH,oBAAU,KAAK;AAAA,YACb,QAAQ;AAAA,YACR,UAAU;AAAA,YACV,WAAW;AAAA,UACb,CAAC;AACD,oBAAU,KAAK;AAAA,YACb,QAAQ;AAAA,YACR,UAAU;AAAA,YACV,WAAW;AAAA,UACb,CAAC;AACD;AAAA,QAEF,KAAK;AACH,oBAAU,KAAK;AAAA,YACb,QAAQ;AAAA,YACR,UAAU;AAAA,YACV,WAAW;AAAA,UACb,CAAC;AACD,oBAAU,KAAK;AAAA,YACb,QAAQ;AAAA,YACR,UAAU;AAAA,YACV,WAAW;AAAA,UACb,CAAC;AACD;AAAA,QAEF,KAAK;AAAA,QACL,KAAK;AACH,oBAAU,KAAK;AAAA,YACb,QAAQ;AAAA,YACR,UAAU;AAAA,YACV,WAAW;AAAA,UACb,CAAC;AACD,oBAAU,KAAK;AAAA,YACb,QAAQ;AAAA,YACR,UAAU;AAAA,YACV,WAAW;AAAA,UACb,CAAC;AACD;AAAA,QAEF;AAEE,oBAAU,KAAK;AAAA,YACb,QAAQ;AAAA,YACR,UAAU;AAAA,YACV,WAAW;AAAA,UACb,CAAC;AAAA,MACL;AAAA,IACF;AAGA,QAAI,sBAAsB,mBAAmB,WAAW;AAEtD,YAAM,kBAAkB,mBAAmB,UAAU;AAAA,QACnD,CAAC,MAAM,EAAE,SAAS;AAAA,MACpB;AAEA,UAAI,gBAAgB,SAAS,GAAG;AAC9B,cAAM,0BAA0B,gBAC7B,OAAO,CAAC,MAAM,EAAE,cAAc,GAAG,EACjC,MAAM,GAAG,CAAC;AAEb,gCAAwB,QAAQ,CAAC,aAAa;AAC5C,yBAAe,KAAK;AAAA,YAClB,OAAO,+BAA+B,SAAS,OAAO;AAAA,YACtD,UAAU;AAAA,YACV,WAAW;AAAA,UACb,CAAC;AAAA,QACH,CAAC;AAAA,MACH;AAGA,YAAM,cAAc,mBAAmB,UAAU;AAAA,QAC/C,CAAC,MAAM,EAAE,SAAS;AAAA,MACpB;AAEA,UAAI,YAAY,SAAS,GAAG;AAC1B,cAAM,eAAe,YAClB,OAAO,CAAC,MAAM,EAAE,cAAc,GAAG,EACjC,MAAM,GAAG,CAAC;AAEb,qBAAa,QAAQ,CAAC,QAAQ;AAC5B,yBAAe,KAAK;AAAA,YAClB,OAAO,4BAA4B,IAAI,OAAO;AAAA,YAC9C,UAAU;AAAA,YACV,WAAW;AAAA,UACb,CAAC;AAAA,QACH,CAAC;AAAA,MACH;AAAA,IACF;AAGA,UAAM,mBAAmB,MAA8B;AAAA,MACrD;AAAA,MACA;AAAA,QACE,WAAW,CAAC,MAAM,OAAO,OAAO,OAAO,KAAK;AAAA,QAC5C,YAAY;AAAA,QACZ,qBAAqB;AAAA,MACvB;AAAA,IACF;AAEA,UAAM,qBAAqB,iBAAiB,IAAI,CAACC,aAAY;AAAA,MAC3D,OAAOA,QAAO,QAAQA,QAAO,aAAa;AAAA,MAC1C,MAAMA,QAAO;AAAA,MACb,MAAMA,QAAO,eAAe;AAAA,MAC5B,WAAWA,QAAO,SAAS;AAAA,IAC7B,EAAE;AAGF,WAAO,qBAAqB;AAC5B,WAAO,iBAAiB;AACxB,WAAO,qBAAqB;AAE5B,YAAQ;AAAA,MACN,4CAA4C,UAAU,MAAM,mBAAmB,eAAe,MAAM;AAAA,IACtG;AAEA,WAAO;AAAA,EACT,SAAS,OAAO;AACd,YAAQ;AAAA,MACN;AAAA,MACA;AAAA,IACF;AACA,WAAO;AAAA,MACL,oBAAoB,CAAC;AAAA,MACrB,gBAAgB,CAAC;AAAA,MACjB,oBAAoB,CAAC;AAAA,MACrB,OAAO,MAAM;AAAA,IACf;AAAA,EACF;AACF;AAGA,IAAO,2CAAQ;AAAA,EACb,MAAM;AAAA,EACN,aACE;AAAA,EACF,aAAa;AAAA,EACb,cAAc;AAAA,EACd,SAAAF;AACF;;;AChoCA,IAAM,WAAW;AAAA,EACf;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF;AAEA,IAAO,gBAAQ;;;ACbf;AAGA,IAAI,OAAO,OAAO,uBAAuB,aAAa;AACpD,SAAO,qBAAqB;AAC9B;AASO,SAAS,kBAAkBG,UAAS,UAAU;AACnD,SAAO,OAAO,QAAQ,YAAY;AAChC,QAAI;AACF,iBAAW,SAAS,GAAG,QAAQ,yBAAyB;AAAA,QACtD,YAAY,OAAO,KAAK,MAAM;AAAA,MAChC,CAAC;AAGD,UAAI,eAAe;AACnB,UACE,UACA,OAAO,WAAW,YAClB,OAAO,KAAK,MAAM,EAAE,WAAW,KAC/B,OAAO,UACP,OAAO,KAAK,OAAO,MAAM,EAAE,WAAW,GACtC;AAEA,uBAAe,CAAC;AAChB;AAAA,UACE;AAAA,UACA,GAAG,QAAQ;AAAA,UACX,EAAE,OAAO;AAAA,QACX;AAAA,MACF,WAAW,UAAU,OAAO,UAAU,OAAO,KAAK,MAAM,EAAE,SAAS,GAAG;AAEpE,cAAM,EAAE,QAAQ,GAAG,YAAY,IAAI;AACnC,uBAAe;AACf;AAAA,UACE;AAAA,UACA,GAAG,QAAQ;AAAA,UACX;AAAA,YACE,iBAAiB,OAAO,KAAK,YAAY;AAAA,UAC3C;AAAA,QACF;AAAA,MACF;AAGA,YAAM,kBAAkB,uBAAuB,YAAY;AAG3D,YAAM,gBAAgB,2BAA2B,QAAQ;AAGzD,YAAM,eAAe,EAAE,GAAG,eAAe,GAAG,gBAAgB;AAG5D,UAAI,aAAa,gBAAgB;AAC/B,eAAO,qBAAqB,aAAa;AAAA,MAC3C;AAGA,YAAM,SAAS,MAAMA,SAAQ,cAAc,OAAO;AAElD,aAAO;AAAA,QACL,SAAS;AAAA,UACP;AAAA,YACE,MAAM;AAAA,YACN,MAAM,OAAO,WAAW,WAAW,SAAS,KAAK,UAAU,MAAM;AAAA,UACnE;AAAA,QACF;AAAA,MACF;AAAA,IACF,SAAS,OAAO;AACd,iBAAW,SAAS,YAAY,QAAQ,iBAAiB;AAAA,QACvD,OAAO,MAAM;AAAA,QACb,OAAO,MAAM;AAAA,MACf,CAAC;AAED,aAAO;AAAA,QACL,SAAS;AAAA,UACP;AAAA,YACE,MAAM;AAAA,YACN,MAAM,KAAK,UAAU;AAAA,cACnB,OAAO;AAAA,cACP,SAAS,MAAM;AAAA,cACf,SAAS,MAAM;AAAA,YACjB,CAAC;AAAA,UACH;AAAA,QACF;AAAA,MACF;AAAA,IACF;AAAA,EACF;AACF;AAQA,SAAS,uBAAuB,OAAO;AACrC,QAAM,kBAAkB,CAAC;AAEzB,MAAI;AAEF,QAAI,SAAS,OAAO,UAAU,UAAU;AAEtC,aAAO,KAAK,KAAK,EAAE,QAAQ,CAAC,QAAQ;AAClC,YAAI,QAAQ,YAAY,QAAQ,aAAa;AAC3C,0BAAgB,GAAG,IAAI,MAAM,GAAG;AAAA,QAClC;AAAA,MACF,CAAC;AAGD,UAAI,MAAM,eAAe;AACvB,YAAI;AAEF,gBAAM,aAAa,KAAK,MAAM,MAAM,aAAa;AACjD,iBAAO,OAAO,iBAAiB,UAAU;AAAA,QAC3C,SAAS,GAAG;AAEV,cACE,OAAO,MAAM,kBAAkB,YAC/B,MAAM,cAAc,SAAS,MAC7B,MAAM,cAAc,SAAS,GAAG,GAChC;AACA,4BAAgB,iBAAiB,MAAM;AAAA,UACzC;AAAA,QACF;AAAA,MACF;AAGA,UAAI,MAAM,cAAc;AACtB,wBAAgB,eAAe,MAAM;AAAA,MACvC;AAEA,UAAI,MAAM,cAAc;AACtB,wBAAgB,eAAe,MAAM;AAAA,MACvC;AAEA,UAAI,MAAM,OAAO;AACf,wBAAgB,QAAQ,MAAM;AAAA,MAChC;AAEA,UAAI,MAAM,MAAM;AACd,wBAAgB,OAAO,MAAM;AAAA,MAC/B;AAAA,IACF,WAES,OAAO,UAAU,UAAU;AAClC,UAAI;AAEF,cAAM,aAAa,KAAK,MAAM,KAAK;AACnC,eAAO,OAAO,iBAAiB,UAAU;AAAA,MAC3C,SAAS,GAAG;AAEV,YAAI,MAAM,SAAS,MAAM,MAAM,SAAS,GAAG,GAAG;AAC5C,0BAAgB,iBAAiB;AAAA,QACnC;AAAA,MACF;AAAA,IACF;AAAA,EACF,SAAS,GAAG;AACV,eAAW,SAAS,4BAA4B,EAAE,OAAO,EAAE;AAAA,EAC7D;AAEA,SAAO;AACT;AAQA,SAAS,2BAA2B,UAAU;AAC5C,UAAQ,UAAU;AAAA,IAChB,KAAK;AACH,aAAO;AAAA,QACL,cAAc;AAAA,QACd,qBAAqB;AAAA,QACrB,4BAA4B;AAAA,QAC5B,qBAAqB;AAAA,QACrB,kBAAkB;AAAA,QAClB,cAAc;AAAA,MAChB;AAAA,IACF,KAAK;AACH,aAAO;AAAA,QACL,gBAAgB,OAAO;AAAA,QACvB,aAAa;AAAA,UACX;AAAA,YACE,MAAM;AAAA,YACN,SAAS;AAAA,UACX;AAAA,QACF;AAAA,QACA,6BAA6B;AAAA,QAC7B,yBAAyB;AAAA,QACzB,wBAAwB;AAAA,MAC1B;AAAA,IACF,KAAK;AACH,aAAO;AAAA,QACL,gBAAgB,OAAO;AAAA,QACvB,OAAO;AAAA,QACP,aAAa;AAAA,UACX,qBAAqB;AAAA,UACrB,kBAAkB;AAAA,QACpB;AAAA,QACA,gBAAgB;AAAA,UACd,mBAAmB;AAAA,QACrB;AAAA,QACA,mBAAmB;AAAA,QACnB,iBAAiB;AAAA,QACjB,gBAAgB;AAAA,MAClB;AAAA,IACF,KAAK;AACH,aAAO;AAAA,QACL,gBAAgB,OAAO;AAAA,QACvB,MAAM;AAAA,QACN,aAAa;AAAA,QACb,mBAAmB;AAAA,QACnB,cAAc;AAAA,MAChB;AAAA,IACF,KAAK;AACH,aAAO;AAAA,QACL,gBAAgB,OAAO;AAAA,QACvB,oBAAoB;AAAA,QACpB,kBAAkB;AAAA,QAClB,iBAAiB;AAAA,QACjB,yBAAyB;AAAA,QACzB,mBAAmB;AAAA,QACnB,SAAS;AAAA,MACX;AAAA,IACF;AACE,aAAO,CAAC;AAAA,EACZ;AACF;AAQO,SAAS,+BAA+BA,UAAS;AACtD,SAAO,kBAAkBA,UAAS,iCAAiC;AACrE;AAQO,SAAS,6BAA6BA,UAAS;AACpD,SAAO,kBAAkBA,UAAS,+BAA+B;AACnE;;;AzB3OA,eAAe,cAAc;AAE3B,MAAI,CAAC,sBAAsB,CAAC,kBAAkB;AAC5C;AAAA,MACE;AAAA,MACA;AAAA,IACF;AACA,YAAQ,KAAK,CAAC;AAAA,EAChB;AAGA,MAAI;AACF,eAAW,QAAQ,4BAA4B;AAC/C,UAAMC,YAAW,YAAY;AAC7B,eAAW,QAAQ,uCAAuC;AAAA,EAC5D,SAAS,OAAO;AACd,eAAW,SAAS,qCAAqC,MAAM,OAAO,EAAE;AACxE,YAAQ,KAAK,CAAC;AAAA,EAChB;AAGA,MAAI;AACF,eAAW,QAAQ,gCAAgC;AACnD,UAAM,iBAAiB;AACvB,eAAW,QAAQ,iCAAiC;AAAA,EACtD,SAAS,OAAO;AACd,eAAW,SAAS,+BAA+B,MAAM,OAAO,EAAE;AAClE,YAAQ,KAAK,CAAC;AAAA,EAChB;AAGA,MAAI;AACF,eAAW,QAAQ,iCAAiC;AACpD,UAAM,yBAAyB;AAC/B,eAAW,QAAQ,2CAA2C;AAAA,EAChE,SAAS,OAAO;AACd;AAAA,MACE;AAAA,MACA,yCAAyC,MAAM,OAAO;AAAA,IACxD;AACA,YAAQ,KAAK,CAAC;AAAA,EAChB;AAGA,QAAM,SAAS,IAAI,UAAU;AAAA,IAC3B,MAAM;AAAA,IACN,SAAS;AAAA,EACX,CAAC;AAGD,aAAW,QAAQ,eAAU;AAC3B,QAAI;AAGJ,QAAI,KAAK,SAAS,mCAAmC;AACnD,uBAAiB,+BAA+B,KAAK,OAAO;AAAA,IAC9D,WAAW,KAAK,SAAS,iCAAiC;AACxD,uBAAiB,6BAA6B,KAAK,OAAO;AAAA,IAC5D,OAAO;AAEL,uBAAiB,kBAAkB,KAAK,SAAS,KAAK,IAAI;AAAA,IAC5D;AAGA,WAAO,KAAK,KAAK,MAAM,KAAK,aAAa,cAAc;AACvD,eAAW,QAAQ,oBAAoB,KAAK,IAAI,EAAE;AAAA,EACpD;AAEA,QAAM,YAAY,IAAI,qBAAqB;AAC3C,aAAW,QAAQ,gCAAgC,QAAQ,GAAG,KAAK;AAEnE,MAAI;AACF,UAAM,OAAO,QAAQ,SAAS;AAC9B,eAAW,QAAQ,qBAAqB;AAAA,EAC1C,SAAS,OAAO;AACd,eAAW,SAAS,qBAAqB,MAAM,OAAO,EAAE;AACxD,YAAQ,KAAK,CAAC;AAAA,EAChB;AACF;AAGA,IACE,YAAY,QAAQ,YAAY,WAChC,QAAQ,IAAI,aAAa,QACzB;AACA,cAAY,EAAE,MAAM,CAAC,UAAU;AAC7B,eAAW,SAAS,mCAAmC,MAAM,OAAO,EAAE;AACtE,YAAQ,MAAM,KAAK;AACnB,YAAQ,KAAK,CAAC;AAAA,EAChB,CAAC;AACH;",
  "names": ["dbClient", "z", "uuidv4", "uuidv4", "uuidv4", "acorn", "uuidv4", "uuidv4", "uuidv4", "uuidv4", "uuidv4", "uuidv4", "uuidv4", "uuidv4", "uuidv4", "path", "uuidv4", "uuidv4", "uuidv4", "uuidv4", "extractKeywords", "z", "handler", "path", "activeFocus", "z", "handler", "handler", "z", "uuidv4", "uuidv4", "pattern", "uuidv4", "patterns", "uuidv4", "handler", "activeFocus", "z", "handler", "clearActiveContext", "result", "handler", "dbClient"]
}
